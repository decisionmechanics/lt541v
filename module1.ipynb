{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c26286-c7a0-4c9a-a57c-64458960121a",
   "metadata": {},
   "source": [
    "# Module 1: Supervised learning algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44a7ba2d-40c1-42c3-bef8-bbecf123edec",
   "metadata": {},
   "source": [
    "In this module, we cover\n",
    "\n",
    "- Overview of classification algorithms (e.g. logistic regression, decision trees, ensemble learning algorithms)\n",
    "- Demonstration of how decision tree classifiers work\n",
    "- Hands-on example of using XGBoost to classify observations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09a43f-da4b-4537-b810-b9910a7ea71c",
   "metadata": {},
   "source": [
    "The [notebooks](https://github.com/decisionmechanics/lt541v) for the course are available on GitHub. Clone or download them to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83306d62-9a76-44f3-9b66-032338f9af39",
   "metadata": {},
   "source": [
    "In this notebook, we make use of the following third-party packages.\n",
    "\n",
    "```bash\n",
    "pip install jupyterlab matplotlib numpy plotly 'polars[all]' scikit-learn xgboost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966deece-0140-4d4f-a74b-b2dc83901040",
   "metadata": {},
   "source": [
    "## Overview of classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75fc92b-961a-4fce-8f1b-9f5ff15e8038",
   "metadata": {},
   "source": [
    "Classification algorithms attempt to assign observations to a number of classes. Two-class (binary) classifiers are a special case.\n",
    "\n",
    "For example, a facial recognition classifier may be used to classify people as employees or members of the public. A spam filter classifies e-mail messages as legitimate or spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562af061-bbfa-41c4-9a04-e60cff9b39e3",
   "metadata": {},
   "source": [
    "Choosing a suitable classification algorithm is as much art as it is science. There are attempts to formalise the process (e.g. Microsoft's [Machine Learning Algorithm Cheat Sheet](https://docs.microsoft.com/en-us/azure/machine-learning/media/algorithm-cheat-sheet/machine-learning-algorithm-cheat-sheet.png)), but the correct choice depends on things like\n",
    "\n",
    "- the task\n",
    "- the data\n",
    "- the skill of the team\n",
    "- the budget\n",
    "- the timeframe\n",
    "                                                                                                                           \n",
    "It's usually worth trying a range of approaches, if time and budget allow it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b10f945-03ad-45b7-af67-6edc51bd4f00",
   "metadata": {},
   "source": [
    "Prepare the (scaled) training and test datasets. We'll use this data to explore the different algorithms.\n",
    "\n",
    "The data will be partitioned into training and test datasets. The latter is used to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb2360-4793-4164-ae0f-d62f53047d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "loan_approval_df = pl.read_csv(\"data/loan-approval-dataset.csv\")\n",
    "\n",
    "loan_approval_with_dummies_df = loan_approval_df.to_dummies(\n",
    "    [\"education\", \"self_employed\"], drop_first=True\n",
    ")\n",
    "\n",
    "loan_approval_features_df = loan_approval_with_dummies_df.drop(\"loan_id\", \"loan_status\")\n",
    "\n",
    "loan_approval_target_df = loan_approval_df.select(\"loan_status\")\n",
    "\n",
    "SEED = 123\n",
    "\n",
    "(\n",
    "    loan_approval_feature_train_df,\n",
    "    loan_approval_feature_test_df,\n",
    "    loan_approval_target_train_df,\n",
    "    loan_approval_target_test_df,\n",
    ") = train_test_split(\n",
    "    loan_approval_features_df,\n",
    "    loan_approval_target_df,\n",
    "    test_size=0.30,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "loan_approval_feature_train_scaled_df = scaler.fit_transform(\n",
    "    loan_approval_feature_train_df\n",
    ")\n",
    "\n",
    "loan_approval_feature_test_scaled_df = scaler.fit_transform(\n",
    "    loan_approval_feature_test_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef7d47-c269-4e49-b629-fd370a6a221e",
   "metadata": {},
   "source": [
    "Do we have balanced target classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b89e13b-c52d-476b-a0a4-186a31b733c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_approval_df.get_column(\"loan_status\").value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6889398e-e970-48d1-bb04-502f49312b10",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf9b53-70ff-4b19-b42d-04fa070ff2dc",
   "metadata": {},
   "source": [
    "The distribution of the target classes gives us a baseline for the performance of our model. If we assume that loans are always approved, we have an accuracy of over 62%.\n",
    "\n",
    "We may already have experts in place analysing loans. How accurate are they? More than 62%? If so, that might be our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a24914-6d32-4ba0-9172-f88d8291987c",
   "metadata": {},
   "source": [
    "What about if we use credit score as an indicator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46cf84-160e-4876-ae62-ca2d18a3c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    loan_approval_df\n",
    "    .with_columns(\n",
    "        (pl.col(\"cibil_score\") >= 550).alias(\"good_credit_score\")\n",
    "    )\n",
    "    .select([\"good_credit_score\", \"loan_status\"])\n",
    "    .group_by([\"good_credit_score\", \"loan_status\"])\n",
    "    .agg(\n",
    "        pl.len().alias(\"count\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ed557-ce8d-416c-bde8-2bfc7b08b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1600 + 2471) / len(loan_approval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11e280-d13c-4fa6-b8f2-f9e7b3f6b994",
   "metadata": {},
   "source": [
    "That gives us an accuracy of 95% . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf2abff-4a5d-48cc-8d02-940c735298fc",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2920a46d-1bcd-45b6-a339-e6e1508f928e",
   "metadata": {},
   "source": [
    "Logistic regression is a statistical method used for binary classification tasks, where the goal is to predict a binary outcome (often coded as 0 or 1). Despite its name, logistic regression is used for classification problems, not regression problems.\n",
    "\n",
    "It begins by calculating a weighted sum of the (input) features. This is similar to how linear regression works.\n",
    "\n",
    "$$\n",
    "    z = b + w_{1}x_{1} + w_{2}x_{2} + \\cdots + w_{n}x_{n}\n",
    "$$\n",
    "\n",
    "$z$ is then passed through a sigmoid (logistic) function to map it to a number in the $[0, 1]$ range.\n",
    "\n",
    "$$\n",
    "    P(y=1|X) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "This represents the probability that the observation belongs to class 1 (e.g. \"yes\").\n",
    "\n",
    "We can visualise this.\n",
    "\n",
    "<img src=\"images/module1-logistic-regression.svg\" alt=\"Logistic regression\" width=\"200px\" />\n",
    "                                                                         \n",
    "A threshold (often 0.5) is then chosen to classify the outcome.\n",
    "\n",
    "- If $P(y=1|X) > 0.5$, the prediction is class 1\n",
    "- If $P(y=1|X) <= 0.5$, the prediction is class 0\n",
    "\n",
    "Classes are separated via a linear decision boundary---a line or a plane---that separates the two classes. This means that logistic regression is only suitable for simpler classification tasks.\n",
    "\n",
    "Logistic regression can be extended to multiclass classification problems using techniques such as [softmax regression](http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5bb09c-2327-4ba3-b7ce-4f70c285f3d5",
   "metadata": {},
   "source": [
    "#### Logistic regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3379db-cb09-4579-b606-f202a52b09a9",
   "metadata": {},
   "source": [
    "Fit the logistic regression model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cd39b-3d9f-436d-ae7b-28053eb336d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression_classifier = LogisticRegression(penalty=None)\n",
    "\n",
    "logistic_regression_classifier.fit(\n",
    "    loan_approval_feature_train_scaled_df,\n",
    "    loan_approval_target_train_df.get_column(\"loan_status\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a095aa-4466-4459-845d-037ae9bb30c9",
   "metadata": {},
   "source": [
    "Review the classes being predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c30e6-35ed-461b-ae84-aecef88c2aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_classifier.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d4bfd-2c40-4fd0-8f34-bb718f25db99",
   "metadata": {},
   "source": [
    "Which features have the most impact on each of the two classes (positive coefficients support \"Rejected\")?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd615c-4458-4d4a-8549-78e7d10e74df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"feature\": loan_approval_feature_train_df.columns,\n",
    "            \"coefficient\": logistic_regression_classifier.coef_[0],\n",
    "        }\n",
    "    )\n",
    "    .sort(\"coefficient\", descending=True)\n",
    "    .plot.bar(\n",
    "        x=\"coefficient\",\n",
    "        y=alt.Y(\"feature\", sort=\"-x\"),\n",
    "    )\n",
    "    .properties(\n",
    "        width=500,\n",
    "        height=300,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd7ff9-25a4-4273-b9e6-8cb6a6dd9238",
   "metadata": {},
   "source": [
    "What are the probabilities that each of the observations in the test dataset have been assigned to the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a97dc-52ce-4c38-a451-7d54a9f3db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_classifier.predict_proba(loan_approval_feature_test_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d336b-334a-4aec-81b2-70abe547e23f",
   "metadata": {},
   "source": [
    "Show the predicted (most probable) class for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb8b88-9c80-4247-a7f7-09db2ba011ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_classifier.predict(loan_approval_feature_test_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9f6fdd-b3a7-4b6b-82d2-45b4066c36d0",
   "metadata": {},
   "source": [
    "How does the model perform on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb9617-d769-4a5c-bac8-05a3f99fe45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        loan_approval_target_test_df.get_column(\"loan_status\"),\n",
    "        logistic_regression_classifier.predict(loan_approval_feature_test_scaled_df),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1149bba-bae2-4598-8f8e-526b3c9f7c50",
   "metadata": {},
   "source": [
    "The accuracy of this model is (a lowly) 91%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032efe6b-ab79-4145-95a7-a536a3a358a7",
   "metadata": {},
   "source": [
    "#### Benefits of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f05242-a6c2-46b1-aa16-292a7eaec111",
   "metadata": {},
   "source": [
    "- **Simplicity**: easy to implement and interpret\n",
    "- **Efficiency**: fast to train, even on large datasets\n",
    "- **Probabilistic output**: provides class probabilities, not just binary predictions\n",
    "- **Works with smaller datasets**: can be used when there’s a limited number of data points\n",
    "- **No strict assumptions**: Does not require normally-distributed features or residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63b03a-9a52-4617-b4be-af7146da1aa7",
   "metadata": {},
   "source": [
    "#### Weaknesses of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe397b6-21fd-4c7d-92b4-c7a5fae572fd",
   "metadata": {},
   "source": [
    "- **Linear decision boundary**: Struggles with complex, non-linear relationships\n",
    "- **Sensitive to outliers**: Outliers can distort the model\n",
    "- **Requires feature scaling**: Performs better when features are scaled or normalised\n",
    "- **Assumes independence**: Assumes that the features are independent of each other\n",
    "- **Limited to binary classification**: Extensions are needed for multiclass problems\n",
    "- **Not suitable for high-dimensional data**: Tends to overfit if there are too many features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16292060-453b-4efc-b272-662447d061f6",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d17032-b6db-42f9-a2c3-6933c7df02cc",
   "metadata": {},
   "source": [
    "[k-Nearest Neighbours]() (k-NN) is another simple classification algorithm.\n",
    "\n",
    "Unlike most supervised learning techniques, it doesn't have an explicit training phase.\n",
    "\n",
    "When asked to classify a new observation, k-NN works as follows.\n",
    "\n",
    "1. Compute the distance between the observation and all the observations in the training data. A number of distance metrics can be used, but Euclidean distance is the most common.\n",
    "\n",
    "$$\n",
    "d = \\sqrt{(x_{1} - x_{2})^{2} + (y_{1} - y_{2})^{2} + (z_{1} - z_{2})^{2} + \\cdots}\n",
    "$$\n",
    "\n",
    "2. Find the $k$ nearest neighbours---i.e. the $k$ points nearest to the observation. $k$ is an input to the algorithm and is critical to its performance.\n",
    "\n",
    "3. Vote on the class. Each of the $k$ neighbours votes on the class and the new observation is classified by the majority vote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70e32f-9bd6-46cf-a117-f1a7f315d54b",
   "metadata": {},
   "source": [
    "#### k-NN example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed1d2f-78eb-4255-b47a-2bd47307c9b5",
   "metadata": {},
   "source": [
    "Fit the k-NN model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c53061-6cf0-4504-8bf1-e7ec2fdb0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = 5\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "knn_classifier.fit(\n",
    "    loan_approval_feature_train_scaled_df,\n",
    "    loan_approval_target_train_df.get_column(\"loan_status\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd66488-2b76-4177-9851-165c2658c338",
   "metadata": {},
   "source": [
    "Show the predicted (majority vote) class for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ddd86-c81d-48fa-b468-999d39d53e87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_classifier.predict(loan_approval_feature_test_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27174ee-6320-4620-bc0f-abccc043f585",
   "metadata": {},
   "source": [
    "How does the model perform on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08bd68-bfaf-43bb-86f1-d48fd790e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        loan_approval_target_test_df.get_column(\"loan_status\"),\n",
    "        knn_classifier.predict(loan_approval_feature_test_scaled_df),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eddca6b-a7c7-4771-9dd5-73dc01ca97a3",
   "metadata": {},
   "source": [
    "The accuracy of this model is 88%. Tweaking $k$ would probably result in improvements, but k-NN is a relatively unsophisticated technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84389a0-1b74-47dd-914b-e79973b9e004",
   "metadata": {},
   "source": [
    "#### Benefits of k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f6a5b8-8414-4da9-a085-66408ee9ce28",
   "metadata": {},
   "source": [
    "- **Simplicity**: easy to understand and implement\n",
    "- **No assumptions**: no assumptions about the underlying data distribution, which makes it suitable for non-linear and complex data distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8794e54a-0356-4be8-a8c6-e43b2de4e491",
   "metadata": {},
   "source": [
    "#### Weakness of k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8ec48-d996-4614-bf3f-f18d677c3e62",
   "metadata": {},
   "source": [
    "- **Computationally expensive**: since it stores all the data and computes distances during prediction, it can be slow and memory-intensive for large datasets\n",
    "- **Curse of dimensionality**: as the number of features increases, the distance between points becomes less meaningful, requiring dimensionality reduction techniques or modifications to the algorithm\n",
    "- **Sensitive to imbalanced data**: if certain classes are overrepresented in the dataset, they might dominate the voting process, leading to biased predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18e438-be4e-46ac-873c-d05816184353",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a59b40-93d4-43ae-b694-e5122ec54a56",
   "metadata": {},
   "source": [
    "[Support Vector Machine (SVM) classifiers](https://www.ibm.com/topics/support-vector-machine) attempt to find a linear decision boundary that separates the classes. They differ from logistic regression classifiers in how they optimise their objectives.\n",
    "\n",
    "SVM classifiers maximise the margin---the distance between the decision boundary the the nearest data points (support vectors) for each class.\n",
    "\n",
    "Unlike logistic regression classifiers, SVM classifiers can handle non-linearly separable data. They achieve this by transforming the original data into a higher dimensional space where it becomes linearly separable. This is known as the kernel trick."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269fecd-74d7-4677-83e0-5f8d0eb805c1",
   "metadata": {},
   "source": [
    "#### SVM example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087f89a-a146-410c-a337-33af71bb52f4",
   "metadata": {},
   "source": [
    "Fit the SVM model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8dc67-0910-4faa-950a-b0653e87991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm_classifier = svm.SVC()\n",
    "\n",
    "svm_classifier.fit(\n",
    "    loan_approval_feature_train_scaled_df,\n",
    "    loan_approval_target_train_df.get_column(\"loan_status\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac91f68b-01a3-482d-bdf6-7be9c2036042",
   "metadata": {},
   "source": [
    "Show the predicted (majority vote) class for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f330ee3-056b-4455-8057-7cc21da4d564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_classifier.predict(loan_approval_feature_test_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6d103-bef9-4cd5-b0d6-303afe9bc88d",
   "metadata": {},
   "source": [
    "How does the model perform on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c8b3e-8531-4499-9f49-fde1138e2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        loan_approval_target_test_df.get_column(\"loan_status\"),\n",
    "        svm_classifier.predict(loan_approval_feature_test_scaled_df),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508ae35-fbbf-486b-a8d3-8c98dc332eb0",
   "metadata": {},
   "source": [
    "The accuracy of this model is 93%. We can tune the following hyperparameters in an attempt to increase this.\n",
    "\n",
    "- **Kernel**: linear, polynomial, radial basis function (RBF)\n",
    "- **Regularisation**: the penalty parameter \\(C\\) controls error tolerance\n",
    "- **Gamma**: controls the extent of overfitting (high values mean more overfitting to the training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e46554-f824-4e8d-8e81-af0d64c4a3d0",
   "metadata": {},
   "source": [
    "Note: In ML, [regularisation](https://www.ibm.com/topics/regularization) is used to prevent overfitting by adding a penalty term to the model's loss function, which discourages complex models by constraining the magnitude of the model's parameters. This helps improve generalisation to unseen data (i.e. reduce overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0718d-4909-4c9f-82c4-cd03f75e69f2",
   "metadata": {},
   "source": [
    "#### Benefits of SVM classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81bde559-8920-486e-8f3b-c6eb228190a9",
   "metadata": {},
   "source": [
    "- **Effective in high-dimensional spaces**: works for cases where the number of dimensions is greater than the number of samples\n",
    "- **Works well with non-linear boundaries**: kernel functions can transform the data to allow it to be separated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6beb6d-af07-4790-b83c-418c857555b1",
   "metadata": {},
   "source": [
    "#### Weakness of SVM classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea49b929-52f6-4939-93a7-8d17f7450702",
   "metadata": {},
   "source": [
    "- **Computationally expensive**: especially for non-linear kernels\n",
    "- **Complicated**: choosing the right kernel and tuning parameters can be challenging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89daec42-d897-4bf7-b071-80e336f36cfd",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53570fd-48b3-4c85-9064-1bc421f4668d",
   "metadata": {},
   "source": [
    "[Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) is a classification algorithm based on [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes'_theorem).\n",
    "\n",
    "Bayes' Theorem updates prior beliefs (probability distributions) based on new evidence (observations).\n",
    "\n",
    "So, the probability that a vector of values $X$ belongs to a class $C_{k}$ is\n",
    "\n",
    "$$\n",
    "P(C_{k}|X) = \\frac{P(X|C_{k})P(C_{k})}{P(X)}\n",
    "$$\n",
    "\n",
    "Or, to put it in words\n",
    "\n",
    "$$\n",
    "posterior = \\frac{prior \\times likelihood}{evidence}\n",
    "$$\n",
    "\n",
    "When comparing two classes, we have the same evidence, so the computation simplifies to\n",
    "\n",
    "$$\n",
    "P(X|C_{k})P(C_{k})\n",
    "$$\n",
    "\n",
    "The naive bit of a Naive Bayes' classifier reference to the assumption of independence between the features. This results in the following simplification.\n",
    "\n",
    "$$\n",
    "P(X|C_{k}) = P(x_{1}|C_{k}) P(x_{2}|C_{k}) \\cdots P(x_{n}|C_{k})\n",
    "$$\n",
    "\n",
    "This results in a _significant_ computational saving when working with the high-dimensional text data that Naive Bayes is often used to classify.\n",
    "\n",
    "Let's review how Naive Bayes might classify an e-mail as spam or ham.\n",
    "\n",
    "1. The training data is used to calculate $P(spam)$ and $P(ham)$.\n",
    "2. For each feature (e.g. significant word in corpus of e-mails), calculate $P(w_{i}|C_{k})$ for each class $k$\n",
    "3. Calculate $P(X|C_{k}) = \\prod_{i=1}^{n}{P(w_{i}|C_{k})}$\n",
    "4. Apply Bayes' Theorem by combining the prior and likelihood to calculate the posterior probabilities $P(C_{k}|X)$\n",
    "5. Choose the class with the highest posterior probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559902d-bad9-4a9d-ad71-77c17c5ecdb2",
   "metadata": {},
   "source": [
    "#### Naive Bayes example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82d02b2-d601-4014-86f7-d2a0346c3f64",
   "metadata": {},
   "source": [
    "The Gaussian Naive Bayes algorithm assumes the likelihood of the features to be normally distributed.\n",
    "\n",
    "Fit the Naive Bayes model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9daa43-ace5-46ae-8e36-e5cb2ae0d3e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "naive_bayes_classifier = GaussianNB()\n",
    "\n",
    "naive_bayes_classifier.fit(\n",
    "    loan_approval_feature_train_scaled_df,\n",
    "    loan_approval_target_train_df.get_column(\"loan_status\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab7035-5e8d-4d46-b20e-baa6021eeff0",
   "metadata": {},
   "source": [
    "Show the predicted (majority vote) class for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a415b-a8bd-4d6e-8bf2-b1cce2bb4d50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "naive_bayes_classifier.predict(loan_approval_feature_test_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff6a4a-84f5-4e3c-9b9d-b4f22688b80a",
   "metadata": {},
   "source": [
    "How does the model perform on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c13180-55cd-4e9b-8b62-58cc52ba2f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        loan_approval_target_test_df.get_column(\"loan_status\"),\n",
    "        naive_bayes_classifier.predict(loan_approval_feature_test_scaled_df),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca91a1-6529-4dfc-a609-b17466222bd3",
   "metadata": {},
   "source": [
    "The accuracy of this model is 92%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e89ae-7a3f-4741-a30e-193770ef9355",
   "metadata": {},
   "source": [
    "#### Benefits of Naive Bayes classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5ea4066-7cda-4dd8-82fa-ac9d1da58081",
   "metadata": {},
   "source": [
    "- **Simple and fast**: easy to implement and computationally efficient, even with large datasets\n",
    "- **Works well with small datasets**: can perform well with relatively small amounts of training data\n",
    "- **Handles categorical data well**: especially effective for text classification and natural language processing tasks\n",
    "- **Less sensitive to irrelevant features**: even with non-relevant features, the algorithm can still perform well\n",
    "- **Robust to noise**: handles noisy data reasonably well in many practical applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f71839-ccd5-4b5e-ae4f-fe6a9bdd7897",
   "metadata": {},
   "source": [
    "#### Weaknesses of Naive Bayes classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a841292-fce9-43cc-8087-b9fb7a08bec4",
   "metadata": {},
   "source": [
    "- **Strong independence assumption**: assumes all features are independent, which is often not true in real-world data\n",
    "- **Poor performance with correlated features**: fails when features are highly dependent or correlated\n",
    "- **Zero probability issue**: if a category in a feature was never observed in the training set, it assigns zero probability to that class (can be handled with techniques like smoothing)\n",
    "- **Limited to simple decision boundaries**: tends to work less well with complex datasets and decision boundaries compared to other algorithms like decision trees or SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b780f174-d784-4c14-9e00-efd3056749fa",
   "metadata": {},
   "source": [
    "### Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1244756a-7cc7-43cc-b9b4-41cb9b175823",
   "metadata": {},
   "source": [
    "Decision tree classifiers work by splitting classification tasks up into two (in the case of binary classifiers) simpler subtasks. They use a recursive divide-and-conquer approach to continually subdivide tasks until they become trivial.\n",
    "\n",
    "The output of training a decision tree classifier is a decision tree that is then used to classify observations.\n",
    "    \n",
    "The training steps are as follows.\n",
    "\n",
    "1. A root node is created that represents the entire dataset.\n",
    "2. The dataset is split on a feature value that provides the most information. Information can be measured in a number of ways, but [Gini impurity](https://victorzhou.com/blog/gini-impurity/) is a popular metric. Optimising based on information gain identifies the best place to split the data into two subgroups.\n",
    "3. The algorithm the recursively partitions each of the subgroups---looking for the optimal splits in each.\n",
    "4. The recursive partitioning stops when one of the following is true.\n",
    "    - All the observations in the partition are of the same class.\n",
    "    - The branch of the tree has reached a pre-defined depth limit (usually defined as a hyperparameter).\n",
    "    - There are less than a predefined number of points in the partition.\n",
    "    - There is no partitioning strategy that will improve the information gain.\n",
    "5. When a partition cannot be split further, the leaf node of the branch is assigned the majority label of all the points in the partition.\n",
    "6. To make a prediction, observations are subjected to the decisions (i.e. splits) defined by the the generated tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d676ac-d8b2-4bce-9fa4-12c4ab3ced7d",
   "metadata": {},
   "source": [
    "#### Decision tree example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b786f6-8a0e-4704-9a64-1fddfc2d098a",
   "metadata": {},
   "source": [
    "Fit the decision tree model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e2dc60-b80a-4997-9332-6ab6cdce0e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier(max_depth=3, random_state=SEED)\n",
    "\n",
    "decision_tree_classifier.fit(\n",
    "    loan_approval_feature_train_scaled_df,\n",
    "    loan_approval_target_train_df.get_column(\"loan_status\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e77bb7e-5178-4099-a706-e153fb375bd1",
   "metadata": {},
   "source": [
    "Review the generated decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b930b-c048-4123-904a-1e0e89335a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "loan_approval_class_names = sorted(loan_approval_df[\"loan_status\"].unique())\n",
    "\n",
    "plt.figure(figsize=(10, 10), dpi=300)\n",
    "plot_tree(\n",
    "    decision_tree_classifier,\n",
    "    feature_names=loan_approval_features_df.columns,\n",
    "    class_names=loan_approval_class_names,\n",
    "    filled=True,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a6d4f-e00c-4356-80f5-d67e0973dad0",
   "metadata": {},
   "source": [
    "We can see which features are most important to the classification process as they are closer to the root of the tree (e.g. CIBIL Score).\n",
    "\n",
    "This information is available from the fitted classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a9093-a2b1-4b44-bc93-582df577585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame(\n",
    "    {\n",
    "        \"feature\": loan_approval_feature_train_df.columns,\n",
    "        \"importance\": decision_tree_classifier.feature_importances_,\n",
    "    }\n",
    ").sort(\"importance\", descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61651563-d89a-46cc-8205-fe1674cb7313",
   "metadata": {},
   "source": [
    "Show the predicted class for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769a66f-b8b9-4d6b-a963-2b9c02a086d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_approval_feature_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb124b-9594-4097-b193-5e5005d22d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier.predict(loan_approval_feature_test_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99da81ec-1a36-40e7-b19d-0d13c5a5a66c",
   "metadata": {},
   "source": [
    "How does the model perform on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8d361e-261f-4b4e-8ea5-9274c48b7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        loan_approval_target_test_df.get_column(\"loan_status\"),\n",
    "        decision_tree_classifier.predict(loan_approval_feature_test_scaled_df),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66879c7d-0f86-4b3f-89e4-730defd0d7d1",
   "metadata": {},
   "source": [
    "The accuracy of this model is 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba79bb-eef5-4e63-ba12-9a0f4e601f3a",
   "metadata": {},
   "source": [
    "#### Benefits of decision tree classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f37bbe-f167-471d-b75f-bcca512a6644",
   "metadata": {},
   "source": [
    "- **Interpretability**: decision trees are easy to visualise and interpret\n",
    "- **Non-parametric**: no assumptions about the underlying data distribution\n",
    "- **Handling of non-linear relationships**: can model non-linear relationships between features and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a64cbd-aeb7-4735-9da5-43f941e4d3fe",
   "metadata": {},
   "source": [
    "#### Weaknesses of decision tree classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce20241-6a89-4ba4-b70f-215c84e70c90",
   "metadata": {},
   "source": [
    "- **Overfitting**: can easily become too complex and overfit the training data\n",
    "- **Instability**: small changes in the data can result in entirely different trees\n",
    "- **Bias**: can be biased towards features with more levels or categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b22b9-8e24-403d-b6c5-f0955738148c",
   "metadata": {},
   "source": [
    "### Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af552aea-b0f3-4dac-898e-7776b80d6b9b",
   "metadata": {},
   "source": [
    "Random forest classifiers combine many decision trees to improve classification accuracy and reduce overfitting.\n",
    "\n",
    "They are a type of ensemble model as they make use of multiple component models (the individual decision trees). Ensemble models have been shown to be effective in ML competitions.\n",
    "\n",
    "Each tree in the forest makes a prediction and the overall prediction is determined by majority vote.                                                               \n",
    "\n",
    "There might be thousands of trees in a random forest.\n",
    "    \n",
    "<img src=\"images/module1-random-forest.png\" alt=\"Random forest\" width=\"600px\" />\n",
    "\n",
    "The random forest produces the individual trees using a subset of the data.\n",
    "\n",
    "- Each tree is trained on a (bootstrapped) subset of the observations\n",
    "- At each split, a random subset of the features is considered\n",
    "\n",
    "This results in the trees seeing the data from different \"perspectives\", thus producing more robust predications.\n",
    "\n",
    "Having multiple trees making predictions allows errors in individual trees to be averaged out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eff551-c863-4fe0-8912-50e7932311ca",
   "metadata": {},
   "source": [
    "#### Random forest example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d7dca-0047-483c-a518-1543718c2ac3",
   "metadata": {},
   "source": [
    "Fit the random forest model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da3d18-d639-4638-88ba-21e5c93ede92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier(max_depth=3, random_state=SEED)\n",
    "\n",
    "random_forest_classifier.fit(\n",
    "    loan_approval_feature_train_scaled_df,\n",
    "    loan_approval_target_train_df.get_column(\"loan_status\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37bd28e-fcd1-42f9-b4b2-2e8f7ed1e74a",
   "metadata": {},
   "source": [
    "We can see which features are most important to the classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9942c-d14f-4c32-a1a0-1b236835ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame(\n",
    "    {\n",
    "        \"feature\": loan_approval_feature_train_df.columns,\n",
    "        \"importance\": random_forest_classifier.feature_importances_,\n",
    "    }\n",
    ").sort(\"importance\", descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90ec797-21b6-46ff-a06c-72bfbb7067a3",
   "metadata": {},
   "source": [
    "As with the decision tree, the CIBIL Score dominates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46d6c13-a233-490a-bd96-e81888644dff",
   "metadata": {},
   "source": [
    "Show the predicted (majority vote) class for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b436b-4d37-4846-87f7-76e30d4f91f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_forest_classifier.predict(loan_approval_feature_test_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a22fc-fdde-4d8e-85f7-0cddce4056ef",
   "metadata": {},
   "source": [
    "How does the model perform on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f74abd-97a9-4a48-b8cf-0a6496a57136",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        loan_approval_target_test_df.get_column(\"loan_status\"),\n",
    "        random_forest_classifier.predict(loan_approval_feature_test_scaled_df),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ace36-d61b-4126-bfbf-6a18898855ce",
   "metadata": {},
   "source": [
    "The accuracy of this model is 94%.\n",
    "\n",
    "Interestingly, the accuracy of the decision tree model was 95%.\n",
    "\n",
    "Remember that accuracy is a pretty rough metric. Also, 95% vs 94% may well be rounding errors. Also, the classification of this dataset appears to be significantly influenced by a single feature (i.e. CIBIL Score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a881feb7-1b8a-4db2-af5d-24cf26f8d44d",
   "metadata": {},
   "source": [
    "#### Benefits of random forest classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d814568b-54a2-4fcb-bd08-6047b3c89488",
   "metadata": {},
   "source": [
    "- **Accuracy**: random forest classifiers tend to be more accurate because they reduce overfitting by averaging the predictions from many trees\n",
    "- **Robustness**: less sensitive to noise in the training data compared to individual decision trees\n",
    "- **Feature importance**: provide a measure of the importance of each feature in making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852cd75-54ef-48cf-ae88-632948815709",
   "metadata": {},
   "source": [
    "#### Weaknesses of random forest classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df2daf-7c61-451d-bfd8-98816f942a8a",
   "metadata": {},
   "source": [
    "- **Interpretability**: the ensemble of trees can make it hard to explain the final decision\n",
    "- **Training time**: training many trees can be computationally expensive and time-consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18935107-88ca-4d38-be39-79cdc8d392eb",
   "metadata": {},
   "source": [
    "### Boosted trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b491b2-ec85-451e-9da5-706370192325",
   "metadata": {},
   "source": [
    "Boosted trees are another ensemble learning technique. They have been found to be very effective in ML competitions and represent the current state-of-the-art in tabular ML.\n",
    "\n",
    "The basic idea behind boosted trees is to combine multiple weak learners (typically decision trees) to form a strong predictive model. They can be used for both classification and regression.\n",
    "\n",
    "There are numerous boosted tree algorithms. Popular ones are\n",
    "\n",
    "- AdaBoost\n",
    "- Gradient Boosting\n",
    "    - XGBoost\n",
    "    - LightGBM\n",
    "    - CatBoost\n",
    "\n",
    "Weak learners are designed to perform slightly better than random guessing. They may be implemented with shallow trees.\n",
    "\n",
    "Boosting involve sequentially training a series of weak learners.  Each learner attempts to correct the mistakes made by the previous learners. The overall idea is that, by combining multiple weak learners, you get much better predictions.\n",
    "\n",
    "Having each individual tree be a _weak_ learner ensures that the insights produced by the model are distributed over a large number of tree, rather than been concentrated in a few.\n",
    "\n",
    "Boosted trees are trained as follows.\n",
    "\n",
    "1. The first decision tree is trained on the original dataset. Its predictions are used to generate the first set of residuals (errors), which are the differences between the predicted and actual target values.\n",
    "2. In each subsequent round, a new decision tree is trained. However, this new tree is not trained on the original dataset but on the residuals (the errors) of the previous model. The new tree focuses on correcting the errors made by the previous model by giving more attention (i.e., higher weight) to the instances that the previous models got wrong.\n",
    "3. The final prediction is obtained by combining the predictions from all the individual trees. The trees are added together, often weighted by their importance in reducing the error. \n",
    "    \n",
    "A learning rate can be provided, which controls how much each tree contributes to the overall prediction. Small learning rates slow down the learning, but allow for more fine tuning.\n",
    "\n",
    "AdaBoost adjusts the weights of misclassified instances in each round. Gradient Boosting focuses on reducing the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7c04c3-9b69-4457-afcb-aa58be9f06f2",
   "metadata": {},
   "source": [
    "#### Boosted tree example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42ff15-c612-4576-a73e-6f9794651532",
   "metadata": {},
   "source": [
    "Fit the boosted tree model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60149554-b06e-4b6a-b160-7e9aa34a7347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "boosted_tree_classifier = GradientBoostingClassifier(random_state=SEED)\n",
    "\n",
    "boosted_tree_classifier.fit(\n",
    "    loan_approval_feature_train_scaled_df,\n",
    "    loan_approval_target_train_df.get_column(\"loan_status\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ad455-fbcb-454d-909f-ec9d1ec0482f",
   "metadata": {},
   "source": [
    "As with decision trees, we can see which features are most important to the classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b3bb1c-a8ff-40dc-9152-cd55dd03e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame(\n",
    "    {\n",
    "        \"feature\": loan_approval_feature_train_df.columns,\n",
    "        \"importance\": boosted_tree_classifier.feature_importances_,\n",
    "    }\n",
    ").sort(\"importance\", descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d5fb5-cfeb-46ed-80b0-1ee6e2b22680",
   "metadata": {},
   "source": [
    "As with the decision tree, the CIBIL Score dominates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16abd3c-dc60-4a51-844f-fbfed34f333e",
   "metadata": {},
   "source": [
    "Show the predicted class for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cdd17-a3be-4aa4-ab25-25b765588a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boosted_tree_classifier.predict(loan_approval_feature_test_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4f360-3b23-4c67-88b4-c65497408f05",
   "metadata": {},
   "source": [
    "How does the model perform on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81725c-8ae1-4cf0-a6e6-8d6f1515adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        loan_approval_target_test_df.get_column(\"loan_status\"),\n",
    "        boosted_tree_classifier.predict(loan_approval_feature_test_scaled_df),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d221815-82cd-4b6a-bd24-5520e9e2dfbe",
   "metadata": {},
   "source": [
    "The accuracy of this model is 96%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80563833-6839-4257-919a-357b8c23eb63",
   "metadata": {},
   "source": [
    "#### Benefits of boosted tree classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4788f296-fb50-484d-9021-cfb15d2d355d",
   "metadata": {},
   "source": [
    "- **High predictive accuracy**: often outperform other models due to their ability to combine weak learners\n",
    "- **Handles mixed data types**: process both numerical and categorical data without much preprocessing\n",
    "- **Captures complex relationships**: excel in modelling non-linear interactions between features\n",
    "- **Robust to overfitting**: with proper regularisation techniques, boosting algorithms effectively prevent overfitting\n",
    "- **Versatile for tasks**: can be applied to both classification and regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67849ecf-ccc8-4ac3-ae97-ad3da0e7e147",
   "metadata": {},
   "source": [
    "#### Weaknesses of boosted tree classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab7d69-3bf9-482f-ae2d-81a831ba4ea1",
   "metadata": {},
   "source": [
    "- **Sensitive to hyperparameters**: performance heavily depends on fine-tuning parameters like learning rate and tree depth\n",
    "- **Risk of overfitting**: without sufficient regularisation or when using too many trees, boosted models may overfit the training data\n",
    "- **Less interpretable**: understanding how boosted trees make decisions is complex\n",
    "- **Computationally expensive**: training, especially with many trees, requires significant time and resources\n",
    "- **Longer training times**: training takes longer than simpler approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a95a452-c461-47e9-8726-4fb4a23d230a",
   "metadata": {},
   "source": [
    "### Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde87a25-0ac8-40b0-a450-4eed29e98370",
   "metadata": {},
   "source": [
    "Neural Networks (NNs) are computational models inspired by the structure and function of the human brain. They consist of layers of interconnected \"neurons\" or nodes, which process and transmit information. Here’s a breakdown of how NNs work:\n",
    "\n",
    "1. **Neurons and layers**\n",
    "    - Neurons: The basic units in a neural network, akin to brain neurons. Each neuron receives input, processes it, and sends an output.\n",
    "    - Layers: ANNs are organised into layers:\n",
    "        - Input layer: Receives the raw data (e.g., images, text).\n",
    "        - Hidden layers: Intermediate layers where computations are performed. The more hidden layers, the deeper the network (hence the term \"deep learning\").\n",
    "        - Output layer: Produces the final prediction or classification (e.g., cat vs. dog in an image classification task).\n",
    "2. **Connections and weights**\n",
    "    - Neurons are connected by \"edges\" or \"weights,\" which determine the strength of connections between neurons. These weights are adjustable and are crucial in learning patterns from data.\n",
    "        - Weights: When information passes through a connection, it is multiplied by a weight. Initially, weights are random, but they are fine-tuned during training.\n",
    "        - Bias: An additional parameter added to each neuron’s output, helping the network make better predictions.\n",
    "3. **Activation function**\n",
    "    - Each neuron has an activation function that determines whether it should \"fire\" (send a signal). Common activation functions include:\n",
    "        - Step: Uses a threshold to map the input to 0 or 1, often used for binary classification.\n",
    "        - Sigmoid: Squashes the input to be between 0 and 1, often used for binary classification.\n",
    "        - ReLU (Rectified Linear Unit): Outputs zero if the input is negative, otherwise passes the input as is.\n",
    "        - Softmax: Converts outputs into probabilities for multi-class classification.\n",
    "4. **Forward propagation**\n",
    "    - Data flows forward through the network:\n",
    "\n",
    "        - Inputs are passed to the neurons in the input layer.\n",
    "        - Neurons in each layer process the inputs by multiplying them by weights, summing them, adding biases, and applying the activation function.\n",
    "        - This process repeats across all layers until the output layer produces a result (e.g., a prediction).\n",
    "5. **Loss function**\n",
    "    - The output is compared to the actual result (e.g., the correct label in supervised learning). The loss function measures how far the prediction is from the truth.\n",
    "        - Common loss functions include mean squared error (for regression tasks) and cross-entropy loss (for classification tasks).\n",
    "6. **Backpropagation and learning**\n",
    "    - After computing the loss, the network adjusts its weights to reduce errors. This is done through backpropagation, which involves:\n",
    "        - Calculating the gradient (the direction and magnitude of change needed) of the loss function with respect to each weight.\n",
    "        - Gradient descent is used to update the weights in small steps to minimise the loss.\n",
    "        - The learning rate determines how large these steps are; too high and the network won’t converge, too low and learning is slow.\n",
    "7. **Training process**\n",
    "    - The network undergoes a training phase where:\n",
    "        - Input data is passed through the network (forward propagation).\n",
    "        - The loss is calculated.\n",
    "        - Weights are adjusted via backpropagation. This process repeats over many iterations (called epochs), gradually improving the network's performance.\n",
    "8. **Generalisation**\n",
    "    - Once trained, the NN should be able to generalise, meaning it can correctly predict outputs for unseen data (e.g., recognising new images it wasn’t explicitly trained on).\n",
    "\n",
    "<img src=\"images/module1-ann.png\" alt=\"Artificial Neural Network\" width=\"600px\" />\n",
    "\n",
    "There are multiple _types_ of NN. The type described above is an **artificial neural network (ANN)**. This term is often used synonymously with NN, but, strictly, it's a type of NN.\n",
    "\n",
    "There are also:\n",
    "\n",
    "- **Convolutional neural networks (CNNs)** which are particularly effective in image recognition tasks. The ubiquity of these kinds of tasks, coupled with the difficulty of solving them using traditional analysis approaches, makes CNNs one of the most popular types of NN.\n",
    "- **GANs (generative adversarial networks)** which use two neural networks, pitching one against the other to improve their accuracy.\n",
    "- **Recurrent neural networks (RNNs)** which are able to process past and input data—i.e. they have memory. ANNs operate only on the current input.\n",
    "- **Transformers**, which are similar to RNNs are used with sequential data. However, transformers have great flexibility in how they traverse the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ec4e8-5fd6-456d-89da-b51d5f72aa82",
   "metadata": {},
   "source": [
    "#### NN example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0304044d-d831-4258-a315-f15e0dec71d8",
   "metadata": {},
   "source": [
    "Fit the NN model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6fd9b-38f6-4302-b483-0b77c705e1c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(11, 50, 2), max_iter=1_000, random_state=SEED\n",
    ")\n",
    "\n",
    "nn_classifier.fit(\n",
    "    loan_approval_feature_train_scaled_df,\n",
    "    loan_approval_target_train_df.get_column(\"loan_status\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b51e90-0707-49b8-bcaf-fab81ad42605",
   "metadata": {},
   "source": [
    "Show the predicted class for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc07326-4732-4a86-81e0-66d691d4b033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_classifier.predict(loan_approval_feature_test_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded547a-ec76-4ce5-a72b-41f949dd9708",
   "metadata": {},
   "source": [
    "How does the model perform on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b98032e-4979-4e1e-830c-de056edebad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        loan_approval_target_test_df.get_column(\"loan_status\"),\n",
    "        nn_classifier.predict(loan_approval_feature_test_scaled_df),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80276e-d121-4c25-b9e7-384757153762",
   "metadata": {},
   "source": [
    "The accuracy of this model is 96%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c6878-4527-4b0f-a16f-d6bf36e2edd6",
   "metadata": {},
   "source": [
    "#### Benefits of NN classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695076a0-2639-4942-b98f-feec0f60603a",
   "metadata": {},
   "source": [
    "- **Learn Complex Patterns**: can identify intricate, non-linear relationships in data, making them suitable for tasks like image recognition and speech processing\n",
    "- **Adaptability**: can be trained for a wide range of tasks and data types, from structured to unstructured, across various industries like healthcare, finance, and AI\n",
    "- **Feature learning**: automatically learn relevant features from raw data, reducing the need for manual feature engineering in fields like image and text processing\n",
    "- **Scalable**: performance improves with larger datasets, making them highly effective for big data applications\n",
    "- **Parallelisation**: training can be parallelised using GPUs, accelerating the process, especially for large-scale networks\n",
    "- **Handle high-dimensional data**: effective with complex, high-dimensional data such as images, videos, or large datasets with many features\n",
    "- **Good generalisation**: can generalise to unseen data and make accurate predictions in real-world applications after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20f4db-4460-4260-b6bc-b418ebb14457",
   "metadata": {},
   "source": [
    "#### Weaknesses of NN classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f4285-8610-4670-be89-72e666f8d440",
   "metadata": {},
   "source": [
    "- **Data hungry**: large datasets are required for optimal performance, and results suffer if data is limited or imbalanced\n",
    "- **Black boxes**: difficult to interpret, making it hard to understand or explain the decision-making process\n",
    "- **Computationally intensive**: training is resource-heavy, requiring significant computational power and time\n",
    "- **Prone to overfitting**: can memorise training data, performing poorly on unseen data.\n",
    "- **Require hyperparameter tuning**: choosing the right architecture and tuning parameters like learning rate, layers, and neurons often requires expertise and trial-and-error\n",
    "- **Sensitive to input quality**: rely on high-quality, well-prepared data; noisy or biased data can severely affect performance\n",
    "- **Limited transfer learning**: often need retraining when applied to different tasks\n",
    "- **Challenging debugging**: debugging and fine-tuning neural networks can be difficult and time-consuming\n",
    "- **Risk of bias**: can inadvertently learn and propagate biases present in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2483a981-0a58-4a5b-a8a8-a940d1a3d16a",
   "metadata": {},
   "source": [
    "## Demonstration of how decision tree classifiers work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc3bd5b-10ec-4b5f-8319-c4376364f4b4",
   "metadata": {},
   "source": [
    "Decision trees use a divide-and-conquer strategy to recursively subdivide the dataset into simpler sub-problems.\n",
    "\n",
    "The dataset is split where it will maximise the available information---e.g. minimise the Gini impurity.\n",
    "\n",
    "For a dataset with $k$ classes, the Gini impurity is calculated by\n",
    "\n",
    "$$\n",
    "    G = 1 - \\sum_{i=1}^{k}p_{i}^{2}\n",
    "$$\n",
    "\n",
    "$p_{i}$ is the probability of selecting an item of class $i$ (i.e. the proportion of that class in the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b408431-ab2c-44a5-af57-e2cb7df5f73f",
   "metadata": {},
   "source": [
    "The penguin dataset has three classes (i.e. species)--Adelie, Chinstrap and Gentoo.\n",
    "\n",
    "We'll simplify the example by talking the two most significant features---`flipper_length_mm` and `bill_length_mm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aaa1db-f6ac-4dd7-a8f0-70c09ea6e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_df = (\n",
    "    pl.read_csv(\"data/penguins.csv\")\n",
    "    .select([\"flipper_length_mm\", \"bill_length_mm\", \"species\"])\n",
    "    .drop_nulls()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90279b1-2bd8-4954-8e6d-6b0d0cb25e79",
   "metadata": {},
   "source": [
    "Define a function to calculate Gini impurity given groups (partitions) and classes (species)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef462308-9270-43f9-95bb-21f812fde0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini_impurity(groups, classes):\n",
    "    # Number of samples at split point\n",
    "    total_samples = sum([len(group) for group in groups])\n",
    "\n",
    "    # Initialize Gini score\n",
    "    gini_impurity = 0.0\n",
    "\n",
    "    # Calculate Gini index for each group (left and right)\n",
    "    for group in groups:\n",
    "        size = len(group)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "\n",
    "        score = 0.0\n",
    "        \n",
    "        # Calculate the score for each class\n",
    "        for class_ in classes:\n",
    "            proportion = [row[-1] for row in group].count(class_) / size\n",
    "            score += proportion ** 2\n",
    "\n",
    "        # Weight the group Gini score by its size\n",
    "        gini_impurity += (1.0 - score) * (size / total_samples)\n",
    "\n",
    "    return gini_impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5c566-4a02-4a49-bd1f-a2436dcc9cb1",
   "metadata": {},
   "source": [
    "Define a function to partition a dataset based on feature and split value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c835ed-d873-402d-8612-9c4f413aca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(df, feature_name, split_value):\n",
    "    left_df = df.filter(pl.col(feature_name) <= split_value)\n",
    "    right_df = df.filter(pl.col(feature_name) > split_value)\n",
    "\n",
    "    return left_df, right_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17230c3a-8cc4-47da-a014-8891ee9a5784",
   "metadata": {},
   "source": [
    "Visualise the Gini impurity at various splits along `flipper_length_mm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597bea25-5e6e-44f7-8dd5-3254421fa278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "classes = penguin_df.get_column(\"species\").unique().to_list()\n",
    "\n",
    "feature_values = sorted(penguin_df.get_column(\"flipper_length_mm\").unique().to_list())\n",
    "\n",
    "gini_impurities = {}\n",
    "\n",
    "for split_value in feature_values:\n",
    "    partitions = partition_data(penguin_df, \"flipper_length_mm\", split_value)\n",
    "\n",
    "    gini_impurities[split_value] = calculate_gini_impurity(\n",
    "        [partition.rows() for partition in partitions], classes\n",
    "    )\n",
    "\n",
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"flipper_length_mm\": gini_impurities.keys(),\n",
    "            \"gini_impurity\": gini_impurities.values(),\n",
    "        }\n",
    "    )\n",
    "    .plot.line(\n",
    "        x=alt.X(\"flipper_length_mm\", title=\"Flipper length (mm)\"),\n",
    "        y=alt.Y(\"gini_impurity\", title=\"Gini impurity\"),\n",
    "    )\n",
    "    .properties(width=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7536c73-74b7-4fb6-9c23-8e3f724bfff4",
   "metadata": {},
   "source": [
    "We can see the Gini impurity is minimised at $\\approx$ 206mm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b412953-1934-4a3f-b8d3-c287f825de47",
   "metadata": {},
   "source": [
    "Define a function to calculate the optimal split (across all features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea293c0-04f9-4567-86a7-fe954d0dd37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_split(df, target_column_name):\n",
    "    feature_column_names = df.drop(target_column_name).columns\n",
    "    classes = penguin_df.get_column(target_column_name).unique().to_list()\n",
    "\n",
    "    split = (None, None, 1)\n",
    "\n",
    "    for feature_column_name in feature_column_names:\n",
    "        feature_values = sorted(df.get_column(feature_column_name).unique().to_list())\n",
    "\n",
    "        for split_value in feature_values:\n",
    "            partitions = partition_data(df, feature_column_name, split_value)\n",
    "\n",
    "            gini_impurity = calculate_gini_impurity(\n",
    "                [partition.rows() for partition in partitions], classes\n",
    "            )\n",
    "\n",
    "            if gini_impurity < split[2]:\n",
    "                split = feature_column_name, split_value, gini_impurity\n",
    "\n",
    "    return split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0517bcd7-fb5c-484e-88d9-9838e4d8cb70",
   "metadata": {},
   "source": [
    "Define a function to recursively split the dataset (i.e. build the decision tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39955d41-4886-42db-b0a6-6fb86688afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_and_conquer(df, target_column_name, maximum_depth, depth=0):\n",
    "    split = calculate_split(df, \"species\")\n",
    "\n",
    "    if split[2] < 1e-6 or depth == 4:\n",
    "        print(f\"{'  ' *  depth}Terminated\")\n",
    "\n",
    "        return\n",
    "\n",
    "    print(f\"{'  ' *  depth}{split[0]} <= {split[1]}\")\n",
    "\n",
    "    left_df = df.filter(pl.col(split[0]) <= split[1])\n",
    "    right_df = df.filter(pl.col(split[0]) > split[1])\n",
    "\n",
    "    divide_and_conquer(left_df, target_column_name, maximum_depth, depth + 1)\n",
    "    divide_and_conquer(right_df, target_column_name, maximum_depth, depth + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a7096d-e0e7-4e84-aa10-854099daa6c4",
   "metadata": {},
   "source": [
    "Create a decision tree with a maximum depth of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6d17e-b711-4d80-8eaa-966b9945662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "divide_and_conquer(penguin_df, \"species\", maximum_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690b9375-e80f-4bb6-8d86-f21c97841e24",
   "metadata": {},
   "source": [
    "Plot the penguins and the splits for the first (left-most) branch of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c69b6-a9ae-467a-bdc8-72daf9a51ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=penguin_df.select(\"flipper_length_mm\").to_series(),\n",
    "    y=penguin_df.select(\"bill_length_mm\").to_series(),\n",
    "    color=penguin_df.select(\"species\").to_series(),\n",
    "    labels={\n",
    "        \"x\": \"flipper length (mm)\",\n",
    "        \"y\": \"bill length (mm)\",\n",
    "        \"color\": \"species\",\n",
    "    },\n",
    "    color_discrete_sequence=px.colors.qualitative.Safe,\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    template=\"simple_white\",\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=12), selector=dict(mode=\"markers\"))\n",
    "\n",
    "fig.add_vline(x=206, line_width=3, line_dash=\"solid\")\n",
    "fig.add_hline(y=43.2, line_width=3, line_dash=\"dash\")\n",
    "fig.add_hline(y=42.3, line_width=3, line_dash=\"dot\")\n",
    "fig.add_hline(y=40.8, line_width=3, line_dash=\"dashdot\")\n",
    "\n",
    "fig.add_shape(\n",
    "    type=\"rect\",\n",
    "    x0=penguin_df.get_column(\"flipper_length_mm\").min(),\n",
    "    x1=206,\n",
    "    y0=penguin_df.get_column(\"bill_length_mm\").min(),\n",
    "    y1=40.8,\n",
    "    opacity=0.5,\n",
    "    line_width=0,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2fece-c209-4867-8105-eccbcb5ccc06",
   "metadata": {},
   "source": [
    "## Hands-on example of using XGBoost to classify observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0998446f-6082-4baa-ba93-21a8d778e136",
   "metadata": {},
   "source": [
    "The Kaggle Lending Club Loan Dataset contains a variety of fields related to loans, borrowers, and their financial behaviour.\n",
    "\n",
    "The original dataset has $\\approx$ 2.5m loans in it. We've extracted a random sample of 250,000 loans to make it more manageable.\n",
    "\n",
    "Below is a breakdown of the key fields in this dataset, which are commonly used for analysis.\n",
    "\n",
    "1. Loan and Credit Information\n",
    "    - `loan_amnt`: The loan amount funded to the borrower.\n",
    "    - `funded_amnt`: The total amount that was actually funded to the borrower.\n",
    "    - `funded_amnt_inv`: The total amount that was funded by investors.\n",
    "    - `term`: The term of the loan, typically 36 or 60 months.\n",
    "    - `int_rate`: The interest rate on the loan.\n",
    "    - `installment`: The monthly payment owed by the borrower.\n",
    "    - `grade`: Lending Club assigned loan grade (A, B, C, etc.).\n",
    "    - `sub_grade`: Lending Club assigned sub-grade for finer granularity within a grade.\n",
    "    - `emp_length`: Length of employment in years, provided by the borrower.\n",
    "    - `home_ownership`: The borrower’s home ownership status (e.g., RENT, OWN, MORTGAGE).\n",
    "    - `annual_inc`: The borrower’s annual income.\n",
    "    - `verification_status`: Whether the borrower's income was verified (e.g. Verified, Source Verified, Not Verified).\n",
    "    - `fico_range_low`: Lower bound of the borrower’s FICO score range at the time of loan issuance.\n",
    "    - `fico_range_high`: Upper bound of the borrower’s FICO score range at the time of loan issuance.\n",
    "\n",
    "2. Loan Status and Performance\n",
    "    - `loan_status`: The current status of the loan (e.g., Fully Paid, Charged Off, Late).\n",
    "    - `pymnt_plan`: Indicates if there is a payment plan for the loan (Y/N).\n",
    "    - `purpose`: The reason the borrower is taking out the loan (e.g., debt_consolidation, credit_card, home_improvement).\n",
    "    - `title`: The loan title provided by the borrower.\n",
    "    - `dti`: Debt-to-income ratio, which is the borrower’s monthly debt payments divided by their monthly income.\n",
    "    - `delinq_2yrs`: The number of delinquencies in the past two years.\n",
    "    - `earliest_cr_line`: The date of the borrower’s earliest reported credit line.\n",
    "    - `inq_last_6mths`: The number of inquiries in the past 6 months.\n",
    "    - `mths_since_last_delinq`: Months since the borrower’s last delinquency.\n",
    "    - `open_acc`: The number of open credit lines in the borrower’s credit file.\n",
    "    - `pub_rec`: The number of derogatory public records.\n",
    "    - `revol_bal`: The borrower’s revolving balance (total credit card debt).\n",
    "    - `revol_util`: Revolving line utilisation rate, or the amount of credit used relative to credit available.\n",
    "    - `total_acc`: The total number of credit lines in the borrower’s credit file.\n",
    "\n",
    "3. Payment and Financial Metrics\n",
    "    - `out_prncp`: The remaining outstanding principal on the loan.\n",
    "    - `out_prncp_inv`: The remaining outstanding principal for investors.\n",
    "    - `total_pymnt`: The total amount paid by the borrower so far.\n",
    "    - `total_pymnt_inv`: The total payment received by investors.\n",
    "    - `total_rec_prncp`: The total principal received to date.\n",
    "    - `total_rec_int`: The total interest received to date.\n",
    "    - `total_rec_late_fee`: The total late fees received.\n",
    "    - `recoveries`: The total amount recovered from the borrower after the loan was charged off.\n",
    "    - `collection_recovery_fee`: Fees incurred while recovering charged-off loans.\n",
    "    - `last_pymnt_d`: The date of the borrower’s last payment.\n",
    "    - `last_pymnt_amnt`: The amount of the borrower’s last payment.\n",
    "\n",
    "4. Borrower Demographics and Other Information\n",
    "    - `addr_state`: The state where the borrower resides.\n",
    "    - `zip_code`: The first 3 digits of the borrower’s postal code.\n",
    "    - `policy_code`: Indicates the public policy governing the loan.\n",
    "    - `application_type`: Whether the loan application is individual or joint.\n",
    "    - `acc_now_delinq`: The number of accounts that are currently delinquent.\n",
    "    - `tot_coll_amt`: The total collection amounts ever owed by the borrower.\n",
    "    - `tot_cur_bal`: The borrower’s total current balance for all accounts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e716634-e6fc-4962-b281-f2ad02a25a37",
   "metadata": {},
   "source": [
    "We are going to build a model to predict whether a borrower will fully repay their loan. There's a `loan_status` column that we'll use to create a new target column called `fully_paid`.\n",
    "\n",
    "Ideally, we'd consult some experts to determine the best features to use to predict load repayment. We'll cover feature engineering in a subsequent module. For now, we'll pick some plausible sounding variables.\n",
    "\n",
    "- `loan_amnt`\n",
    "- `term`\n",
    "- `int_rate`\n",
    "- `grade` (using dummy variables)\n",
    "- `emp_length`\n",
    "- `home_owner` (derived from `home_ownership`)\n",
    "- `annual_inc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0daf0a-0221-4fe6-8f94-d14f07f2ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_raw_df = (\n",
    "    pl.read_parquet(\"data/lending-club-sample.parquet\")\n",
    "    .with_columns(\n",
    "        pl.col(\"home_ownership\").is_in([\"OWN\", \"MORTGAGE\"]).alias(\"home_owner\"),\n",
    "        (pl.col(\"loan_status\") == \"Fully Paid\").alias(\"fully_paid\"),\n",
    "        ((pl.col(\"fico_range_low\") + pl.col(\"fico_range_high\")) / 2).alias(\n",
    "            \"fico_average\"\n",
    "        ),\n",
    "    )\n",
    "    .select(\n",
    "        [\n",
    "            \"loan_amnt\",\n",
    "            \"term\",\n",
    "            \"int_rate\",\n",
    "            \"grade\",\n",
    "            \"emp_length\",\n",
    "            \"home_owner\",\n",
    "            \"annual_inc\",\n",
    "            \"fico_average\",\n",
    "            \"fully_paid\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "lending_club_df = lending_club_raw_df\n",
    "\n",
    "lending_club_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ae804-2c6d-493f-8a87-1164600bdaa4",
   "metadata": {},
   "source": [
    "Check if there are any null values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3bbc85-62fa-4275-98aa-ac9b39ae2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_df.null_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651ec36-289e-4db3-aaa1-225a619c9561",
   "metadata": {},
   "source": [
    "Drop any observations with null `emp_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd4595-864a-49ef-8ebd-6000afd48ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_df = lending_club_raw_df.filter(pl.col(\"emp_length\").is_not_null())\n",
    "\n",
    "lending_club_df.null_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e9ff7-76bd-4724-98b9-1dfd547e531e",
   "metadata": {},
   "source": [
    "For the ML model to be effective, we need a reasonable number of examples of each class we are predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3150c-2128-4b35-b029-5a8504d5f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_df.get_column(\"fully_paid\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8dd23-a73f-4852-9db9-a8e5b1405cf7",
   "metadata": {},
   "source": [
    "`grade` is a categorical field. We could encode it using dummy variables, but, as it's ordinal, let's map it to a linear value. This involves a number of implicit assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02365551-d101-4cf2-89f6-8dd05d477c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_df = lending_club_raw_df.filter(\n",
    "    pl.col(\"emp_length\").is_not_null()\n",
    ").with_columns(\n",
    "    pl.col(\"grade\")\n",
    "    .replace(\n",
    "        {\n",
    "            \"A\": 7,\n",
    "            \"B\": 6,\n",
    "            \"C\": 5,\n",
    "            \"D\": 4,\n",
    "            \"E\": 3,\n",
    "            \"F\": 2,\n",
    "            \"G\": 1,\n",
    "        }\n",
    "    )\n",
    "    .str.to_integer()\n",
    ")\n",
    "\n",
    "lending_club_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f2ff3d-1c22-424b-a93c-2d498e17c646",
   "metadata": {},
   "source": [
    "Split the data into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c63a56-af24-4681-ae22-1c8a2c734e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    lending_club_feature_train_df,\n",
    "    lending_club_feature_test_df,\n",
    "    lending_club_target_train_df,\n",
    "    lending_club_target_test_df,\n",
    ") = train_test_split(\n",
    "    lending_club_df.drop(\"fully_paid\"),\n",
    "    lending_club_df.select(\"fully_paid\"),\n",
    "    test_size=0.30,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "lending_club_feature_train_scaled = scaler.fit_transform(lending_club_feature_train_df)\n",
    "\n",
    "lending_club_feature_test_scaled = scaler.fit_transform(lending_club_feature_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ea110-31fd-462d-aa89-8fb2d3b6099a",
   "metadata": {},
   "source": [
    "Fit a \"stump\" classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc291f-1de5-4a6b-a90b-669cc54abe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "stump_classifier = xgb.XGBClassifier(max_depth=1, n_estimators=1, random_state=SEED)\n",
    "\n",
    "stump_classifier.fit(\n",
    "    lending_club_feature_train_scaled,\n",
    "    lending_club_target_train_df.get_column(\"fully_paid\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03eb10-7d40-47da-a3a2-857d61738022",
   "metadata": {},
   "source": [
    "How accurate is the stump classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dce8a0-4180-4481-a159-0c7d9459dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "stump_classifier.score(\n",
    "    lending_club_feature_test_scaled,\n",
    "    lending_club_target_test_df.get_column(\"fully_paid\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96af61c9-0b10-4479-a13a-1a6f4f90d0ef",
   "metadata": {},
   "source": [
    "So we'd hope to be able to beat this when we tune our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2179e8-0284-4ce4-aa42-32e335c4bec3",
   "metadata": {},
   "source": [
    "We can examine the tree. There's only one, as we set `n_estimators` to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef94b8-1634-45bd-b89e-448d2fc48d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stump_classifier.get_booster().feature_names = lending_club_feature_train_df.columns\n",
    "\n",
    "xgb.plot_tree(stump_classifier);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d7e3b-3b73-4f63-b64a-49ff2a016fd2",
   "metadata": {},
   "source": [
    "The leaf values are logit values. We can convert them to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca570f4-020d-4d13-92bd-e583dfa45190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def inverse_logit(p):\n",
    "    return math.exp(p) / (1 + math.exp(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c924163-75c1-4801-9d22-effd79be25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "(inverse_logit(0.0732165724), inverse_logit(-0.169875145))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f19a3-6b3b-4d6c-87f1-2afbb44057b4",
   "metadata": {},
   "source": [
    "Lower terms suggest that the loan is more likely to be paid back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366ecda-12f9-4713-b83c-97211ec62dbf",
   "metadata": {},
   "source": [
    "Create and train an a more sophisticated XGBoost classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dbe56a-7393-4585-83c0-5527a3286d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_classifier = xgb.XGBClassifier(max_depth=4, random_state=SEED)\n",
    "\n",
    "lending_club_classifier.fit(\n",
    "    lending_club_feature_train_scaled,\n",
    "    lending_club_target_train_df.get_column(\"fully_paid\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b72450a-e10c-49a9-b38d-55d122620139",
   "metadata": {},
   "source": [
    "Evaluate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b199cbb-93ad-4735-8fbb-1ccf46f86480",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        lending_club_target_test_df.get_column(\"fully_paid\"),\n",
    "        lending_club_classifier.predict(lending_club_feature_test_scaled),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8c590-13d5-49a9-970f-69dc0bf6009a",
   "metadata": {},
   "source": [
    "60% accuracy. Still pretty poor. As we get into real-world data, the answers don't just fall into our lap. When we look at worked examples, they often end up with high levels of accuracy, making it look like ML is a panacea. Building good models is an iterative process, with many blind alleys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c139b0-91f1-47e9-83d3-7c764bfc7244",
   "metadata": {},
   "source": [
    "Try using different features. Does changing the `max_depth` hyperparameter make any difference? What other changes might we consider to try and improve the quality of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
