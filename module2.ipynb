{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db925f26-f05b-4834-9557-aa0166db671d",
   "metadata": {},
   "source": [
    "# Module 2: Model evaluation and validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "579b0c32-439f-44b0-8852-0c0f056b7788",
   "metadata": {},
   "source": [
    "In this module, we cover\n",
    "\n",
    "- Training and test datasets\n",
    "- Evaluation metrics (e.g. accuracy, recall, precision, F1 score, AUC)\n",
    "- Cross-validation\n",
    "- Over and underfitting\n",
    "- Hyperparameters\n",
    "- Validation datasets\n",
    "- Model tuning\n",
    "- Hands-on example of evaluating and tuning an ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a23e1c-f26d-4b15-ae71-bda81a42d484",
   "metadata": {},
   "source": [
    "The [notebooks](https://github.com/decisionmechanics/lt541v) for the course are available on GitHub. Clone or download them to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e5e19-4414-42c2-93e3-5d7f8fc15c07",
   "metadata": {},
   "source": [
    "In this notebook, we make use of the following third-party packages.\n",
    "\n",
    "```bash\n",
    "pip install jupyterlab feature-engine hyperopt matplotlib scikit-learn scikit-plot scipy yellowbrick\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62622c70-23bc-473d-b3e3-c32e2280bf65",
   "metadata": {},
   "source": [
    "Examples will make use of data from Kaggle's [2018 Machine Learning and Data Science Survey](https://www.kaggle.com/datasets/kaggle/kaggle-survey-2018).\n",
    "\n",
    "We will use this data to try and predict whether someone is a data scientist or a software engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f0e51f-1ec9-4260-b81b-2e5c64a8d156",
   "metadata": {},
   "source": [
    "Some of the packages have deprecation warnings. These can be disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3631d7-ec91-4bf6-b8c6-55f0e3691b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f480f9-bc58-4efe-adaf-335d5c97634e",
   "metadata": {},
   "source": [
    "## Data preparation using pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef1c006-9164-48d1-a0a3-d4d44615b6ad",
   "metadata": {},
   "source": [
    "Import the survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c706eb-c63f-43ee-852d-11e35f6762ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import janitor.polars\n",
    "import polars as pl\n",
    "\n",
    "survey_raw_df = pl.read_csv(\"data/ml-ds-survey.csv\").clean_names()\n",
    "\n",
    "survey_raw_df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74900240-1bcb-4ce6-a0bf-bae709bc57bf",
   "metadata": {},
   "source": [
    "Looks like we have two header rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04f473-c7dc-425e-991a-eb59317691a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_raw_df = pl.read_csv(\n",
    "    \"data/ml-ds-survey.csv\", skip_rows_after_header=1, infer_schema_length=None\n",
    ").clean_names()\n",
    "\n",
    "survey_raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceb60ba-8cf8-41b5-b083-a1c4880aba6f",
   "metadata": {},
   "source": [
    "Create a function that reads the raw data, cleans it up and selects the desired features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6323c4-7961-4825-9233-f5669ea86666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    survey_raw_df = pl.read_csv(\n",
    "        \"data/ml-ds-survey.csv\", skip_rows_after_header=1, infer_schema_length=None\n",
    "    ).clean_names()\n",
    "\n",
    "    top_majors = survey_raw_df.head(n=3).get_column(\"q5\").to_list()\n",
    "\n",
    "    return (\n",
    "        survey_raw_df.rename(\n",
    "            {\n",
    "                \"q1\": \"gender\",\n",
    "                \"q3\": \"country\",\n",
    "                \"q2\": \"age\",\n",
    "                \"q4\": \"education\",\n",
    "                \"q8\": \"experience\",\n",
    "                \"q9\": \"compensation\",\n",
    "                \"q5\": \"major\",\n",
    "                \"q16_part_1\": \"python\",\n",
    "                \"q16_part_2\": \"r\",\n",
    "                \"q16_part_3\": \"sql\",\n",
    "                \"q6\": \"title\",\n",
    "            }\n",
    "        )\n",
    "        .select(\n",
    "            \"gender\",\n",
    "            \"country\",\n",
    "            pl.col(\"age\").str.slice(0, 2).cast(pl.Int8),\n",
    "            pl.col(\"education\")\n",
    "            .replace(\n",
    "                {\n",
    "                    \"No formal education past high school\": 12,\n",
    "                    \"Some college/university study without earning a bachelor’s degree\": 13,\n",
    "                    \"Bachelor’s degree\": 16,\n",
    "                    \"Master’s degree\": 18,\n",
    "                    \"Professional degree\": 19,\n",
    "                    \"Doctoral degree\": 20,\n",
    "                    \"I prefer not to answer\": None,\n",
    "                }\n",
    "            )\n",
    "            .cast(pl.Int8),\n",
    "            pl.when(pl.col(\"major\").is_in(top_majors))\n",
    "            .then(pl.col(\"major\"))\n",
    "            .otherwise(pl.lit(\"Other\")),\n",
    "            pl.col(\"experience\")\n",
    "            .str.replace(r\"\\s*\\+\", \"\")\n",
    "            .str.split(\"-\")\n",
    "            .list.first()\n",
    "            .cast(pl.Int8),\n",
    "            pl.col(\"compensation\")\n",
    "            .fill_null(0)\n",
    "            .replace(\n",
    "                {\n",
    "                    \"500,000+\": \"500\",\n",
    "                    \"I do not wish to disclose my approximate yearly compensation\": None,\n",
    "                }\n",
    "            )\n",
    "            .str.split(\"-\")\n",
    "            .list.first()\n",
    "            .cast(pl.Int32)\n",
    "            * 1000,\n",
    "            (pl.col(\"python\").fill_null(\"\") == \"Python\").cast(pl.Int8),\n",
    "            (pl.col(\"r\").fill_null(\"\") == \"R\").cast(pl.Int8),\n",
    "            (pl.col(\"sql\").fill_null(\"\") == \"SQL\").cast(pl.Int8),\n",
    "            \"title\",\n",
    "        )\n",
    "        .filter(\n",
    "            pl.col(\"country\").is_in([\"China\", \"India\", \"United States of America\"]),\n",
    "            pl.col(\"title\").is_in([\"Data Scientist\", \"Software Engineer\"]),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f90202-c1f5-4a87-87d7-57d6067dc524",
   "metadata": {},
   "source": [
    "If we haven't already loaded the dataset and stored it in parquet format, do that now. Otherwise, load the data from the parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2ef38-4c88-4aef-a340-21de2188e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isfile(\"temp/ml-ds-survey.parquet\"):\n",
    "    survey_df = load_data()\n",
    "\n",
    "    survey_df.write_parquet(\"temp/ml-ds-survey.parquet\")\n",
    "else:\n",
    "    survey_df = pl.read_parquet(\"temp/ml-ds-survey.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef38600-a1fa-4d95-b2e2-3e29119fa512",
   "metadata": {},
   "source": [
    "We've filtered the data to only include the three most populous countries (India, China and the USA). This ensures we have a reasonable number of samples for each country.\n",
    "\n",
    "We've also simplified our task to performing a binary classification, by including only data scientists and software engineers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0469bd-0461-4120-aae4-71d145c228f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine import encoding, imputation\n",
    "from sklearn import pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_pipeline = pipeline.Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"encode_categoricals\",\n",
    "            encoding.OneHotEncoder(\n",
    "                top_categories=5,\n",
    "                drop_last=True,\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "numeric_pipeline = pipeline.Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"imputate\",\n",
    "            imputation.MeanMedianImputer(imputation_method=\"median\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical_pipeline\", categorical_pipeline, [\"gender\", \"country\", \"major\"]),\n",
    "        (\"numeric_pipeline\", numeric_pipeline, [\"education\", \"experience\"]),\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "column_transformer.set_output(transform=\"polars\")\n",
    "\n",
    "data_preperation_pipeline = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"column_transformer\", column_transformer),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d7ecc9-6f30-4f49-9f6d-c95b04c7ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "display(data_preperation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec1468-4842-4d80-a198-563eee64ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = survey_df.select(\n",
    "    [\n",
    "        \"gender\",\n",
    "        \"country\",\n",
    "        \"age\",\n",
    "        \"education\",\n",
    "        \"major\",\n",
    "        \"experience\",\n",
    "        \"compensation\",\n",
    "        \"python\",\n",
    "        \"r\",\n",
    "        \"sql\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_df = survey_df.select(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e82ea-f1cd-4d25-bdee-77fd0ea65569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "SEED = 123\n",
    "\n",
    "X_train_df, X_test_df, y_train_df_, y_test_df_ = model_selection.train_test_split(\n",
    "    X_df, y_df, test_size=0.3, random_state=SEED, stratify=y_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac5e16-f88d-477f-9d9e-fc6de1ad5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_ = data_preperation_pipeline.fit_transform(X_train_df.to_pandas())\n",
    "\n",
    "X_train_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921ae74-6696-4612-a710-7af348643144",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df_ = data_preperation_pipeline.fit_transform(X_test_df.to_pandas())\n",
    "\n",
    "X_test_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b00d1c-63fa-4696-8831-8060fe6fffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "y_train = LabelEncoder().fit_transform(y_train_df_.get_column(\"title\"))\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37944dca-ca53-4ba0-9592-328072bce819",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = LabelEncoder().fit_transform(y_test_df_.get_column(\"title\"))\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110db6f-bbb5-45b8-9e5f-93b51681309c",
   "metadata": {},
   "source": [
    "## Classification using pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15bde6-cc9c-4e72-8966-7637b5a11f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_pipeline = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"column_transformer\", column_transformer),\n",
    "        (\"classifier\", XGBClassifier(random_state=SEED)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d62569d-85d4-4d6e-9f82-5d93fbd67c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipeline.fit(X_train_df.to_pandas(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429443d8-47a2-442c-848e-139a7a803719",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipeline.predict(X_test_df.to_pandas())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb6edd-9476-4ff8-8549-4582bac8a040",
   "metadata": {},
   "source": [
    "## Model quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2169c929-68fc-4e23-8e08-ca1cf760ed47",
   "metadata": {},
   "source": [
    "Before we can use a model we have to ask ourselves, \"Is it any good?\"\n",
    "\n",
    "We need some objective way of measuring the models. There are many different metrics for assessing model quality.\n",
    "\n",
    "We also need some idea of the baseline. Is our model better than the alternative? The alternative might be a naive approach (e.g. assuming the weather next week will be the same as today), or it might be another model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b354022-5a1d-4cf6-b755-a4993f46758b",
   "metadata": {},
   "source": [
    "## Training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d25dbf-06aa-4fe0-b644-58c2d3d3ef2f",
   "metadata": {},
   "source": [
    "We've been splitting our dataset into training and test partitions. This is to allow us to evaluate our models.\n",
    "\n",
    "When we train our models, we don't train them on the entire dataset. We keep some data aside, not letting the model see it, so we can test the performance of the model.\n",
    "\n",
    "The proportion of the observations set aside depends on factors like the amount of data available, the ML algorithm being used, and the problem under study. Typical proporitions are 50%, 30%, 25% and 10%.\n",
    "\n",
    "It's important that we don't have duplicate values that appear in both the training and test datasets. If we do, this means that the training phase gets to sneak a peak at the \"exam answers\", so its quality will be overstated when we apply it to our test data.\n",
    "\n",
    "We also need to make sure that the test and training datasets are representative of the full dataset.\n",
    "\n",
    "Using stratified sampling (via the `stratify` parameter) when we split the data will help to ensure this---especially when we have very unbalanced classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7abf2dc-1ffe-4f7b-bf44-a336bc58229f",
   "metadata": {},
   "source": [
    "Do the training and test dataset have a similar target composition to the original dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14358923-80bf-4c5e-b3e2-4092583d3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "(survey_df.get_column(\"title\").value_counts(normalize=True, sort=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ff95e-ed2b-4b39-aecf-b51993d06051",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"title\": y_train,\n",
    "        }\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"title\").replace_strict(\n",
    "            {\n",
    "                0: \"Data Scientist\",\n",
    "                1: \"Software Engineer\",\n",
    "            },\n",
    "            return_dtype=pl.String,\n",
    "        )\n",
    "    )\n",
    "    .get_column(\"title\")\n",
    "    .value_counts(normalize=True, sort=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff4ecd0-fee5-4f74-8bba-d6d8933a7347",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"title\": y_test,\n",
    "        }\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"title\").replace_strict(\n",
    "            {\n",
    "                0: \"Data Scientist\",\n",
    "                1: \"Software Engineer\",\n",
    "            },\n",
    "            return_dtype=pl.String,\n",
    "        )\n",
    "    )\n",
    "    .get_column(\"title\")\n",
    "    .value_counts(normalize=True, sort=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a30e6-2274-4b2a-87fb-59d18b08bb8a",
   "metadata": {},
   "source": [
    "Validation datasets will be introduced when we discuss hyperparameters in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024910ad-5cb4-4627-8793-142c28e0d654",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4251ae1-44a6-43c9-b857-84e91c5ac338",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ca95c-a13f-4bab-b94f-ff41c0b6684c",
   "metadata": {},
   "source": [
    "Accuracy is simply the proportion of correct predicitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e63553-9f37-4cbe-840a-17a42ae693b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = xgb_pipeline.predict(X_test_df.to_pandas())\n",
    "\n",
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"actual\": y_test,\n",
    "            \"predicted\": predicted,\n",
    "        }\n",
    "    )\n",
    "    .select(\n",
    "        (pl.col(\"actual\") == pl.col(\"predicted\")).alias(\"agreement\"),\n",
    "    )\n",
    "    .mean()\n",
    "    .item()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3044c48-6587-417e-bdb6-b632070c0893",
   "metadata": {},
   "source": [
    "It's available from the scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0cf4cd-17ce-493c-bbe6-24c845a700cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipeline.score(X_test_df.to_pandas(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88bb0e9-ea11-456e-9506-c47a8e8ad48f",
   "metadata": {},
   "source": [
    "#### Limitations of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768bc0b-5afe-4f30-8c7b-a4bcf504d918",
   "metadata": {},
   "source": [
    "Accuracy is an attractive evaluation metric as it's very simple. However, it has serious limitations.\n",
    "\n",
    "It assumes that the target classes are fairly well balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53823c43-28e7-45eb-a1cc-6b68c0004f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    survey_df.get_column(\"title\")\n",
    "    .value_counts()\n",
    "    .plot.arc(\n",
    "        theta=\"count\",\n",
    "        color=\"title\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb03b6-9155-47ee-952f-6d2182792a69",
   "metadata": {},
   "source": [
    "In the case of our survey data, this assumption holds---we have relatively equal numbers of data scientists and software engineers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d95f3-eed7-4bd3-add0-3b3166e06379",
   "metadata": {},
   "source": [
    "However, consider a rare disease---a disease that only occurs in 1% of the population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8658b2-79b8-4e8d-a3d9-ba4a25d61f1b",
   "metadata": {},
   "source": [
    "We'll create a dataset with four features (x1, x2, x3, x4) and a target (whether the person has the disease). \n",
    "\n",
    "Generate 1000 observtions. Use random numbers for the four features and set 99% of the targets to 0 (doesn’t have the disease) and 1% to 1 (does have the disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7140749-eb4f-4ac6-8cf3-8bc5440b4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "data = np.random.randint(0, 100, size=(1000, 4))\n",
    "\n",
    "rare_disease_df = pl.DataFrame(data, schema=[\"x1\", \"x2\", \"x3\", \"x4\"]).with_columns(\n",
    "    pl.Series(name=\"target\", values=[0] * 990 + [1] * 10)\n",
    ")\n",
    "\n",
    "rare_disease_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffac7a9-f327-4b4e-b809-2a06c1376330",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    rare_disease_df.get_column(\"target\")\n",
    "    .value_counts()\n",
    "    .plot.arc(\n",
    "        theta=\"count\",\n",
    "        color=\"target\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b31a6e-e839-421b-b4b8-21a42974bff0",
   "metadata": {},
   "source": [
    "There is _no_ predictive information in the four features---they are random values. They are completely unrelated to the target.\n",
    "\n",
    "Let's use them to predict the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e1054-6a75-4f92-b0e9-96f723787492",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    rare_disease_X_train_df,\n",
    "    rare_disease_X_test_df,\n",
    "    rare_disease_y_train_df,\n",
    "    rare_disease_y_test_df,\n",
    ") = model_selection.train_test_split(\n",
    "    rare_disease_df.select([\"x1\", \"x2\", \"x3\", \"x4\"]),\n",
    "    rare_disease_df.select(\"target\"),\n",
    "    test_size=0.3,\n",
    "    random_state=SEED,\n",
    "    stratify=rare_disease_df.select(\"target\"),\n",
    ")\n",
    "\n",
    "rare_disease_classifier = XGBClassifier().fit(\n",
    "    rare_disease_X_train_df.to_pandas(), rare_disease_y_train_df.get_column(\"target\")\n",
    ")\n",
    "\n",
    "rare_disease_classifier.score(\n",
    "    rare_disease_X_test_df.to_pandas(), rare_disease_y_test_df.get_column(\"target\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ac6c3-6a02-42ea-8507-28abb7bc09ab",
   "metadata": {},
   "source": [
    "That junk model is 99% accurate! Great! But...wait...how can it be?\n",
    "\n",
    "It's because we have unbalanced target classes. We can get the same results from a simple (baseline) model---i.e. no-one has the disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8490ba0-8adf-4b03-9f3f-f18a19ddb60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    rare_disease_y_test_df.rename(\n",
    "        {\n",
    "            \"target\": \"actual\",\n",
    "        }\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.lit(0).alias(\"predicted\"),\n",
    "    )\n",
    "    .select(\n",
    "        (pl.col(\"actual\") == pl.col(\"predicted\")).alias(\"agreement\"),\n",
    "    )\n",
    "    .mean()\n",
    "    .item()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0432b29-8054-4004-a6e6-d01dc9f58e0e",
   "metadata": {},
   "source": [
    "Accuracy can also be a poor metric when the classes are balanced. In classification tasks, we convert a predicted probability into a class. As accuracy is based on the predicted _class_, it doesn't consider the predicted _probability_.\n",
    "\n",
    "This means that a predicted probability of 51% carries the same weight as one of 99%. Quality metrics that can make use of the probability values provide a more nuanced assessment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b9692a-2616-4c70-be2e-b7e50510c068",
   "metadata": {},
   "source": [
    "Accuracy isn't a _useless_ metric---we use it throughout the course. It's easy for people to interpret. But you have to be careful when interpreting it, especially when you have unbalanced target classes.\n",
    "\n",
    "Always use a variety of quality metrics when assessing your models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aab061-dee3-4174-914c-244713ac822b",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ca698-369f-43dd-9f7e-0f588bff4b77",
   "metadata": {},
   "source": [
    "Confusion matrices are used to evaluate the performance of a classification model by displaying the counts of true positives, true negatives, false positives, and false negatives. They help visualise where the model is making accurate predictions and where it may be misclassifying, aiding in identifying patterns of errors.\n",
    "\n",
    "<img src=\"images/module2-confusion-matrix.png\" alt=\"Confusion matrix\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ef9d4-2423-4d6b-a7e5-a5da7b367ec5",
   "metadata": {},
   "source": [
    "The terms \"positive\" and \"negative\" don't have pejorative interpretations in ML. For example, a cancer diagnosis may be labelled as a positive outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1973c98-9f4b-476d-95d7-a4fcc9263b1e",
   "metadata": {},
   "source": [
    "We can count cells of the table using the `confusion_matrix` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae94464-5a2b-4ec6-bf77-742f10391f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa70ed-e2c0-4fb5-afa8-5343d0fc8c71",
   "metadata": {},
   "source": [
    "In this matrix,\n",
    "\n",
    "- the top-left is a correct prediction of Data Scientist (0)\n",
    "- the bottom-right is a correct prediction of Software Engineer (1)\n",
    "- the top-right is a incorrect prediction of Software Engineer\n",
    "- the bottom-left is a incorrect prediction of Data Scientist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edeb242-d063-4e94-a942-b9579045dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"actual\": y_test,\n",
    "            \"predicted\": predicted,\n",
    "        }\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.all().replace_strict(\n",
    "            {\n",
    "                0: \"Data Scientist\",\n",
    "                1: \"Software Engineer\",\n",
    "            },\n",
    "            return_dtype=pl.String,\n",
    "        )\n",
    "    )\n",
    "    .group_by(pl.all())\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .sort(\"count\", descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb532e3-3e62-449a-9b95-05a5ead6ae99",
   "metadata": {},
   "source": [
    "We can normalise the counts in the confusion matrix, which can make it easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e974d7-353e-4762-96d3-e8f5bc4f6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predicted, normalize=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab415d9b-5218-4794-b5be-eaac47f3f35d",
   "metadata": {},
   "source": [
    "It can be hard to remember what the columns and rows in a confusion matrix represent, so visualising it, with sensible labels, eases interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b62c8-e0e0-4c42-aa39-db2128f4aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(y_test, predicted, normalize=\"true\"),\n",
    "    display_labels=[\"DS\", \"SE\"],\n",
    ").plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3e7d6-d34d-46e6-a002-b9ef9e6a6843",
   "metadata": {},
   "source": [
    "The confusion matrix can provide insights into the _type_ of errors the model is making. There are problems in which false negatives are worse than false positives and vice versa.\n",
    "\n",
    "For example, if we define testing positive for cancer as a \"positive\" outcome, then we want to tune the model to prioritise false positives over false negatives. This will generally reduce the overall accuracy of the model, but makes sense in the context of the domain under study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44202562-d41e-4aef-9038-e63477ced852",
   "metadata": {},
   "source": [
    "Are false positives or false negatives more desirable in your industry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d61bfa5-90d3-443a-84be-ff738cafaca3",
   "metadata": {},
   "source": [
    "As with accuracy, confusion matrices are calculated using predicted classes, rather than probabilties, so the same concerns apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73180fd2-c04e-4283-a719-3735059a28a3",
   "metadata": {},
   "source": [
    "### Precision score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb2cd0-5374-48dc-ac4d-f0aa1b7e442e",
   "metadata": {},
   "source": [
    "Precision is the measure of the accuracy of positive predictions made by a model, calculated as the ratio of true positives to the sum of true positives and false positives. It reflects how often the model's positive predictions are actually correct, making it particularly important in scenarios where false positives need to be minimized.\n",
    "\n",
    "<img src=\"images/module2-precision-score.png\" alt=\"Precision score calculation\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acdbb8-f80d-408d-ab91-5d8d01000959",
   "metadata": {},
   "source": [
    "We can obtain a precision score using `precision_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02acf0-ec88-427b-87f6-049d0be96e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(y_test, predicted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4479759-c470-4a96-825e-a26bec0db3a8",
   "metadata": {},
   "source": [
    "Precision is \"How good are we at identifying X?\" It's a useful metric for investment models. You are willing to pass on a few \"too good to be true\" cars to reduce your chances of ending up with a lemon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf45a7c-c288-4e9c-bbf2-8f3befc6dd9b",
   "metadata": {},
   "source": [
    "Is precision a useful metric in your industry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9da851-45e8-4314-b500-00e05f0bfd6c",
   "metadata": {},
   "source": [
    "As with accuracy, precision scores are calculated using predicted classes, rather than probabilties, so the same concerns apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba765c-f7b7-4492-96d8-1e0f5e9831a8",
   "metadata": {},
   "source": [
    "### Recall score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a2f14-f086-448d-9cf6-8fb1d399703b",
   "metadata": {},
   "source": [
    "Recall (senstivity) is the measure of a model's ability to identify all relevant positive instances, calculated as the ratio of true positives to the sum of true positives and false negatives. It indicates how well the model captures actual positives, making it crucial in contexts where missing positive cases is costly.\n",
    "\n",
    "<img src=\"images/module2-recall-score.png\" alt=\"Recall score calculation\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f216ef-b3d7-4f8a-826c-a74c506046b1",
   "metadata": {},
   "source": [
    "We can obtain a recall score using `recall_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d72f79-47f3-441f-ae10-ea24ea45534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_score(y_test, predicted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b82a752d-6568-40da-bc3e-c793604f675b",
   "metadata": {},
   "source": [
    "Recall is \"How good are we at identifying _not_ X?\" It's a useful metric for a customer churn model. You are willing to overplease a few satisfied customers to make sure you don’t lose any."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc7e8a6-8394-46d2-912a-577a188a8668",
   "metadata": {},
   "source": [
    "Is recall a useful metric in your industry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f208a-8985-4e87-9a11-e8da8f8ff145",
   "metadata": {},
   "source": [
    "As with accuracy, recall scores are calculated using predicted classes, rather than probabilties, so the same concerns apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b9114-5fb6-4bba-af09-203f5a38c437",
   "metadata": {},
   "source": [
    "### F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291914a-37c5-4fd7-b04f-56a472317ec6",
   "metadata": {},
   "source": [
    "F1 scores combine precision and recall into a single metric. It differs from accuracy in being more robust where there are unbalanced classes.\n",
    "\n",
    "It's calculated by taking the harmonic mean of the precision and recall scores.\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc3454-fe61-4316-80c5-fa0621383035",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, predicted)\n",
    "recall = recall_score(y_test, predicted)\n",
    "\n",
    "2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1d2b6-9c44-4a11-b6ef-a895a4040f50",
   "metadata": {},
   "source": [
    "We can calculate this directly using `f1_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd45ab7-6e9b-4dd5-a45b-9eeb31521a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13283a40-e58d-4683-883b-16efae9c0c71",
   "metadata": {},
   "source": [
    "As with recall and precision, F1 scores are calculated using predicted classes, rather than probabilties, so the same concerns apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc5a3f-88a9-4d94-bce4-9e962f0e0e49",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8a338-f889-4ded-a05d-585abc871a3d",
   "metadata": {},
   "source": [
    "You can get accuracy, precision score, recall score, f1 score and other information from a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41323cd2-dcec-4585-a452-188132d01c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predicted, target_names=[\"DS\", \"SE\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7427b96-d7d7-4496-beb9-d7f73c275d7d",
   "metadata": {},
   "source": [
    "The macro average calculates the unweighted mean of metrics like precision, recall, and F1 score across all classes. This means each class contributes equally to the final average, regardless of its frequency in the dataset, making macro averages especially useful for understanding overall model performance in imbalanced datasets.\n",
    "\n",
    "The weighted average calculates the mean of metrics like precision, recall, and F1 score, but it weights each class's contribution according to its support (i.e. the number of true instances of each class). This approach provides a more balanced view that accounts for class distribution, making it helpful in accessing predictive accuracy in imbalanced datasets where more frequent classes might dominate the overall metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00c075b-40ff-49ed-acee-b6f9fc557415",
   "metadata": {},
   "source": [
    "### Receiver Operator Characteristic and Area Under Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf7a6e-f55c-4d04-ba6f-e80bda562ff7",
   "metadata": {},
   "source": [
    "Receiver Operator Characteristic (ROC) curves were use in the Second World War to assess radars. They allowed the study of tradeoffs between actual contacts and ghost images.\n",
    "\n",
    "Unlike the other metrics we've looked at so far, ROC curves consider the probability of predictions via the decision threshold (the point at which a probability is considered to move from negative to positive).\n",
    "\n",
    "An ROC curve is a graphical representation that shows the performance of a binary classifier across various threshold settings. It plots the true positive rate (recall) against the false positive rate, illustrating the trade-off between correctly identifying positives and avoiding false positives as the decision threshold changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6221a-ae1e-4cb6-9667-9a6a96f0608d",
   "metadata": {},
   "source": [
    "The true positive rate (TPR) is calculated as follows.\n",
    "\n",
    "$$\n",
    "TPR = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "The false positive rate (FPR) is calculated as follows.\n",
    "\n",
    "$$\n",
    "FPR = \\frac{FP}{FP + TN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308129ba-c420-4588-8e61-480045397786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "y_scores = xgb_pipeline.predict_proba(X_test_df.to_pandas())[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"ROC curve\")\n",
    "plt.plot(\n",
    "    [0, 1], [0, 1], \"k--\", label=\"Random guess\"\n",
    ")  # Dashed diagonal for random guessing\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a001a530-3ed1-41e7-9f93-82d227f5eff4",
   "metadata": {},
   "source": [
    "The ROC curve illustrates how the model is trading off identify true positives versus misclassifying false positives. Good models pull the curve towards the top-left of the chart.\n",
    "\n",
    "The chart lets us see how we can trade off reducing false positives with reducing the number of true positives we identify. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2abb44-f0f4-48bb-81ba-e57ef3a8875c",
   "metadata": {},
   "source": [
    "We can calculate the ROC curve manually to see how it's constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e5e33-4037-47c4-9b36-0e11e942bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_df = pl.DataFrame(\n",
    "    {\n",
    "        \"actual\": y_test,\n",
    "        \"threshold\": y_scores,\n",
    "    }\n",
    ")\n",
    "\n",
    "points = []\n",
    "\n",
    "for i in range(0, 1001):\n",
    "    decision_threshold = i / 1000\n",
    "\n",
    "    threshold_df = probability_df.with_columns(\n",
    "        pl.when(pl.col(\"threshold\") < decision_threshold)\n",
    "        .then(pl.lit(0))\n",
    "        .otherwise(pl.lit(1))\n",
    "        .alias(\"predicted\")\n",
    "    )\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(\n",
    "        y_test, threshold_df.get_column(\"predicted\")\n",
    "    ).ravel()\n",
    "\n",
    "    fpr = fp / (fp + tn)\n",
    "    tpr = tp / (tp + fn)\n",
    "\n",
    "    points.append((fpr, tpr))\n",
    "\n",
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"fpr\": [point[0] for point in points],\n",
    "            \"tpr\": [point[1] for point in points],\n",
    "        }\n",
    "    ).plot.point(\n",
    "        x=\"fpr\",\n",
    "        y=\"tpr\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d59d1-3ac4-4429-b3c6-9b6566ab5020",
   "metadata": {},
   "source": [
    "We can compute a single index from the ROC curve---Area Under Curve (AUC). It's pretty much as described---the area under the curve. As the limits of both axes of the chart are [0, 1], the AUC falls between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080712c-44c7-4ab3-933e-05bb15f5bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66c1085-f513-477d-811c-05daf12aba69",
   "metadata": {},
   "source": [
    "### Precision-Recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba461d9-ad32-48b1-8c3c-7f3c40f64a74",
   "metadata": {},
   "source": [
    "We can also plot the precision score against the recall score to examine the tradeoffs between these competing metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a450d9-4e94-48f8-9184-200082f1b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import precision_recall_curve\n",
    "\n",
    "precision_recall_curve(XGBClassifier(), X_train_df_, y_train, X_test_df_, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689de57-d22d-4a5a-b56f-7423c5e6e2f6",
   "metadata": {},
   "source": [
    "If we want to avoid false positives, we can try to improve the precision score, but we see that this will have a negative impact on our recall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c550198-3b91-4d57-bc87-4185d5adee52",
   "metadata": {},
   "source": [
    "### Log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e397e3e-b1c3-4855-8d28-e7d59a333459",
   "metadata": {},
   "source": [
    "Log loss quantifies the accuracy of a classifier by comparing the predicted probability $p_{i}$ for each instance with the actual label \n",
    "$y_{i}$.\tFor a binary classification problem, the formula for log loss is:\n",
    "\n",
    "$$\n",
    "Log\\ Loss = - \\frac{1}{N} \\sum_{i=1}^{N} (y_{i} log(p_{i}) + (1 - y_{i}) log(1 - p_{i}))\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "- $N$ is the number of observations\n",
    "- $y_{i}$ is the class label for observation $i$\n",
    "- $p_{i}$ is the predicted probability of observation $i$ being in the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06d8e3-c126-4895-bb32-4cfb20a25cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2479dd-5f24-4f53-b19f-2bbd670ad1a3",
   "metadata": {},
   "source": [
    "The value of log loss isn't very instructive on it's own. It's a relative metric. Better models have lower log loss values. A perfect classifier would have a log loss of 0.\n",
    "\n",
    "As log loss uses the _probabilities_ of the predictions, it utilises more information than class-based metrics. However, it is much less intuitive than most of the other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65539293-17e3-483c-97c9-80d2b3f624a5",
   "metadata": {},
   "source": [
    "### Cumulative gains curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e7cea1-af48-4c64-99ef-51032d8eecc8",
   "metadata": {},
   "source": [
    "Cumulative gains curves are used to assess how well a model distinguishes between positive and negative instances. \n",
    "\n",
    "In the context of marketing, for example, a cumulative gains curve helps determine how many customers are likely to respond to a campaign if targeted based on model predictions. \n",
    "    \n",
    "The curve plots the cumulative (ordered) proportion of positive outcomes (like responders) on the y-axis against the cumulative proportion of the population on the x-axis. A steeper curve indicates a stronger model, showing that a higher proportion of positives are captured within a smaller portion of the population.\n",
    "                                                                                                                                                                The random baseline reflects what would happen without the model, serving as a benchmark. An ideal model would have a high initial slope, capturing many positives early, and then flattening as the remaining instances become less likely to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7394adb-cca9-40a4-9b1c-2dcb9e0a0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_cumulative_gain\n",
    "\n",
    "plot_cumulative_gain(y_test, xgb_pipeline.predict_proba(X_test_df.to_pandas()));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6783d-6640-4c1a-ab80-af788f503514",
   "metadata": {},
   "source": [
    "Looking at the chart, the 40% of the population who are \"most obviously\" software engineers represent 60% of all instances of software engineers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4a0e7-b1a3-493d-b620-951e71b93057",
   "metadata": {},
   "source": [
    "### Lift curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85e04f-a35c-4a64-8e3a-37baf5c5d920",
   "metadata": {},
   "source": [
    "Lift curves show the improvement (or \"lift\") that a model provides over random selection.\n",
    "\n",
    "The curve plots the lift factor on the y-axis against the proportion of the population or sample on the x-axis (as with the cumulative gains curve). At any given point on the curve, the lift factor represents how much better the model is at capturing positive instances compared to a random baseline. \n",
    "\n",
    "For example, a lift of 1.8 at 5% of the population means that, in the top 5% of ranked predictions, the model captures 1.8 times as many positive cases as would be expected by random chance. A strong model will have a high lift at the beginning of the curve, showing it can concentrate positive outcomes in the top ranks, and the curve will typically decline as more of the population is included, eventually converging to a lift of 1, where it performs no better than random selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad044417-a13c-4007-ad9d-08d35a4214c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_lift_curve\n",
    "\n",
    "plot_lift_curve(y_test, xgb_pipeline.predict_proba(X_test_df.to_pandas()));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3219d-ff0d-4457-a46d-0e9ac9e49370",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40798e75-828e-4711-88d8-0248c2e6705f",
   "metadata": {},
   "source": [
    "Cross-validation allows us to assess how well a model generalises to an independent dataset. It helps to prevent overfitting. In cross-validation, the data is split into multiple subsets or \"folds,\" and the model is trained and evaluated on these folds.\n",
    "\n",
    "It works as follows.\n",
    "\n",
    "1. The data is divided into $k$ subsets (folds) of equal or near-equal size. Commonly, $k$ is set to 5 or 10.\n",
    "\n",
    "2. The model is trained on $k−1$ of the folds and tested on the remaining fold. This process is repeated $k$ times, each time using a different fold for testing and the rest for training.\n",
    "\n",
    "3. After all $k$ iterations, the evaluation metrics (like accuracy) are averaged to get an overall estimate of the model's performance.\n",
    "\n",
    "<img src=\"images/module2-cross-validation.png\" alt=\"Cross validation\" width=\"800\" />\n",
    "\n",
    "There are different forms of cross-validation.\n",
    "\n",
    "- $k$-Fold cross-validation is the Standard approach, where $k$ is the number of folds.\n",
    "- Leave-One-Out Cross-Validation (LOOCV) is a special case of $k$-fold where $k$ equals the number of data points. Each iteration uses all data points except one for training, with the single remaining point for testing.\n",
    "- Stratified $k$-Fold: Basically $k$-fold, but maintains the distribution of target variable classes across folds, which is useful for imbalanced datasets.\n",
    "\n",
    "Cross-validation provides a more robust measure of model performance compared to a single training-test split, as it ensures the model is evaluated across multiple subsets of the data. This leads to better insights into how the model will perform in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e906b39-5095-4508-b375-9432aa64a151",
   "metadata": {},
   "source": [
    "We can conduct a 5-fold validation on the survey pipeline, using F1 scores as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40d121-b114-4f60-b097-7634809e7f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "y = LabelEncoder().fit_transform(y_df.get_column(\"title\"))\n",
    "\n",
    "scores = cross_val_score(xgb_pipeline, X_df.to_pandas(), y, cv=5, scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b830efe4-f615-449c-9b09-bd20059e90a1",
   "metadata": {},
   "source": [
    "We have $k$=5 metrics---one for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92229063-1170-4df7-9951-3f362778ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca384c7f-4044-4658-8157-ec4155efa956",
   "metadata": {},
   "source": [
    "These can be combined as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519218b-a0fa-45a2-9328-63a8fe352364",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505aff2-0871-4483-a83f-bda2210bb418",
   "metadata": {},
   "source": [
    "## Over and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ebb2c2-112e-4e90-b94e-a33f9111822e",
   "metadata": {},
   "source": [
    "A learning curve visualisation is a graphical representation that shows the model's performance a function of the training data size. It helps track how well a model is learning and generalising by plotting metrics against the number of training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d535cc3-ce80-46f3-96e7-80f263473c10",
   "metadata": {},
   "source": [
    "<img src=\"images/module2-learning-curve.png\" alt=\"Model fitting\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b09e05d-24f3-44ce-8232-b6242d326cfb",
   "metadata": {},
   "source": [
    "When both the training and test curves have low accuracy, this indicates that the model is too simple (underfitting). If the training curve shows high accuracy, but the test curve has low accurary, this indicates overfitting.\n",
    "\n",
    "Good models tend to have both curves converging to a high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45610c5d-a75b-4f6f-8b55-e6665f986248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import learning_curve\n",
    "\n",
    "learning_curve(xgb_pipeline, X_df.to_pandas(), y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286cfc22-52a7-4994-add3-90ea6030f20e",
   "metadata": {},
   "source": [
    "If you are using a white box modelling approach, then a very complicated model can also be a sign of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8867ef-a9a8-427b-a223-a09c582930fa",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadacb56-2c27-4571-a9af-f3d2d7929123",
   "metadata": {},
   "source": [
    "Hyperparameters are settings that define how the model is trained rather than the values learned by the model itself. Unlike parameters (weights and biases) that are adjusted during the learning process, hyperparameters are set _before_ the training starts and remain fixed unless manually tuned.\n",
    "\n",
    "Hyperparameters are a key tool for controlling over and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72453197-4faa-4cf6-a4cf-7c8a8027801e",
   "metadata": {},
   "source": [
    "We can view all the hyperparameters for a Scikit Learn estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f53ca4-6881-4cb8-b6b9-61cbf1678661",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb5b3a-93d2-41ab-b603-ed9475bb57f1",
   "metadata": {},
   "source": [
    "Hyperparameters commonly employed to tweak XGBoost classifiers, include\n",
    "\n",
    "- `early_stopping_rounds`: Stop if we go this number of rounds without an improvement\n",
    "- `learning_rate`: After each boosting round, multiple weights by this value. (0, 1] range. Lower is more conservative and general requires more estimators. \n",
    "- `max_depth`: Maximum depth of each tree\n",
    "- `n_estimators`: Number of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f1573-5c08-4228-b80c-86265729cb01",
   "metadata": {},
   "source": [
    "We saw earlier that our model appeared to be overfitting. Let's restrict the depth of the trees to see if that helps.\n",
    "\n",
    "Storing hyperparameters in a dictionary makes it easy to group and reuse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2984d67-5c03-4af6-8375-9bedd5259a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\": 2,\n",
    "}\n",
    "\n",
    "classifier = XGBClassifier(**params)\n",
    "classifier.fit(X_train_df_, y_train)\n",
    "\n",
    "classifier.score(X_test_df_, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237cc1c-abd5-44b8-88dd-16d72754961b",
   "metadata": {},
   "source": [
    "That's an improvement over the default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6a112-f31e-41e0-9a68-f9ea8c0184c4",
   "metadata": {},
   "source": [
    "Review the learning curve for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ced245-8b4d-45de-8930-055cddf908cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_ = data_preperation_pipeline.fit_transform(X_df.to_pandas())\n",
    "\n",
    "learning_curve(classifier, X_df_.to_pandas(), y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6badb4-f23b-4246-9524-bf9303865f5f",
   "metadata": {},
   "source": [
    "Again, an improvement over the default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a0c9e-d812-484a-bf3e-77344a01a1ad",
   "metadata": {},
   "source": [
    "We can also look at a baseline model as a background to our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8865e8-bb8d-4de6-9551-f84d6574ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\": 1,\n",
    "    \"n_estimators\": 1,\n",
    "}\n",
    "\n",
    "classifier = XGBClassifier(**params)\n",
    "classifier.fit(X_train_df_, y_train)\n",
    "\n",
    "classifier.score(X_test_df_, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945931a-30da-4075-a1ee-c3a98115b8f3",
   "metadata": {},
   "source": [
    "Which feature is the classifier using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bbc1b1-5e1c-4230-b068-3bef38e97137",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X_test_df_.columns)[classifier.feature_importances_ == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6f06a-5994-4258-994a-f05f0e8b585b",
   "metadata": {},
   "source": [
    "## Validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417782e7-78f6-4417-9acf-789d88fc014d",
   "metadata": {},
   "source": [
    "When applying different hyperparameters, there's a risk of \"hyperparameter hacking\"---i.e. changing the parameters until you find a set that happen to work well with the specific test data you have. This will result in poor performance when applied to real-world data.\n",
    "\n",
    "To prevent this, we can split our data into train, test and _validation_ sets. Tuning is done against the validation data, with the final parameters being checked agains the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f837e81f-a661-4d1a-b541-74e6d475e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(\n",
    "    X_df, y_df, validate_size=0.25, test_size=0.25, random_state=None, stratify=None\n",
    "):\n",
    "    X_remainder_df, X_test_df, y_remainder_df, y_test_df = (\n",
    "        model_selection.train_test_split(\n",
    "            X_df,\n",
    "            y_df,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=stratify,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    X_train_df, X_validate_df, y_train_df, y_validate_df = (\n",
    "        model_selection.train_test_split(\n",
    "            X_remainder_df,\n",
    "            y_remainder_df,\n",
    "            test_size=validate_size / (1 - test_size),\n",
    "            random_state=random_state,\n",
    "            stratify=stratify,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return X_train_df, X_validate_df, X_test_df, y_train_df, y_validate_df, y_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab5b04-ea95-45e3-b7e2-b5ea422b8c1f",
   "metadata": {},
   "source": [
    "When using cross-validation, we don't need a validation dataset. If you have a small dataset, cross-validation will make better use of the data when performing model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e63161-e0e4-46d2-858b-f958e0edda2a",
   "metadata": {},
   "source": [
    "## Model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20072d5b-98a1-4b14-8581-f5e158879adb",
   "metadata": {},
   "source": [
    "Looking for signs of over or underfitting, and guessimating a better hyperparameter is very \"hit and miss\". It would help to have a more formal approach.\n",
    "\n",
    "We can _search_ the hyperparameter space using a number of different approaches.\n",
    "\n",
    "Note that we still need to use our judgement and intuition. Searching for hyperparameters is expensive, and we need to decide what, where and how to conduct the search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5975e84-9c7b-44a8-b226-c6f6b4f86f68",
   "metadata": {},
   "source": [
    "### Exhaustive grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d8198-9585-4926-9b52-4c7361c24cf6",
   "metadata": {},
   "source": [
    "What is the best value for `max_depth`? Setting it to 2 seemed to improve the model. But would 3 have been even better? Or 1? Or 5?\n",
    "\n",
    "We can have the computer try all the different values and pick the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67552325-2c8d-4e19-92ee-9b2b80cbc893",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "\n",
    "for max_depth in range(1, 11):\n",
    "    params = {\n",
    "        \"max_depth\": max_depth,\n",
    "    }\n",
    "\n",
    "    classifier = XGBClassifier(**params, random_state=SEED)\n",
    "    classifier.fit(X_train_df_, y_train)\n",
    "\n",
    "    scores[max_depth] = classifier.score(X_test_df_, y_test)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70933610-c038-4068-9811-76b0c410f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"max_depth\": scores.keys(),\n",
    "            \"score\": scores.values(),\n",
    "        }\n",
    "    ).plot.line(\n",
    "        x=\"max_depth\",\n",
    "        y=\"score\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669465c0-3c45-481f-a503-c67ced7ce351",
   "metadata": {},
   "source": [
    "We can see that the optimal maximum depth is 3, and we start to see overfitting beyond that.\n",
    "\n",
    "Note that I should strictly be using a validation set here, but we don't need it to demonstrate the concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3005e14-aebd-4bff-9b10-a3e188ce4717",
   "metadata": {},
   "source": [
    "But this is only one hyperparameter. What about the others?\n",
    "\n",
    "We can search through multiple parameter combinations using an exhaustive grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32780f0-e101-4a5b-b538-8e639d038140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.3, 0.5],\n",
    "    \"max_depth\": [1, 2, 3, 5, 10],\n",
    "    \"n_estimators\": [100, 500],\n",
    "}\n",
    "\n",
    "xgb_classifer = XGBClassifier(early_stopping_rounds=5, random_state=SEED)\n",
    "\n",
    "cv = GridSearchCV(xgb_classifer, param_grid, cv=5).fit(\n",
    "    X_train_df_, y_train, eval_set=[(X_test_df_, y_test)], verbose=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1804d-493f-4468-a416-dde3af71bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_, cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c0a57-0333-4fb3-a258-8c27292a1362",
   "metadata": {},
   "source": [
    "We can plug these values back into a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2f662-e238-4f14-9b10-aa72c00b2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier(**cv.best_params_, random_state=SEED).fit(X_train_df_, y_train).score(\n",
    "    X_test_df_, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8c613-5a81-49d9-a1ad-bb953a4b5f6b",
   "metadata": {},
   "source": [
    "The scores may not match as the grid search is using cross-validation, so the test datasets won't be identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7886c138-f9d8-4142-be90-f4430f033516",
   "metadata": {},
   "source": [
    "### Randomized parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6456a2-b00a-403e-9296-297beb984ed6",
   "metadata": {},
   "source": [
    "Exhaustive grid search is straightforward, but it can be computationally expensive when searching over a large parameter space.\n",
    "\n",
    "Randomized parameter optimization randomly samples hyperparameter candidates from distributions. This means that you aren't testing all combinations, but allows you to efficiently explore a wider parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d475c-bea8-49bd-a872-d64aaf3ee8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": uniform(),\n",
    "    \"max_depth\": list(range(1, 11)),\n",
    "    \"n_estimators\": list(range(50, 1000, 50)),\n",
    "}\n",
    "\n",
    "cv = RandomizedSearchCV(\n",
    "    xgb_classifer,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    n_iter=10,\n",
    "    random_state=SEED,\n",
    ").fit(X_train_df_, y_train, eval_set=[(X_test_df_, y_test)], verbose=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee8472-fabc-410e-903f-88c21b349813",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_, cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9523dae-a39c-4475-b46b-744add26fcf6",
   "metadata": {},
   "source": [
    "### Successive halving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f9e19-9207-4a32-bb8d-f3c1ae4575a3",
   "metadata": {},
   "source": [
    "Successive halving evaluates candidate parameter combinations using a limited number of resources (e.g. observations). It them takes a number of the winners from that round, and gives them more resources. This process continues until we have a single winner.\n",
    "\n",
    "The resource increase should be large enough so that improvements in scores outweigh differences due to statistical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e86ea8-ec2d-4fc5-ae54-9a31c40244c4",
   "metadata": {},
   "source": [
    "Scikit Learn has two successive halving estimators---[`HalvingGridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV) and [`HalvingRandomSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html#sklearn.model_selection.HalvingRandomSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1166d8-fcca-4fa8-80a2-8c34ed0efedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": uniform(),\n",
    "    \"max_depth\": list(range(1, 11)),\n",
    "    \"n_estimators\": list(range(50, 1000, 50)),\n",
    "}\n",
    "\n",
    "cv = HalvingRandomSearchCV(\n",
    "    xgb_classifer,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    random_state=SEED,\n",
    ").fit(X_train_df_, y_train, eval_set=[(X_test_df_, y_test)], verbose=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4dac6-f11f-4e62-bf2b-37408da1e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_, cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084cf5ae-a14f-4834-b312-21bf1148d64d",
   "metadata": {},
   "source": [
    "### Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47291fea-6383-410b-b031-385618e5b1ef",
   "metadata": {},
   "source": [
    "Hyperopt uses Bayesian optimisation to search the parameter space. Promising areas are explored in more detail. This makes it faster than other parameter search approaches, but it can get stuck in suboptimal areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0e7fe-4d9c-46be-b75e-114aa0bb40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials, fmin, hp, tpe\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def objective(space):\n",
    "    classifier = XGBClassifier(\n",
    "        learning_rate=space[\"learning_rate\"],\n",
    "        n_estimators=space[\"n_estimators\"],\n",
    "        max_depth=int(space[\"max_depth\"]),\n",
    "    )\n",
    "\n",
    "    classifier.fit(X_train_df_, y_train)\n",
    "\n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_train_df_, y=y_train, cv=5)\n",
    "\n",
    "    return 1 - accuracies.mean()\n",
    "\n",
    "\n",
    "space = {\n",
    "    \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.5, 0.01),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", range(2, 10, 1)),\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", range(20, 205, 5)),\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=10, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f315dee-beba-4f5d-9eb8-00bc32ca91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af037d-7a92-48e1-abfb-9b01e1b65dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier(**best).fit(X_train_df_, y_train).score(X_test_df_, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240de51-2bdd-4f7c-a36e-6b7d0e0d29c8",
   "metadata": {},
   "source": [
    "## Hands-on example of evaluating and tuning an ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7dbbd-f383-4a9c-810b-9cce47132194",
   "metadata": {},
   "source": [
    "In this hands-on section you will work with the Lending Club dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de80219-0671-4f42-a3ae-15ed6362a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_df = pl.read_parquet(\"data/lending-club-sample-preprocessed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f53a99b-b9e9-4c2b-9c37-5960c63ce288",
   "metadata": {},
   "source": [
    "You will\n",
    "\n",
    "- Review the different evaluation scores\n",
    "- Display a learning curve to see if the model is under or overfitting\n",
    "- Try different hyperparameters\n",
    "- Tune the model by searching the parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c9059-ff04-405c-aa86-4e257306089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(\n",
    "    lending_club_feature_train_df,\n",
    "    lending_club_feature_validate_df,\n",
    "    lending_club_feature_test_df,\n",
    "    lending_club_target_train_df,\n",
    "    lending_club_target_validate_df,\n",
    "    lending_club_target_test_df,\n",
    ") = train_validate_test_split(\n",
    "    lending_club_df.drop(\"fully_paid\"),\n",
    "    lending_club_df.select(\"fully_paid\"),\n",
    "    validate_size=0.10,\n",
    "    test_size=0.10,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "lc_X_train = scaler.fit_transform(lending_club_feature_train_df)\n",
    "\n",
    "lc_y_train = lending_club_target_train_df.get_column(\"fully_paid\")\n",
    "\n",
    "lc_X_validate = scaler.fit_transform(lending_club_feature_validate_df)\n",
    "\n",
    "lc_y_validate = lending_club_target_validate_df.get_column(\"fully_paid\")\n",
    "\n",
    "lc_X_test = scaler.fit_transform(lending_club_feature_test_df)\n",
    "\n",
    "lc_y_test = lending_club_target_test_df.get_column(\"fully_paid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef5dd7e-6fd4-408d-90aa-a1985a1a6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_classifier = XGBClassifier(random_state=SEED)\n",
    "\n",
    "lending_club_classifier.fit(\n",
    "    lc_X_train,\n",
    "    lc_y_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f76b0-31d0-4e65-a2e7-5f63178a8011",
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_classifier.score(lc_X_validate, lc_y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb20a0-70be-4d58-b223-d8497aab77ef",
   "metadata": {},
   "source": [
    "Examine some of the other evaluation metrics. How to do they compare to the accuracy score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb2c53-ae30-4e27-9c00-3cbd01f50a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab51c7c-ff1c-4984-bb07-1ff64163c2e5",
   "metadata": {},
   "source": [
    "What is a better metric for this model---precision or recall? Why?\n",
    "\n",
    "Is the model focusing on the best metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ecf85-9c6a-4089-83fe-9e30cd44241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the precision and recall metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b66a8c-54ca-4aa3-9c84-8e7d537a76c1",
   "metadata": {},
   "source": [
    "Review the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae931cc6-1e17-4f4c-9c18-04a32f2c3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9bf725-16da-4afe-a093-95da31c1394e",
   "metadata": {},
   "source": [
    "Review the learning curve for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec27e5-207e-4691-b1a3-37fdfbb57a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the learning curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ebd01-29b6-46e0-8d15-f7c9a5a4c4f7",
   "metadata": {},
   "source": [
    "Is the model over or underfitting or overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5147655-466b-4d06-beb1-32075293cad4",
   "metadata": {},
   "source": [
    "Review the XGBoost hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e08bb1-ac18-4e90-ba10-3e47f0ec06fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the estimator's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c097ddd-d5b2-4f37-98ff-fc68bb820baa",
   "metadata": {},
   "source": [
    "Train a new model, with a new hyperparameter, to improve the accurary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c336c56f-10e6-416b-a20f-f7c4dcb90049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new model, updating a hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815489e-66ee-4805-a438-6f4fcba11363",
   "metadata": {},
   "source": [
    "Did it improve the accurary score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457be0bb-5b7c-4e72-b148-bbf3c59612a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46dfe1c-a78a-4ad8-b7ee-d8432ddd7254",
   "metadata": {},
   "source": [
    "Perform an exhaustive grid search and obtain the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b596f3c9-e0ee-4248-aa8e-57ec4eb170b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an exhaustive grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705be0f-3065-40c3-8876-e80404d6a896",
   "metadata": {},
   "source": [
    "Did your search improve the accuracy score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d76ea-99de-45a5-8bef-d32117ca991b",
   "metadata": {},
   "source": [
    "Create a new model with these optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f7a01-59cc-4f5a-9c4a-e7dce390bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with the optimal parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599352e-ca8b-47b8-9ca0-2880e536563c",
   "metadata": {},
   "source": [
    "Evaluate this model using the _test_ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a28481-8723-4da9-aa94-f0515189015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate the new model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da9eed-90ee-484c-9d93-0c2bba1a942a",
   "metadata": {},
   "source": [
    "Did it perform well on the test data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
