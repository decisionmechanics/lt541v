{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96bee616-9012-4f7d-9183-d797563bd8a5",
   "metadata": {},
   "source": [
    "# Module 3: Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890332c-1980-423a-ab1a-a60d499ed568",
   "metadata": {},
   "source": [
    "In this module, we cover\n",
    "\n",
    "- The importance of data quality\n",
    "- Sources of error\n",
    "- Model interpretation\n",
    "- Feature transformation and scaling\n",
    "- Feature engineering\n",
    "- Feature selection\n",
    "- Handling imbalanced datasets\n",
    "- Modelling subgroups\n",
    "- Hands-on example of selecting features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f4969e-e89a-486b-90a1-4d73a89b6aad",
   "metadata": {},
   "source": [
    "The [notebooks](https://github.com/decisionmechanics/lt541v) for the course are available on GitHub. Clone or download them to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9b0ee-f4d9-4a6d-96c3-46606bbbcd6a",
   "metadata": {},
   "source": [
    "In this notebook, we make use of the following third-party packages.\n",
    "\n",
    "```bash\n",
    "pip install jupyterlab category-encoders imbalanced-learn numpy 'pandera[polars]' 'polars[all]' pyjanitor scipy seaborn xgbfir xgboost yellowbrick\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7635cb-4463-4147-af71-f3e5f4d0f21c",
   "metadata": {},
   "source": [
    "Some of the packages we use have deprecation warnings. These can be disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869558bf-df8a-4c07-bb44-4bc255a2a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e6aae-1522-4923-ad5d-9f57eea95891",
   "metadata": {},
   "source": [
    "## The importance of data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a164ab77-9792-45b1-884b-f858f19c2516",
   "metadata": {},
   "source": [
    "<img src=\"images/module3-gigo.png\" alt=\"GIGO\" width=\"500\" />\n",
    "\n",
    "ML is a data-driven tool. The quality of the data directly affects the performance of the model---garbage in, garbage out.\n",
    "\n",
    "Data quality impacts\n",
    "\n",
    "- **Model accuracy**: High-quality data enables the model to learn patterns accurately. Poor data quality, like missing values, noise, or biases, can lead to incorrect predictions and reduced model accuracy.\n",
    "\n",
    "- **Generalisation:** Clean and representative data helps models generalise well to new, unseen data. Poor quality data can lead to overfitting, where the model performs well on training data, but poorly on new data.\n",
    "\n",
    "- **Bias reduction**: If data is biased, the model will likely make biased decisions. Ensuring balanced and representative data reduces the chances of unfair outcomes, especially in areas like healthcare or hiring.\n",
    "\n",
    "- **Interpretability and trust**: High-quality data improves interpretability, helping stakeholders understand and trust model decisions. When data is noisy or inconsistent, model outputs are harder to explain and justify.\n",
    "\n",
    "- **Efficiency in training**: Good data quality reduces the need for complex preprocessing and error correction, speeding up model training and improving computational efficiency.\n",
    "\n",
    "- **Cost savings**: Detecting and fixing data issues early on prevents costly adjustments in later stages. Poor data quality can lead to incorrect results, requiring re-training and re-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bf2b9-a19e-4c2d-b9da-653ac5ce47b4",
   "metadata": {},
   "source": [
    "Data goes through a lifecycle, and quality must be maintained at every stage.\n",
    "\n",
    "<img src=\"images/module3-data-lifecycle.png\" alt=\"Data lifecycle\" width=\"200\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b60d39-fd8f-45ce-831c-d5cba5cae6d2",
   "metadata": {},
   "source": [
    "Data governance processes help raise the quality bar.\n",
    "\n",
    "<img src=\"images/module3-data-governance.png\" alt=\"Data governance\" width=\"200\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450dc64e-512b-48cb-8175-5700eadf1b73",
   "metadata": {},
   "source": [
    "What are some of the primary data sources in your organisation? How would you rate the quality of the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f07e551-5559-4204-a167-158c65f70128",
   "metadata": {},
   "source": [
    "### Data formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2aaad2-1ecd-43e0-9bbf-55de01b75ce9",
   "metadata": {},
   "source": [
    "Data will rarely be provided in a nice, tabular CSV file.\n",
    "\n",
    "We need to be able to convert to/from formats such as\n",
    "\n",
    "- Excel\n",
    "- JSON\n",
    "- JSON lines\n",
    "- Text\n",
    "- Relational databases\n",
    "- Non-relational databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c333ebd-8e8f-4f5a-bdf8-02959d89e378",
   "metadata": {},
   "source": [
    "## Sources of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a9997-fc1d-4f6c-90a6-0105a08cc538",
   "metadata": {},
   "source": [
    "Quality problems can occur from many sources.\n",
    "\n",
    "- **Data entry errors**: Human errors in manual data entry, such as typos or inconsistent formatting, can lead to incorrect values that skew model training.\n",
    "\n",
    "- **Incomplete data**: Missing values, which may occur due to data collection limitations or human error, can reduce the information available to the model and affect its performance.\n",
    "\n",
    "- **Outliers and noise**: Outliers, or values that deviate significantly from the norm, may be genuine but can also represent errors or anomalies in data. Noise, or random variations in data, can make it hard for the model to identify real patterns.\n",
    "\n",
    "- **Sampling bias**: If a dataset does not represent the underlying population accurately, the model will learn biases that do not generalise well. For instance, if a dataset has a demographic imbalance, predictions may be biased toward the over-represented group.\n",
    "\n",
    "- **Data drift**: Over time, data may change in its properties or distribution, especially in dynamic fields like finance or social media. Models trained on outdated data may perform poorly on new data.\n",
    "\n",
    "- **Measurement and instrumentation errors**: Errors in the tools or processes used to collect data, such as faulty sensors or inaccurate measurements, introduce inaccuracies that can lead to incorrect model predictions.\n",
    "\n",
    "- **Data duplication**: Duplicated data points can lead to over-representation of certain patterns, which can bias model training and result in overfitting.\n",
    "\n",
    "- **Labelling errors**: In supervised learning, incorrect labels can mislead the model during training. These errors often arise in tasks that require subjective judgment, such as image labelling or sentiment analysis.\n",
    "\n",
    "- **Inconsistent data formats**: In datasets that integrate information from multiple sources, inconsistent formats or units (like mixing metric and imperial measurements) can introduce errors.\n",
    "\n",
    "- **Human bias**: If data is collected in a way that reflects human biases, models trained on it may learn those biases. For example, biased language in text data can result in biased natural language models.\n",
    "\n",
    "What other sources of errors have you encountered?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd5048-f293-4427-989a-fdb81dfe8987",
   "metadata": {},
   "source": [
    "### Data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a60b51-6d8b-43d6-9d83-351389ced74e",
   "metadata": {},
   "source": [
    "We can formally validate our data according to a schema. Specifications such as [JSON Schema](https://json-schema.org) allow us to exert greater control over the quality of our datasets.\n",
    "\n",
    "[Pandera](https://pandera.readthedocs.io/en/stable/) is a library that allows us to perform data validation on dataframes, including Polars dataframes. Using Pandera, we can validate\n",
    "\n",
    "- column names\n",
    "- field types\n",
    "- non-nullablity\n",
    "- numeric limits (e.g. > 0)\n",
    "- numeric ranges\n",
    "- valid categorical values\n",
    "- value patterns (via regex)\n",
    "- column-wide statistics (e.g. median, standard deviation)\n",
    "- custom validation rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442705b4-6392-449f-b78e-45b8ccf64cc2",
   "metadata": {},
   "source": [
    "Let's create a simple schema for (partially) validating the shark incident dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593f63e-42d9-4559-91d4-59070b5991ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera.polars as pa\n",
    "\n",
    "\n",
    "class SharkIncidentSchema(pa.DataFrameModel):\n",
    "    uin: int\n",
    "    incident_year: int = pa.Field(in_range={\"min_value\": 1700, \"max_value\": 2024})\n",
    "    victim_injury: str = pa.Field(isin=[\"fatal\", \"injured\", \"uninjured\"])\n",
    "    latitude: float\n",
    "    shark_length_m: float = pa.Field(gt=0, nullable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f1f89-f98d-40ca-99fc-3f3aba9ddc3e",
   "metadata": {},
   "source": [
    "We can now using this schema to validate our data at ingress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf33cb3-4320-4d19-92ec-fccc42e5758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "try:\n",
    "    (\n",
    "        pl.scan_csv(\"data/shark-incidents.csv\", infer_schema_length=None)\n",
    "        .pipe(lambda df_: SharkIncidentSchema.validate(df_, lazy=True))\n",
    "        .collect()\n",
    "    )\n",
    "except pa.errors.SchemaErrors as e:\n",
    "    print(json.dumps(e.message, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6691ff-93b4-46e3-8d68-1c7975e8933c",
   "metadata": {},
   "source": [
    "Once we fix the problems with the data, our data will validate and be read in successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb04f3-8f5c-44c7-9de9-156a46bdc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import janitor.polars\n",
    "\n",
    "(\n",
    "    pl.read_csv(\"data/shark-incidents.csv\", infer_schema_length=None)\n",
    "    .clean_names()\n",
    "    .filter(\n",
    "        pl.col(\"uin\").is_not_null(),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"victim_injury\").replace(\n",
    "            {\n",
    "                \"Injured\": \"injured\",\n",
    "            }\n",
    "        ),\n",
    "        pl.col(\"latitude\").str.strip_chars().cast(pl.Float64),\n",
    "    )\n",
    "    .pipe(lambda df_: SharkIncidentSchema.validate(df_, lazy=True))\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c46ed-7d88-4244-9b49-18990e84a0cc",
   "metadata": {},
   "source": [
    "Creating a schema for your dataset can be considered part of the EDA process. It forces you to be explicit about the nature of your data. The schema provides a formal description of what assumptions you are making about the incoming data.\n",
    "\n",
    "And, if the assumptions are broken, it alerts you as the data drifts away from your expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c14650-c4e2-49c2-b322-8405a340a678",
   "metadata": {},
   "source": [
    "We can produce a more complete schema for validating the World Bank GDP dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911672aa-7465-4198-b4a3-c9414a3c6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GdpSchema(pa.DataFrameModel):\n",
    "    country_name: str = pa.Field(nullable=False)\n",
    "    country_code: str = pa.Field(str_length=3, nullable=False)\n",
    "    indicator_name: str = pa.Field(str_matches=r\"GDP \\(current US\\$\\)\", nullable=False)\n",
    "    indicator_code: str = pa.Field(str_matches=r\"NY\\.GDP\\.MKTP\\.CD\", nullable=False)\n",
    "    year: int = pa.Field(in_range={\"min_value\": 1960, \"max_value\": 2023})\n",
    "    gdp: float = pa.Field(gt=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0e645-fa65-41d5-9faa-5634c403a288",
   "metadata": {},
   "source": [
    "Is there anything missing from this schema? Could we make it tighter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f5042-929c-467c-8814-a268f3b84267",
   "metadata": {},
   "source": [
    "We can now validate the dataset as we injest it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa31979-704d-4710-842a-5023afa240de",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_csv(\n",
    "        \"data/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_31795.csv\",\n",
    "        skip_rows=4,\n",
    "    )\n",
    "    .clean_names()\n",
    "    .unpivot(\n",
    "        index=[\"country_name\", \"country_code\", \"indicator_name\", \"indicator_code\"],\n",
    "        variable_name=\"year\",\n",
    "        value_name=\"gdp\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"year\").replace(\"\", None),\n",
    "        pl.col(\"gdp\").replace(\"\", None),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"year\").cast(pl.Int64),\n",
    "        pl.col(\"gdp\").cast(pl.Float64),\n",
    "    )\n",
    "    .filter(pl.col(\"gdp\").is_not_null())\n",
    "    .pipe(lambda df_: GdpSchema.validate(df_, lazy=True))\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a8881-1d6e-4353-a470-cfc56f701a26",
   "metadata": {},
   "source": [
    "Take a dataset---one of your own, or one from the course---and define a set of constraints that should be applied to ensure it's valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eef65f-0feb-401e-b57b-c4f7988fb372",
   "metadata": {},
   "source": [
    "## Model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a608e0-bb7d-4c77-88c2-5eb79a4de2c2",
   "metadata": {},
   "source": [
    "White box models, such as logistic regression and decision trees, allow us to peer inside the model. We can review how it is using the features in making its predictions. This can give us insights that we can use to tweak the behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66a840-5937-4e0a-bd0d-084d78209b19",
   "metadata": {},
   "source": [
    "We'll use data from Kaggle's [2018 Machine Learning and Data Science Survey](https://www.kaggle.com/datasets/kaggle/kaggle-survey-2018) to see what insights the models can provide.\n",
    "\n",
    "The data will be used to try and predict whether someone is a data scientist or a software engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963832a-cdec-4a95-85a5-77e6efa7935f",
   "metadata": {},
   "source": [
    "Build a transformation pipeline to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d056bb0-ca9a-4d20-9d2e-2c1444718b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine import encoding, imputation\n",
    "from sklearn import pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_pipeline = pipeline.Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"encode_categoricals\",\n",
    "            encoding.OneHotEncoder(\n",
    "                top_categories=5,\n",
    "                drop_last=True,\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "numeric_pipeline = pipeline.Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"imputate\",\n",
    "            imputation.MeanMedianImputer(imputation_method=\"median\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical_pipeline\", categorical_pipeline, [\"gender\", \"country\", \"major\"]),\n",
    "        (\n",
    "            \"numeric_pipeline\",\n",
    "            numeric_pipeline,\n",
    "            [\"age\", \"education\", \"experience\", \"compensation\"],\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "column_transformer.set_output(transform=\"polars\")\n",
    "\n",
    "data_preperation_pipeline = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"column_transformer\", column_transformer),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e182ab19-6b20-4a1d-be1c-d3fddfbb5b95",
   "metadata": {},
   "source": [
    "Load the data and split it into features and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590cebea-f44e-469a-b576-71a7c3f64290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "survey_df = pl.read_parquet(\"data/ml-ds-survey.parquet\")\n",
    "\n",
    "X_df = survey_df.select(\n",
    "    [\n",
    "        \"gender\",\n",
    "        \"country\",\n",
    "        \"age\",\n",
    "        \"education\",\n",
    "        \"major\",\n",
    "        \"experience\",\n",
    "        \"compensation\",\n",
    "        \"python\",\n",
    "        \"r\",\n",
    "        \"sql\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_df = survey_df.select(\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e077c4-ffbc-424d-a264-313b40c6087b",
   "metadata": {},
   "source": [
    "Split the data into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813196d-4327-4c1d-8ffd-69cb9015646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "SEED = 123\n",
    "\n",
    "X_train_df, X_test_df, y_train_df_, y_test_df_ = model_selection.train_test_split(\n",
    "    X_df, y_df, test_size=0.3, random_state=SEED, stratify=y_df\n",
    ")\n",
    "\n",
    "X_train_df_ = data_preperation_pipeline.fit_transform(X_train_df.to_pandas())\n",
    "X_test_df_ = data_preperation_pipeline.fit_transform(X_test_df.to_pandas())\n",
    "y_train = LabelEncoder().fit_transform(y_train_df_.get_column(\"title\"))\n",
    "y_test = LabelEncoder().fit_transform(y_test_df_.get_column(\"title\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae847f54-40fa-4fce-8e3b-cd4247f57c5e",
   "metadata": {},
   "source": [
    "This data can now be used to train a variety of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d49b0ee-dd9e-46fd-8e33-406763f4bf00",
   "metadata": {},
   "source": [
    "### Logistic regression interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3a9a2-cfc8-4d0e-93ad-691c586b9f97",
   "metadata": {},
   "source": [
    "Logistic regression, like linear regression, finds coefficients for each feature. A weighted sum of the features is then used to make the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092ee53-e49f-412f-b238-b06a72176d6e",
   "metadata": {},
   "source": [
    "Build a job title classifier using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a044a21-2808-49e1-bce9-f33f11320584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "logistic_regression_classifier = LogisticRegression(penalty=None)\n",
    "logistic_regression_classifier.fit(scaler.fit_transform(X_train_df_), y_train)\n",
    "\n",
    "logistic_regression_classifier.score(scaler.transform(X_test_df_), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb2b0e3-c1a7-4df4-9e75-183ad139e039",
   "metadata": {},
   "source": [
    "How did the model weight the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12752e18-b137-4dc7-ad70-20a705ca2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_classifier.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b8992-0626-47c0-b3f6-e4940eb1d2ce",
   "metadata": {},
   "source": [
    "It's easier to view this graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c704b58-4df3-47e7-8a16-071ca28273d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"feature\": X_train_df_.columns,\n",
    "            \"coefficient\": logistic_regression_classifier.coef_[0],\n",
    "        }\n",
    "    )\n",
    "    .with_columns(pl.col(\"feature\").str.split(\"__\").list.last())\n",
    "    .sort(\"coefficient\", descending=True)\n",
    "    .plot.bar(\n",
    "        x=\"coefficient\",\n",
    "        y=alt.Y(\"feature\", sort=None),\n",
    "    )\n",
    "    .properties(\n",
    "        width=800,\n",
    "        height=500,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0121da7-79b9-43c9-8e5f-eed469e6a549",
   "metadata": {},
   "source": [
    "Features with positive coefficients offer more support for a prediction of software engineer. Those with negative coefficents push more to a prediction of data scientist.\n",
    "\n",
    "Years of experience and majoring in Computer Science are strong indicators that you are a software engineer. Data scientist is a relatively recent career path.\n",
    "\n",
    "R experience and level of education are strong predictors that you are a data scientist. Data scientists tend to need a broad range of technical skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a84cc7-8454-437c-83b8-990762de8bcb",
   "metadata": {},
   "source": [
    "Gender doesn't seem to play a particularly significant role. Maybe we could remove it from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df96fbc-dd68-43b8-9c40-05e8f799bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_classifier = LogisticRegression(penalty=None)\n",
    "logistic_regression_classifier.fit(\n",
    "    scaler.fit_transform(X_train_df_.select(pl.exclude(\"^.*gender.*$\"))), y_train\n",
    ")\n",
    "\n",
    "logistic_regression_classifier.score(\n",
    "    scaler.transform(X_test_df_.select(pl.exclude(\"^.*gender.*$\"))), y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6df996-bca5-4ff2-9fb0-68d39359dde1",
   "metadata": {},
   "source": [
    "This results in a slightly less accurate, but simpler, model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a1e60-23ee-4ac6-b41c-879a9ebe0a66",
   "metadata": {},
   "source": [
    "We can review the new coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb3264-cb53-4020-a3e9-b5190333137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"feature\": X_train_df_.select(pl.exclude(\"^.*gender.*$\")).columns,\n",
    "            \"coefficient\": logistic_regression_classifier.coef_[0],\n",
    "        }\n",
    "    )\n",
    "    .with_columns(pl.col(\"feature\").str.split(\"__\").list.last())\n",
    "    .sort(\"coefficient\", descending=True)\n",
    "    .plot.bar(\n",
    "        x=\"coefficient\",\n",
    "        y=alt.Y(\"feature\", sort=None),\n",
    "    )\n",
    "    .properties(\n",
    "        width=600,\n",
    "        height=400,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9451a-5e8b-493c-9c16-242c104b9a04",
   "metadata": {},
   "source": [
    "### Decision tree interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd53d8d-4782-4017-82a1-a2a8435961af",
   "metadata": {},
   "source": [
    "Decision trees are also white-box models---we can review how they are making their predictions. Let's apply a decision tree classifier to our job title problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77e613-302f-4f53-898d-bce5889f657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier(max_depth=4, random_state=SEED)\n",
    "decision_tree_classifier.fit(scaler.fit_transform(X_train_df_), y_train)\n",
    "\n",
    "decision_tree_classifier.score(scaler.transform(X_test_df_), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2f77eb-a9a7-49c7-97d2-323e546bbe5a",
   "metadata": {},
   "source": [
    "We can see what features the decision tree classifier considered to be important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b319084-c33b-471f-bec3-ce2e8f8d2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0187184f-a767-450e-ab3b-4949ed161946",
   "metadata": {},
   "source": [
    "Decision trees don't need to consider all the features. Some features may not be used at all in the partitioning process.\n",
    "\n",
    "This means that decision trees perform automatic feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f891003a-94b1-463f-8b0e-564c528c22c6",
   "metadata": {},
   "source": [
    "Again, it's easier to interpret the feature importances if we plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09810b-a262-4688-a2c6-a4e6689d4bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"feature\": X_train_df_.columns,\n",
    "            \"importance\": decision_tree_classifier.feature_importances_,\n",
    "        }\n",
    "    )\n",
    "    .filter(\n",
    "        pl.col(\"importance\") > 0,\n",
    "    )\n",
    "    .with_columns(pl.col(\"feature\").str.split(\"__\").list.last())\n",
    "    .sort(\"importance\", descending=True)\n",
    "    .plot.bar(\n",
    "        x=\"importance\",\n",
    "        y=alt.Y(\"feature\", sort=None),\n",
    "    )\n",
    "    .properties(\n",
    "        width=600,\n",
    "        height=400,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9eb1a-3270-4030-966c-ef914909bf1f",
   "metadata": {},
   "source": [
    "We don't have +ve and -ve feature importances, as with logistic regression coefficients, as decision trees can fit non-linear relationships. This means that features can support _both_ predictions in different parts of the tree.\n",
    "\n",
    "Decision trees may pick out different features that logistic regression, but the fact we see some correlation between the two models gives us some confidence that features such as R experience, college major, level of education and years of experience are probably useful indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8519e37c-8153-49a0-aea0-715260e7d465",
   "metadata": {},
   "source": [
    "We can visualise part of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345459eb-16a0-47d7-8724-31aee22e69de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "title_class_names = sorted(y_df[\"title\"].unique())\n",
    "\n",
    "plt.figure(figsize=(10, 10), dpi=300)\n",
    "plot_tree(\n",
    "    decision_tree_classifier,\n",
    "    feature_names=[column_name.split(\"__\")[-1] for column_name in X_train_df_.columns],\n",
    "    class_names=title_class_names,\n",
    "    filled=True,\n",
    "    max_depth=1,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1efa899-1d78-4001-89a9-351707d623a2",
   "metadata": {},
   "source": [
    "This aligns with the feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d41ad98-0a7f-4106-b635-490c8b6a66ba",
   "metadata": {},
   "source": [
    "### Random forest interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce41e5f-7647-496b-9191-200bdf326e4e",
   "metadata": {},
   "source": [
    "Random forests are _theorectically_ white-box models, but, in practice, they can be more challenging to interpret. Let's apply a random forest classifier to our job title problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71bdbd8-8dca-446c-8ae4-1f4e445f67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier(max_depth=4, random_state=SEED)\n",
    "random_forest_classifier.fit(scaler.fit_transform(X_train_df_), y_train)\n",
    "\n",
    "random_forest_classifier.score(scaler.transform(X_test_df_), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdca27c-7aee-4cbc-a64e-89d5f78e3584",
   "metadata": {},
   "source": [
    "We can examine the model's learning curve to see if it's under or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6212be-7798-4df3-822a-63a540e3967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import learning_curve\n",
    "\n",
    "learning_curve(random_forest_classifier, X_train_df_.to_pandas(), y_train_df_[\"title\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c8fdd-1887-4a87-9f64-b67f6905c571",
   "metadata": {},
   "source": [
    "How many trees are in our forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b76794-7f89-4164-96ba-ab0c91b82aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_forest_classifier.estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09468ee3-04ef-4f21-a219-abaccdcbbad9",
   "metadata": {},
   "source": [
    "Which features are important in the random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23835762-ac45-4b56-9141-3f4aa3158187",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "random_forest_classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9c053-1bc4-4d78-848e-14879e9f23e9",
   "metadata": {},
   "source": [
    "It's considering _all_ the features, which is suprising given what we saw with the simpler models. This can be an indication that there's overfitting.\n",
    "\n",
    "Let's plot the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adce0b-488d-4e20-85b3-d3246fa09b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"feature\": X_train_df_.columns,\n",
    "            \"importance\": random_forest_classifier.feature_importances_,\n",
    "        }\n",
    "    )\n",
    "    .filter(\n",
    "        pl.col(\"importance\") > 0,\n",
    "    )\n",
    "    .with_columns(pl.col(\"feature\").str.split(\"__\").list.last())\n",
    "    .sort(\"importance\", descending=True)\n",
    "    .plot.bar(\n",
    "        x=\"importance\",\n",
    "        y=alt.Y(\"feature\", sort=None),\n",
    "    )\n",
    "    .properties(\n",
    "        width=600,\n",
    "        height=400,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c0af7-76e2-42d5-bf17-7375f8d00bdb",
   "metadata": {},
   "source": [
    "Again, we are seeing that R experience, college major, education and experience are all important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f8599-a4bb-43bc-b054-e069df3afbf3",
   "metadata": {},
   "source": [
    "We can review individual trees from the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c73f896-b8a1-4d1a-aa01-08c3162305a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "title_class_names = sorted(y_df[\"title\"].unique())\n",
    "\n",
    "plt.figure(figsize=(10, 10), dpi=300)\n",
    "plot_tree(\n",
    "    random_forest_classifier.estimators_[0],\n",
    "    feature_names=[column_name.split(\"__\")[-1] for column_name in X_train_df_.columns],\n",
    "    class_names=title_class_names,\n",
    "    filled=True,\n",
    "    max_depth=1,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c72c9-5204-4f54-8dc4-2b2109ff0f7c",
   "metadata": {},
   "source": [
    "This is of limited value as the model is the ensemble of all 100 trees. Looking at a single tree only provides a small part of the overall picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05981eda-d1a5-464a-812d-5a616867a2dc",
   "metadata": {},
   "source": [
    "### XGBoost interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85696b5a-4ac0-4d92-8f18-c57b648cb5fe",
   "metadata": {},
   "source": [
    "As with random forests, XGBoost models are _theorectically_ white-box models, but, in practice, they are challenging to interpret. The idea of fitting chains of residuals is challenging to map to an intuitive interpretation---even more so that with random forests.\n",
    "\n",
    "Let's apply an XGBoost classifier to our job title problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501fa0b-e02e-4db6-9e11-30ed6fb1dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(max_depth=4, random_state=SEED)\n",
    "xgb_classifier.fit(scaler.fit_transform(X_train_df_), y_train)\n",
    "\n",
    "xgb_classifier.get_booster().feature_names = X_train_df_.columns\n",
    "\n",
    "xgb_classifier.score(scaler.transform(X_test_df_), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff5c79-a1fa-457b-8271-4d4390531017",
   "metadata": {},
   "source": [
    "The XGBoost model is less accurate than the random forest. Is there any indication that we are overfitting?\n",
    "\n",
    "Let's review the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f889a5a-3d70-4afe-ba59-dcb1f36f8475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_curve(xgb_classifier, X_train_df_.to_pandas(), y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c15ab0-9eac-4bce-bc6f-0b831a36a6a3",
   "metadata": {},
   "source": [
    "Which features are important in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cfa5fd-1360-4586-bd64-c79242725a5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68e529e-e122-4e0a-94d6-f098d057d93f",
   "metadata": {},
   "source": [
    "It's considering most of the features. The only one it's ignoring is people who prefer to self-describe their gender.\n",
    "\n",
    "Let's plot the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8dfb03-20b1-4836-8772-7615e31ce807",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "xgb.plot_importance(xgb_classifier, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49582ba1-65c8-47fa-ace1-db1f68a449cc",
   "metadata": {},
   "source": [
    "Here we are seeing that experience and education are still prominent, but compensation and age have risen to the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132941bf-baff-48a7-b229-73bc2e825841",
   "metadata": {},
   "source": [
    "We can review individual boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8175ce-f40e-4252-b207-ac73dfa4ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(48, 48))\n",
    "xgb.plot_tree(xgb_classifier, num_trees=0, rankdir=\"LR\", ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be881c26-5cc3-4314-9e19-3f244391a9cf",
   "metadata": {},
   "source": [
    "Another approach we can use to try and interpret an XGBoost model is to train a decision tree on it's predictions and then review the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9b82e-0a3c-43ec-8a72-9c1cc05789e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "surrogate_regressor = DecisionTreeRegressor(max_depth=4)\n",
    "surrogate_regressor.fit(\n",
    "    X_train_df_, xgb_classifier.predict_proba(scaler.fit_transform(X_train_df_))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0c01d-170d-45fd-a156-317d636d32c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"feature\": X_train_df_.columns,\n",
    "            \"importance\": surrogate_regressor.feature_importances_,\n",
    "        }\n",
    "    )\n",
    "    .filter(\n",
    "        pl.col(\"importance\") > 0,\n",
    "    )\n",
    "    .with_columns(pl.col(\"feature\").str.split(\"__\").list.last())\n",
    "    .sort(\"importance\", descending=True)\n",
    "    .plot.bar(\n",
    "        x=\"importance\",\n",
    "        y=alt.Y(\"feature\", sort=None),\n",
    "    )\n",
    "    .properties(\n",
    "        width=600,\n",
    "        height=400,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be32e1-8e9b-48c4-985b-5931e7793753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "title_class_names = sorted(y_df[\"title\"].unique())\n",
    "\n",
    "plt.figure(figsize=(10, 10), dpi=300)\n",
    "plot_tree(\n",
    "    surrogate_regressor,\n",
    "    feature_names=[column_name.split(\"__\")[-1] for column_name in X_train_df_.columns],\n",
    "    class_names=title_class_names,\n",
    "    filled=True,\n",
    "    max_depth=1,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0fc91-1189-4ff0-88a8-28b1e3ba5210",
   "metadata": {},
   "source": [
    "## Feature transformation and scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c3b4b-e07a-4895-b01f-3058b25ccc94",
   "metadata": {},
   "source": [
    "We can transform and scale individual features to improve performance, accuracy and, occasionally, interpretation.\n",
    "\n",
    "Popular approaches include\n",
    "\n",
    "- Z-scores\n",
    "- MinMax scaling\n",
    "- MaxAbs scaling\n",
    "- Robust scaling\n",
    "- Quantile transformer scaling\n",
    "- Power transformer scaling\n",
    "- Unit vector normalisation\n",
    "- Log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3276343-ee20-4c6f-aad8-7cdd1892814d",
   "metadata": {},
   "source": [
    "### Z-scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c4adf-b8a5-4cfd-90c6-01d50d96a329",
   "metadata": {},
   "source": [
    "Calculating z-scores is a popular scaling method. Z-scores have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Centering the data can be useful when interpreting the intercept in linear regression, for example. Scaling to unit standard deviation can also reduce the impact of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9e772d-df87-4598-b791-304dc6b6fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\"data/shark-incidents.parquet\")\n",
    "    .select(\n",
    "        (pl.col(\"shark_length_m\") - pl.col(\"shark_length_m\").mean())\n",
    "        / pl.col(\"shark_length_m\").std()\n",
    "    )\n",
    "    .filter(pl.col(\"shark_length_m\").is_not_null())\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e774ea-a1b0-4652-a360-b60d8db44bc7",
   "metadata": {},
   "source": [
    "### MinMax scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d7bd6b-778a-4abe-ba6c-2b9f90ed6352",
   "metadata": {},
   "source": [
    "MinMax scaling is popular as it's easy to explain. It's also useful if you have an algorithm that requires a specific range of values (such as 0-1) or if you want to preserve the original shape of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cafbed6-2ea9-465b-af22-54247c34a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\"data/shark-incidents.parquet\")\n",
    "    .select(\n",
    "        (pl.col(\"shark_length_m\") - pl.col(\"shark_length_m\").min())\n",
    "        / (pl.col(\"shark_length_m\").max() - pl.col(\"shark_length_m\").min())\n",
    "    )\n",
    "    .filter(pl.col(\"shark_length_m\").is_not_null())\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03382db-aca6-49f3-9a99-42412d4e4e27",
   "metadata": {},
   "source": [
    "### MaxAbs scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dea344-35b5-4ffd-8108-661db1dd387e",
   "metadata": {},
   "source": [
    "MaxAbs scaling scales the data into the range [-1, 1]. It's computationally efficient and is appropriate if you wish to preserve the signs of the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83cfa5d-55b6-44e2-8fe5-c8f8ecaf866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\"data/shark-incidents.parquet\")\n",
    "    .select(\n",
    "        pl.col(\"shark_length_m\") / pl.col(\"shark_length_m\").max().abs(),\n",
    "    )\n",
    "    .filter(pl.col(\"shark_length_m\").is_not_null())\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a41f2-b943-4521-9b9a-ce3d957c5bde",
   "metadata": {},
   "source": [
    "### Robust scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39553f-afe5-4b1a-8d6f-ac2e46cf0923",
   "metadata": {},
   "source": [
    "As robust scaling makes use of the IQR, it's robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42653068-2af1-4e97-90f2-f0acbe65d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\"data/shark-incidents.parquet\")\n",
    "    .select(\n",
    "        (pl.col(\"shark_length_m\") - pl.col(\"shark_length_m\").quantile(0.25))\n",
    "        / (\n",
    "            pl.col(\"shark_length_m\").quantile(0.75)\n",
    "            - pl.col(\"shark_length_m\").quantile(0.25)\n",
    "        )\n",
    "    )\n",
    "    .filter(pl.col(\"shark_length_m\").is_not_null())\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ca17b-276c-497b-b0cd-e658852166be",
   "metadata": {},
   "source": [
    "### Quantile transformer scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc0c8e-daf5-4733-bf8f-fc481ee513f5",
   "metadata": {},
   "source": [
    "Quantile scaling maps the original data to follow a specific distribution, typically a uniform or normal distribution. It is often used to reduce the impact of outliers and make data distributions more comparable by transforming data to match a chosen reference distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda87b15-c2f0-4f1b-b2df-d003cfe958d5",
   "metadata": {},
   "source": [
    "Examine the distribution of the distance from shore at which the incident occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896895d4-8934-48ff-a09c-cb0bec4d2eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\"data/shark-incidents.parquet\")\n",
    "    .filter(pl.col(\"distance_to_shore_m\").is_not_nan())\n",
    "    .get_column(\"distance_to_shore_m\")\n",
    "    .hist(bin_count=30)\n",
    "    .plot.bar(x=\"breakpoint\", y=\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af553600-1da6-48e1-8396-da5c985c777f",
   "metadata": {},
   "source": [
    "Compare that with the data after quantile scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8f22a-099e-44c9-ab8b-64db7f1ec42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "quantile_transformer = QuantileTransformer()\n",
    "\n",
    "(\n",
    "    pl.read_parquet(\"data/shark-incidents.parquet\")\n",
    "    .select(cs.numeric().exclude(([\"latitude\", \"longitude\"])))\n",
    "    .pipe(\n",
    "        lambda df_: pl.from_numpy(\n",
    "            quantile_transformer.fit_transform(df_),\n",
    "            schema=df_.columns,\n",
    "        )\n",
    "    )\n",
    "    .filter(pl.col(\"distance_to_shore_m\").is_not_nan())\n",
    "    .get_column(\"distance_to_shore_m\")\n",
    "    .hist(bin_count=30)\n",
    "    .plot.bar(x=\"breakpoint\", y=\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92428295-3f6b-4d9f-baf8-14eed717c1b8",
   "metadata": {},
   "source": [
    "### Log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e3c51-87f6-495a-8110-2750e2932315",
   "metadata": {},
   "source": [
    "Log transformations allow us to convert highly skewed distributions to more symmetric distributions.\n",
    "\n",
    "For example, financial data is usually highly right-skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17644669-c52b-45d4-9e66-afc9cd77eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_per_capita = (\n",
    "    pl.read_csv(\"data/world-bank-gdp.csv\")\n",
    "    .filter(\n",
    "        pl.col(\"year\") == 2023,\n",
    "        pl.col(\"gdp\").is_not_null(),\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"gdp\") / pl.col(\"population\")).alias(\"gdp_per_capita\"),\n",
    "    )\n",
    "    .get_column(\"gdp_per_capita\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f6595-98ea-49c2-bcae-12f3e2950dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    gdp_per_capita.hist(bin_count=15).plot.bar(\n",
    "        x=\"breakpoint\",\n",
    "        y=\"count\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c02c3b-e2f3-4f82-9d36-766d6ddfb1af",
   "metadata": {},
   "source": [
    "If we take the log of GDP/capita, we see a more symmetric distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30fd38f-74d7-41a3-889e-b400390764da",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    gdp_per_capita.log()\n",
    "    .hist(bin_count=15)\n",
    "    .plot.bar(\n",
    "        x=\"breakpoint\",\n",
    "        y=\"count\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b3a818-b5ff-470a-95e1-57fba265859d",
   "metadata": {},
   "source": [
    "This makes it easier to see patterns and can improve the performance of machine learning algorithms. The downside is that it's harder to interpret the numbers in the context of the original domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e1dbd-53a0-4700-a726-f988b23c5142",
   "metadata": {},
   "source": [
    "### Power transformer scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20da3ad-1e14-47dd-ac77-3d01c90f212b",
   "metadata": {},
   "source": [
    "Similarly to log transformations, power scaling is used to make data more normally distributed by applying a power-based transformation. This can help reduce skewness and stabilise variance in features, improving model performance for algorithms that are sensitive to the distribution of data.\n",
    "\n",
    "It's more flexible than simple log scaling.\n",
    "\n",
    "There are two common types of power transformations.\n",
    "\n",
    "- Box-Cox (only applies to positive values)\n",
    "- Yeo-Johnson (allows both positive and negative values)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7298727-8d26-4eb0-91a2-0098de6c424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "power_transformer = PowerTransformer()\n",
    "\n",
    "(\n",
    "    pl.read_parquet(\"data/shark-incidents.parquet\")\n",
    "    .select(cs.float().exclude(([\"latitude\", \"longitude\"])))\n",
    "    .pipe(\n",
    "        lambda df_: pl.from_numpy(\n",
    "            power_transformer.fit_transform(df_),\n",
    "            schema=df_.columns,\n",
    "        )\n",
    "    )\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d84f1d-545d-4071-aa6e-9e8543f63a9f",
   "metadata": {},
   "source": [
    "### Unit vector normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5776b808-a69f-4c9f-8355-7db298d847bc",
   "metadata": {},
   "source": [
    "Unit vector normalization converts a vector into a unit vector with a magnitude of 1 while preserving its direction. This is done by dividing each component of the vector by its magnitude, calculated as the square root of the sum of its components squared.\n",
    "\n",
    "This scaling approach is commonly using in algorithms that are sensitive to magnitude differences, such as k-NN or cosine similarity-based models (e.g. recommendation engines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2850542a-830e-470c-a7e0-9a480beae7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normaliser = Normalizer()\n",
    "\n",
    "normalized_df = (\n",
    "    pl.read_parquet(\"data/shark-incidents.parquet\")\n",
    "    .select(cs.float().exclude(([\"latitude\", \"longitude\"])))\n",
    "    .filter(~pl.any_horizontal(pl.all().is_null()))\n",
    "    .pipe(\n",
    "        lambda df_: pl.from_numpy(\n",
    "            normaliser.fit_transform(df_),\n",
    "            schema=df_.columns,\n",
    "        )\n",
    "    )\n",
    "    .head()\n",
    ")\n",
    "\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3f9de-402d-40f7-8295-73f9e5489320",
   "metadata": {},
   "outputs": [],
   "source": [
    "(normalized_df.select(pl.all() ** 2).sum_horizontal().alias(\"sum_of_squares\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a96bcf-377e-45b2-9213-484a5463c42d",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71569fcf-de40-449d-ac60-8305f9ca1126",
   "metadata": {},
   "source": [
    "Feature transformation and scaling are forms of feature engineering---i.e. the process of creating transforming, or extracting features from raw data to improve model performance.\n",
    "\n",
    "We engineer features to enhance their predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e68003-2598-4998-96c4-6f694bbe2533",
   "metadata": {},
   "source": [
    "### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f23c2a-92c1-4070-ab79-0d00ec9c445a",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a popular dimensionality reduction approach.\n",
    "\n",
    "We'll use it here to reduce the dimensionality of the [Palmer penguins dataset](https://allisonhorst.github.io/palmerpenguins/articles/intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6a286-6f96-4fa1-a34b-cf1bfc42752e",
   "metadata": {},
   "source": [
    "PCA cant handle missing values, so well just remove observations with missing data.\n",
    "\n",
    "Probabilistic PCA (PPCA) can be used when you have missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e3b25-6dd0-4230-897f-923711e63df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_df = pl.read_csv(\"data/penguins.csv\").drop_nulls()\n",
    "\n",
    "penguin_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8047901-51be-4d8a-bfe0-39eac0251eba",
   "metadata": {},
   "source": [
    "PCA only works with numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da802e-44e0-4ff5-bbdb-c0ff5dd55094",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_numeric_df = penguin_df.select(cs.numeric().exclude(\"year\"))\n",
    "\n",
    "penguin_numeric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047bd4e7-9f3a-4c13-89f9-8c0217045370",
   "metadata": {},
   "source": [
    "PCA is affected by scale, so we'll convert our dataset to z-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400942a-be67-45d9-8328-9df662a1739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "penguin_z_score_df = pl.from_numpy(zscore(penguin_numeric_df.to_numpy(), ddof=1)).pipe(\n",
    "    lambda df_: df_.rename(dict(zip(df_.columns, penguin_numeric_df.columns)))\n",
    ")\n",
    "\n",
    "penguin_z_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f22860-388e-494d-b608-3fe5c2e304fa",
   "metadata": {},
   "source": [
    "Transform the dataset to its principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80749169-4fd8-480b-92f0-50f566bc5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "principal_components = pca.fit_transform(penguin_z_score_df.to_numpy())\n",
    "\n",
    "principal_component_df = pl.concat(\n",
    "    [\n",
    "        penguin_df.select(\"species\"),\n",
    "        pl.from_numpy(\n",
    "            principal_components,\n",
    "            [\"component1\", \"component2\", \"component3\", \"component4\"],\n",
    "        ),\n",
    "    ],\n",
    "    how=\"horizontal\",\n",
    ")\n",
    "\n",
    "principal_component_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c4141-7410-4bbf-9696-42915f92a53a",
   "metadata": {},
   "source": [
    "How much of the variable is explained by each principal component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d8e2b-7981-4e22-8a7e-2534d69771a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b50f0-ba2a-4a34-80d5-d03fde56657b",
   "metadata": {},
   "source": [
    "We can see that 88% of the total variance is explained by the first two components.\n",
    "\n",
    "This lets us visualise the data as a two-dimensional scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be148af0-1d98-4304-93b7-e92a9b330931",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    principal_component_df.plot.point(\n",
    "        x=\"component1\", y=\"component2\", color=\"species\"\n",
    "    ).properties(width=500, height=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ff51d-c140-415f-ac4b-77cc758ff4a3",
   "metadata": {},
   "source": [
    "We can see that the Gentoos can be trivially identified, but seperating the Adelies from the Chinstraps is more challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b50d84-b6ba-4aef-b34b-8de83d9f2ac5",
   "metadata": {},
   "source": [
    "PCA isn't the only dimensionality reduction technique. There are alternative approaches.\n",
    "\n",
    "One that is popular in classification problems is Linear Discriminant Analysis (LDA). LDA is a supervised learning approach that takes the class labels into account when looking to isolate components with maximum variance.\n",
    "\n",
    "PCA focuses on capturing the most significant features in the data overall. LDA explicitly aims to improve the distinction between categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb26b9-c4c1-4dd9-b733-21434846ee52",
   "metadata": {},
   "source": [
    "Again, we'll transform the penguin dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81fa88-8974-4b7c-b6fc-6a481bd858c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pl.read_csv(\"data/penguins.csv\").drop([\"island\", \"sex\", \"year\"]).drop_nulls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325b097-51e7-4cf3-b243-aea20d7dfcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "penguin_df = (\n",
    "    pl.read_csv(\"data/penguins.csv\").drop([\"island\", \"sex\", \"year\"]).drop_nulls()\n",
    ")\n",
    "\n",
    "lda_df = pl.from_numpy(\n",
    "    LinearDiscriminantAnalysis(n_components=2).fit_transform(\n",
    "        penguin_df.drop(\"species\"), penguin_df.select(\"species\")\n",
    "    ),\n",
    "    schema=[\"component1\", \"component2\"],\n",
    ").with_columns(penguin_df.get_column(\"species\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a081e7-8c51-4575-a360-9af93244278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    lda_df.plot.point(x=\"component1\", y=\"component2\", color=\"species\").properties(\n",
    "        width=500, height=500\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e2811-dce7-4a20-8a91-e608a2a968e5",
   "metadata": {},
   "source": [
    "Compare this with the plot produced earlier using PCA. There appears to be better separation of the Adelies and Chinstraps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df720f9-6998-49b8-b2d1-ad6a74fe4cee",
   "metadata": {},
   "source": [
    "[Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD) is another popular dimensionality reduction technique. It tends to be used in textual analysis and recommandation systems. It's more computationally intensive then PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a88422-19a1-45b8-a28a-b5b11b605195",
   "metadata": {},
   "source": [
    "### Working with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b27dc84-1d29-491a-8120-fac5aa05fc93",
   "metadata": {},
   "source": [
    "Many popular ML techniques can't handle datasets with missing values. Even [those that can](https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values) will often handle missing values in ways that are unintuitive, thus complicating interpretation.\n",
    "\n",
    "The best approach to dealing with missing values is not to have any---i.e. improve the quality of your dataset. Of course, that's not always possible, but it's the first thing that should be considered. Are they missing values _really_ necessary, or is it a flaw in the data collection process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d727fd-5ec7-4920-82c0-6c681ede7fe7",
   "metadata": {},
   "source": [
    "The most common approach to dealing with missing values is to delete any observations where the data isn't complete. This is dangerous. Unless you are sure that missing values are randomly distributed within your dataset, this approach can bias your model.\n",
    "\n",
    "It's rare to find that missing values don't have a _systemic_ cause. Reasons for missing data include\n",
    "\n",
    "- Certain groups in a population may be less likely to respond\n",
    "- A piece of equipment may be broken\n",
    "- A data entry clerk may be untrained\n",
    "- Someone may have performed an incorrect merge\n",
    "- Data may not be available for certain days/times\n",
    "- Network or storage failures\n",
    "- Anonymisation\n",
    "\n",
    "Can you think of any other reasons why data might be missing? Think about some of the dataset you have been working with recently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58b47b5-6cf8-43ae-8083-6b7fd894a98b",
   "metadata": {},
   "source": [
    "So...if we can't avoid the missing data, and we can't remove observations without complete data, what _can_ we do?\n",
    "\n",
    "We have to replace the missing values with our best estimate of what the actual value might be. We need to impute values for those that are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e90cd-2593-4687-9c5b-cdccc3d2a506",
   "metadata": {},
   "source": [
    "Univariate imputation considers each variable _in isolation_ when considering how to impute missing values. This makes it easy to perform, but can result in values that don't make sense in the context of the entire observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00dff0-3f60-4222-9bf8-4f19f4e57f80",
   "metadata": {},
   "source": [
    "We can impute missing shark lengths in our shark incident dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d480e39-bb88-434d-8501-daecafaac0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_lengths = pl.read_parquet(\"data/shark-incidents.parquet\").get_column(\n",
    "    \"shark_length_m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d663920-57cb-4371-92a0-06f14b75554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_lengths.null_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033325c7-6e01-48a1-8026-95276e52efce",
   "metadata": {},
   "source": [
    "Impute a literal value (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9bab73-d664-48f2-88aa-e09cb41040c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    shark_lengths.fill_null(0)\n",
    "    .value_counts()\n",
    "    .plot.bar(\n",
    "        x=\"shark_length_m\",\n",
    "        y=\"count\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3699132-cc1b-4628-bc73-ca776b871244",
   "metadata": {},
   "source": [
    "Impute the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece9409-2aab-4c79-84aa-8d9b6c8ccac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    shark_lengths.fill_null(shark_lengths.max())\n",
    "    .value_counts()\n",
    "    .plot.bar(\n",
    "        x=\"shark_length_m\",\n",
    "        y=\"count\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82208956-b49c-464f-8cae-1ccf9abc00ac",
   "metadata": {},
   "source": [
    "Impute the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f41c3d-4fec-48fd-bc0d-bd155d70e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    shark_lengths.fill_null(shark_lengths.mean())\n",
    "    .value_counts()\n",
    "    .plot.bar(\n",
    "        x=\"shark_length_m\",\n",
    "        y=\"count\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a361e5-7f5d-470f-a290-e353261350c8",
   "metadata": {},
   "source": [
    "Impute the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e38d3-ec32-4671-a601-455699538e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    shark_lengths.fill_null(shark_lengths.median())\n",
    "    .value_counts()\n",
    "    .plot.bar(\n",
    "        x=\"shark_length_m\",\n",
    "        y=\"count\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd3ee3-503e-4d55-bbbd-1aae8b104966",
   "metadata": {},
   "source": [
    "None of these seem particularly satisfactory, but they at least let the ML model use all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69d350-6eef-4561-9ab2-ed0897a4c6a4",
   "metadata": {},
   "source": [
    "Multivariate feature imputation uses the non-missing values in an observation to predict (estimate) the missing values. Linear regression is a popular way of doing the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038016e8-f21c-440c-ac11-931525531981",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_2023_df = (\n",
    "    pl.read_csv(\"data/world-bank-gdp.csv\")\n",
    "    .filter(pl.col(\"year\") == 2023, pl.col(\"income_group\") != \"\")\n",
    "    .drop(\"year\")\n",
    "    .with_columns(pl.col(\"gdp\").is_null().alias(\"missing_gdp\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f67ebf-ea52-4c9d-8333-257e9e78e95d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdp_2023_df.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8274d1-3aa9-4d9a-8777-cde509f6fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_feature_df = gdp_2023_df.select([\"income_group\", \"population\", \"gdp\"]).to_dummies(\n",
    "    \"income_group\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c7f79-5edd-4b67-b95b-db8f9df69337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "iterative_imputer = IterativeImputer(max_iter=10, random_state=SEED)\n",
    "\n",
    "iterative_imputed_value_df = pl.from_numpy(\n",
    "    iterative_imputer.fit_transform(gdp_feature_df), schema=gdp_feature_df.columns\n",
    ")\n",
    "\n",
    "with pl.Config(tbl_rows=len(gdp_2023_df)):\n",
    "    with pl.StringCache():\n",
    "        pl.Series(\n",
    "            [\"Low income\", \"Lower middle income\", \"Upper middle income\", \"High income\"]\n",
    "        ).cast(pl.Categorical)\n",
    "\n",
    "        print(\n",
    "            gdp_2023_df.with_columns(\n",
    "                pl.col(\"income_group\").cast(pl.Categorical),\n",
    "                iterative_imputed_value_df.get_column(\"gdp\"),\n",
    "            )\n",
    "            .filter(pl.col(\"missing_gdp\"))\n",
    "            .select([\"country_name\", \"income_group\", \"population\", \"gdp\"])\n",
    "            .sort([\"income_group\", \"population\"])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf7252-087b-482b-8d89-c9613aa986a3",
   "metadata": {},
   "source": [
    "Nearest neighbours imputation is another multivariate imputation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c20d4-b36e-4a86-8678-660c83a5925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn_imputer = KNNImputer()\n",
    "\n",
    "knn_imputed_value_df = pl.from_numpy(\n",
    "    knn_imputer.fit_transform(gdp_feature_df), schema=gdp_feature_df.columns\n",
    ")\n",
    "\n",
    "with pl.Config(tbl_rows=len(gdp_2023_df)):\n",
    "    with pl.StringCache():\n",
    "        pl.Series(\n",
    "            [\"Low income\", \"Lower middle income\", \"Upper middle income\", \"High income\"]\n",
    "        ).cast(pl.Categorical)\n",
    "\n",
    "        print(\n",
    "            gdp_2023_df.with_columns(\n",
    "                pl.col(\"income_group\").cast(pl.Categorical),\n",
    "                knn_imputed_value_df.get_column(\"gdp\"),\n",
    "            )\n",
    "            .filter(pl.col(\"missing_gdp\"))\n",
    "            .select([\"country_name\", \"income_group\", \"population\", \"gdp\"])\n",
    "            .sort([\"income_group\", \"population\"])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd65d79-6b9b-4eee-8e0c-8fb68294fa34",
   "metadata": {},
   "source": [
    "### Encoding categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f720b2-371d-4a44-9b77-1b1e591245f8",
   "metadata": {},
   "source": [
    "Categorical variables (such as education level, colours, etc.) need to be encoded before they can be used as features in an ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2696232-5e41-41fe-9e7c-3dc8b6850b92",
   "metadata": {},
   "source": [
    "If the feature only has a few values, we've seen that we can transform them into dummy variables. A new feature (column) is created for each level (value), with a 1 indicating the presence of that value, and 0 its absence.\n",
    "\n",
    "We generally denote one value as the reference value. That is the value that is indicated by the absence of all the others. So, if we had a categorical feature called `day_of_week` we might have `Sunday` as the reference value, and the dummy features would be `day_of_week_monday` to `day_of_week_saturday`. There would be no column for the reference value as it would be the default if all the other day features were 0---i.e. if it's not Monday, Tuesday, Wednesday, Thursday, Friday or Saturday then it must be Sunday.\n",
    "                                                                                                                                                                \n",
    "In general, if we have $n$ values, we will end up with $n-1$ dummy features.                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd42253-5ac6-43d7-a8fe-6cd3c5b49f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    gdp_2023_df.with_columns(pl.col(\"income_group\").alias(\"income_group_\"))\n",
    "    .select([\"income_group\", \"income_group_\"])\n",
    "    .to_dummies(\"income_group_\", drop_first=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96cf06e-8e57-4fd1-afa6-d7ef16d63e4b",
   "metadata": {},
   "source": [
    "If the categorical feature has many unique values, we can use binary encoding. This will constrain the number of dummy features to be $log_{2}$ of the number of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0620e-713b-4936-8bb6-e01ea77a2609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "gdp_df = pl.read_csv(\"data/world-bank-gdp.csv\")\n",
    "\n",
    "binary_encodings = pl.from_pandas(\n",
    "    BinaryEncoder(cols=\"country_code\").fit_transform(\n",
    "        gdp_df.select([\"country_name\", \"country_code\"]).to_pandas()\n",
    "    )\n",
    ")\n",
    "\n",
    "binary_encodings.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b42d5f-c8f9-4727-89f8-01c9fefe74ae",
   "metadata": {},
   "source": [
    "If there's a natural order to the data, we can encode that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dbec43-e552-4261-b1ca-0d2da177bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encodings = OrdinalEncoder(\n",
    "    categories=[\n",
    "        [\"Low income\", \"Lower middle income\", \"Upper middle income\", \"High income\"]\n",
    "    ]\n",
    ").fit_transform(\n",
    "    gdp_df.filter(pl.col(\"income_group\") != \"\").select(pl.col(\"income_group\"))\n",
    ")\n",
    "\n",
    "(\n",
    "    gdp_df.filter(pl.col(\"income_group\") != \"\")\n",
    "    .select(pl.col(\"income_group\"))\n",
    "    .with_columns(pl.Series(name=\"encoding\", values=ordinal_encodings))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff48a5e-b29b-49ad-bb48-9207ff5ada76",
   "metadata": {},
   "source": [
    "Target encoding converts categorical variables into numerical values based on the relationship between each category and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ff24d-dba2-44ef-b0b9-c22fc4afb10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "shark_incident_df = pl.read_parquet(\"data/shark-incidents.parquet\")\n",
    "\n",
    "pl.from_numpy(\n",
    "    TargetEncoder().fit_transform(\n",
    "        shark_incident_df.select(\"shark_common_name\"),\n",
    "        shark_incident_df.get_column(\"victim_injury\"),\n",
    "    )\n",
    ").with_columns(\n",
    "    shark_incident_df.select(\"shark_common_name\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f2dc3-ccbd-4412-9f1f-72da985dd4af",
   "metadata": {},
   "source": [
    "As this is an ML technique, we may see variations due to fold splits, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f52c67f-a9b7-42b9-a998-64e0fc5e5c65",
   "metadata": {},
   "source": [
    "### Extracting temporal features from timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef243166-d1b1-4a84-a16d-9313e0d7bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    shark_incident_df.filter(pl.col(\"time_of_incident\").is_not_null())\n",
    "    .select(\"time_of_incident\")\n",
    "    .sample(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed52d56-b8d0-4fca-a91a-567be9f5da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    shark_incident_df.with_columns(\n",
    "        shark_incident_df.get_column(\"time_of_incident\")\n",
    "        .cut(breaks=[500, 800, 1200, 1500, 1700, 1900, 2100], left_closed=True)\n",
    "        .replace_strict(\n",
    "            {\n",
    "                \"[-inf, 500)\": \"night\",\n",
    "                \"[500, 800)\": \"early morning\",\n",
    "                \"[800, 1200)\": \"late morning\",\n",
    "                \"[1200, 1500)\": \"early afternoon\",\n",
    "                \"[1500, 1700)\": \"late afternoon\",\n",
    "                \"[1700, 1900)\": \"early evening\",\n",
    "                \"[1900, 2100)\": \"late evening\",\n",
    "                \"[2100, inf)\": \"night\",\n",
    "            }\n",
    "        )\n",
    "        .alias(\"period_of_incident\")\n",
    "    )\n",
    "    .select([\"time_of_incident\", \"period_of_incident\"])\n",
    "    .sample(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233ca69-b941-4fde-bd5c-b60f9c9f90d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    shark_incident_df.with_columns(\n",
    "        pl.col(\"time_of_incident\")\n",
    "        .cut(breaks=[500, 800, 1200, 1500, 1700, 1900, 2100], left_closed=True)\n",
    "        .replace_strict(\n",
    "            {\n",
    "                \"[-inf, 500)\": \"night\",\n",
    "                \"[500, 800)\": \"early morning\",\n",
    "                \"[800, 1200)\": \"late morning\",\n",
    "                \"[1200, 1500)\": \"early afternoon\",\n",
    "                \"[1500, 1700)\": \"late afternoon\",\n",
    "                \"[1700, 1900)\": \"early evening\",\n",
    "                \"[1900, 2100)\": \"late evening\",\n",
    "                \"[2100, inf)\": \"night\",\n",
    "            }\n",
    "        )\n",
    "        .alias(\"period_of_incident\")\n",
    "    )\n",
    "    .select([\"time_of_incident\", \"period_of_incident\"])\n",
    "    .filter(pl.col(\"time_of_incident\").is_not_null())\n",
    "    .unique()\n",
    "    .sort(\"time_of_incident\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898da3e9-8e21-42a5-aa6a-988d52fc6386",
   "metadata": {},
   "source": [
    "### Interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb025c47-ac40-4f65-b795-f1d85398f0b2",
   "metadata": {},
   "source": [
    "Interaction effects in machine learning occur when the relationship between one feature (or variable) and the target outcome depends on the value of another feature. For example, in a model predicting house prices, the effect of a house's size on price might depend on its locationlarger houses may increase in value more significantly in urban areas than in rural ones.\n",
    "\n",
    "In linear modelling, we can represent this as follows.\n",
    "\n",
    "$$\n",
    "y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{1}x_{2} + \\epsilon\n",
    "$$\n",
    "\n",
    "We can then use $\\beta_{3}$ to determine the _type_ of interaction effect.\n",
    "\n",
    "- If $\\beta_{3}$ is positive, and $x_{1}$ and $x_{2}$ also have an impact, then the interaction is synergystic.\n",
    "- If $\\beta_{3}$ is negative, and $x_{1}$ and $x_{2}$ also have an impact, then the interaction is antagonistic.\n",
    "- If $\\beta_{3}$ is negligible, then the relationship between $x_{1}$ and $x_{2}$ is additive.\n",
    "- If $\\beta_{3}$ is non-negligible, then it's rare that either $x_{1}$ or $x_{2}$ wouldn't also have an impact. This would be atypical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fefc40-b662-4b55-aae6-d475a69c47b2",
   "metadata": {},
   "source": [
    "We can create a new interaction term by multiplying columns together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485e5a0-0b6d-427c-b97b-16afc2f5f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    survey_df.with_columns(\n",
    "        (pl.col(\"education\").cast(pl.Int64) * pl.col(\"experience\")).alias(\n",
    "            \"education_experience\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bad78d-1cb0-4879-954c-b3113eb05fc6",
   "metadata": {},
   "source": [
    "Approaches such as decision trees and neural networks can model interaction effects directly. Other techniques, such as logistic regression, require the interaction terms to be engineered.\n",
    "\n",
    "In the case of decision trees, a feature, $X_{1}$, does _not_ interact with another feature, $X_{2}$ if\n",
    "\n",
    "- they are not on the same branch or\n",
    "- $X_{2}$ is used the same way in each branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2869b0e2-a572-4597-8e82-c17ea6a73942",
   "metadata": {},
   "source": [
    "The [XGBoost Feature Interactions Reshaped](https://github.com/limexp/xgbfir) library ranks features and feature interactions in XGBoost models.\n",
    "\n",
    "It ranks the feature interactions according to various metrics, and provides an \"Average Rank\" score.\n",
    "\n",
    "The library produces an Excel workbook, which we'll query using Polars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45836ae0-69c1-47ab-8c56-134207d97520",
   "metadata": {},
   "source": [
    "Write the report to `temp/survey-fi.xlsx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb6d87b-485a-4691-9b09-feb88b357ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgbfir\n",
    "\n",
    "xgbfir.saveXgbFI(\n",
    "    xgb_classifier,\n",
    "    feature_names=X_train_df_.columns,\n",
    "    OutputXlsxFile=\"temp/survey-fi.xlsx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728dce08-5455-4618-bb30-2cc21923b17f",
   "metadata": {},
   "source": [
    "What worksheets are in the workbook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e67ea-84e0-4fca-8e2f-2fd946e19bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "workbook = load_workbook(\"temp/survey-fi.xlsx\", read_only=True)\n",
    "workbook.sheetnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576773c4-71cf-41ba-948a-c0fa6a62cc6f",
   "metadata": {},
   "source": [
    "The \"Depth 0\" worksheet contains the feature metrics. The \"Depth 1\" worksheet shows interactions between _pairs_ of features. And, as you might have guessed, the \"Depth 2\" worksheet contains interactions between _three_ metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b9ad6-0c59-40fb-a355-afd1a254229e",
   "metadata": {},
   "source": [
    "Review the features, ranked by average rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf524928-8bde-48a3-aff4-54175ed9a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_excel(\"temp/survey-fi.xlsx\", sheet_name=\"Interaction Depth 0\")\n",
    "    .with_columns(\n",
    "        pl.col(\"Interaction\")\n",
    "        .str.replace_all(\"numeric_pipeline__\", \"\")\n",
    "        .str.replace_all(\"categorical_pipeline__\", \"\")\n",
    "    )\n",
    "    .sort(\"Average Rank\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10aef7-c9c6-4668-89d7-d557790ecf08",
   "metadata": {},
   "source": [
    "R experience, education and years of experience continue to be highlighted.\n",
    "\n",
    "Gender seems to be less important than other features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54dfe3c-9f4e-4c35-abad-d6ffb1e73c12",
   "metadata": {},
   "source": [
    "What about the interactions between pairs of features. Review the highest ranked interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f903b-788a-4ffa-b7ba-7c769fa9a36e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pl.Config(tbl_rows=1000, fmt_str_lengths=1000, tbl_width_chars=1000):\n",
    "    print(\n",
    "        pl.read_excel(\"temp/survey-fi.xlsx\", sheet_name=\"Interaction Depth 1\")\n",
    "        .with_columns(\n",
    "            pl.col(\"Interaction\")\n",
    "            .str.replace_all(\"numeric_pipeline__\", \"\")\n",
    "            .str.replace_all(\"categorical_pipeline__\", \"\")\n",
    "        )\n",
    "        .sort(\"Average Rank\")\n",
    "        .select([\"Interaction\", \"Average Rank\"])\n",
    "        .head(10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503be4e7-50d0-4cab-8a2c-2b4b22e4f0ba",
   "metadata": {},
   "source": [
    "Education tends to be followed by experience.\n",
    "\n",
    "Let's see if these values are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5802c-ae6d-4009-bfc8-9342d55c396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df.select(pl.corr(\"education\", \"experience\", method=\"spearman\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2f305-ec68-432c-9f71-335575e03e49",
   "metadata": {},
   "source": [
    "It's possible that the model is teasing out a non-linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a00d33-c802-478a-8963-6416b36f57f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn.objects as so\n",
    "\n",
    "(\n",
    "    so.Plot(survey_df, x=\"experience\", y=\"education\", color=\"title\")\n",
    "    .add(so.Dots(alpha=0.9, pointsize=2), so.Jitter(x=0.7, y=1))\n",
    "    .add(so.Line(), so.PolyFit())\n",
    "    .scale(color=\"viridis\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fabc9d-f9b6-4718-a774-0ac61f7bb330",
   "metadata": {},
   "source": [
    "Mid-career data scientists appear to have more education that those at earlier or later stages of their careers. This effect isn't as pronounced for software engineers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f18550-cd21-4716-ac8c-961fc9fb5e7d",
   "metadata": {},
   "source": [
    "What about a relationship between R experience and compensation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605649cd-62cc-44c0-9872-10f35e575848",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    survey_df.select([\"compensation\", \"r\"])\n",
    "    .sample(1000)\n",
    "    .plot.boxplot(x=\"r\", y=\"compensation\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b10d8e3-337a-43b1-a929-5570ad85b02c",
   "metadata": {},
   "source": [
    "We can also review the interactions between three features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d932f25-2207-48e3-99c7-3aabc46682a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pl.Config(tbl_rows=1000, fmt_str_lengths=1000, tbl_width_chars=1000):\n",
    "    print(\n",
    "        pl.read_excel(\"temp/survey-fi.xlsx\", sheet_name=\"Interaction Depth 2\")\n",
    "        .with_columns(\n",
    "            pl.col(\"Interaction\")\n",
    "            .str.replace_all(\"numeric_pipeline__\", \"\")\n",
    "            .str.replace_all(\"categorical_pipeline__\", \"\")\n",
    "        )\n",
    "        .sort(\"Average Rank\")\n",
    "        .select([\"Interaction\", \"Average Rank\"])\n",
    "        .head(10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d9e4a-5522-4301-9d22-7e70ef38911a",
   "metadata": {},
   "source": [
    "Note that gender doesn't seem to feature prominently in the interaction metrics, suggesting it doesn't have a direct _or_ synergystic/antagonistic effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01948e-8633-4edb-aa16-6a71b3a4e2d4",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e55c8-0df0-4c5e-b042-3989fa68d0e6",
   "metadata": {},
   "source": [
    "Feature selection is the process of selecting the most relevant features from the existing feature set by removing redundant, irrelevant or noisy ones.\n",
    "\n",
    "The goal to is reduce dimensionality, prevent overfitting and improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f19b76-aa7e-4eb1-9d1a-37f2304bec74",
   "metadata": {},
   "source": [
    "### Domain expertise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9e353-e769-4b9f-b881-009dc3060b07",
   "metadata": {},
   "source": [
    "Domain expertise can be the most significant tool in feature selection. Understanding how the variables are related to each other, and to the value to be predicted, can significantly simplify the modelling burden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8751e4f-72cc-4c3f-9845-21d938edf40b",
   "metadata": {},
   "source": [
    "Influence diagrams are an effective way to represent domain knowledge. The capture the _causal_ relationships between variables, which are difficult to tease out of traditional machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afac5b5-3fc4-4d20-a0c7-5f0dfb7258b7",
   "metadata": {},
   "source": [
    "<img src=\"images/module3-influence-diagram.png\" alt=\"Influence diagram\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f43854-558a-4b73-a7e0-591f51110d97",
   "metadata": {},
   "source": [
    "### Correlated features/targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e44c4-1c32-443c-969a-f8f82ce0799f",
   "metadata": {},
   "source": [
    "We can examine correlations between features, and between features and the target.\n",
    "\n",
    "Correlated features may suggest double counting.\n",
    "\n",
    "Features that are correlated with the target may be useful predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc80453-b5d9-414d-bce2-11ed229af838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(\n",
    "    pl.from_pandas(\n",
    "        data_preperation_pipeline.fit_transform(\n",
    "            X_df.with_columns(y_df)\n",
    "            .with_columns(\n",
    "                pl.when(pl.col(\"title\") == \"Software Engineer\")\n",
    "                .then(1)\n",
    "                .otherwise(0)\n",
    "                .alias(\"title\")\n",
    "            )\n",
    "            .to_pandas()\n",
    "        )\n",
    "        .rename(\n",
    "            lambda column_name: column_name.replace(\"categorical_pipeline__\", \"\")\n",
    "            .replace(\"numeric_pipeline__\", \"\")\n",
    "            .replace(\"remainder__\", \"\")\n",
    "        )\n",
    "        .rename(\n",
    "            {\n",
    "                \"major_Computer science (software engineering, etc.)\": \"major_cs\",\n",
    "            }\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .corr(method=\"spearman\")\n",
    "        .reset_index(names=\"variable\")\n",
    "    )\n",
    "    .select(\n",
    "        \"variable\",\n",
    "        \"age\",\n",
    "        \"education\",\n",
    "        \"experience\",\n",
    "        \"compensation\",\n",
    "        \"major_cs\",\n",
    "        \"r\",\n",
    "        \"title\",\n",
    "    )\n",
    "    .to_pandas()\n",
    "    .set_index(\"variable\"),\n",
    "    cmap=\"RdBu\",\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ").set(xlabel=\"\", ylabel=\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d5678-9fc0-4c0c-aec6-8ec7e4fd0ecf",
   "metadata": {},
   "source": [
    "We can see that age and experience are quite strongly correlated. Age also seems quite weakly correlated with the target variable.\n",
    "\n",
    "Country also seems to be correlated with compensation, meaning that country may just be acting as a proxy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046bf31e-5fc7-4ef2-b763-7df9350815d6",
   "metadata": {},
   "source": [
    "We could look at correlations between interaction terms and the target, although we quickly hit problems due to combinatorial explosion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd65ab-ab12-424f-8f7e-24f8bff89fad",
   "metadata": {},
   "source": [
    "### Removing uninformative features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da4dd9-09e6-4a0c-a3a2-0ee131f9516d",
   "metadata": {},
   "source": [
    "Features with low variance don't provide much predictive value. We may wish to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128414b-3868-4b3d-b59c-55ac8d915a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(X_test_df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d0b64c-79b9-4a09-815f-c6b65d943d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_train_df_.with_columns(\n",
    "        (pl.all() - pl.all().min()) / (pl.all().max() - pl.all().min())\n",
    "    )\n",
    "    .var()\n",
    "    .transpose(include_header=True, header_name=\"feature\", column_names=[\"variance\"])\n",
    "    .with_columns(\n",
    "        pl.col(\"feature\")\n",
    "        .str.replace_all(\"categorical_pipeline__\", \"\")\n",
    "        .str.replace_all(\"numeric_pipeline__\", \"\")\n",
    "    )\n",
    "    .sort(\"variance\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9044a9-bd34-4912-ad9e-ddf67fc1451a",
   "metadata": {},
   "source": [
    "We need to balance the lack of information in the feature with the need to include data about minority groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec33e66-bf54-4158-b5af-488ba081e1db",
   "metadata": {},
   "source": [
    "### Statistical filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe149f6-530a-4feb-a933-50654ccc650f",
   "metadata": {},
   "source": [
    "We can select features based on how they perform on statistical tests against the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27953ef0-719f-4d1f-bc5f-3d7c7bd607cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "penguin_df = (\n",
    "    pl.read_csv(\"data/penguins.csv\")\n",
    "    .select(\n",
    "        [\n",
    "            \"body_mass_g\",\n",
    "            \"flipper_length_mm\",\n",
    "            \"bill_depth_mm\",\n",
    "            \"bill_length_mm\",\n",
    "            \"species\",\n",
    "        ]\n",
    "    )\n",
    "    .drop_nulls()\n",
    ")\n",
    "\n",
    "k_best_selector = SelectKBest(k=2).fit(\n",
    "    penguin_df.drop(\"species\"), penguin_df.select(\"species\")\n",
    ")\n",
    "\n",
    "column_indexes = k_best_selector.get_support(indices=True)\n",
    "\n",
    "penguin_df.select(pl.nth(column_indexes.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cffc46-a191-4749-9621-00a232a678dd",
   "metadata": {},
   "source": [
    "### Sequential feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67548a7-442c-470d-9ed2-920abd9cd052",
   "metadata": {},
   "source": [
    "Sequential feature selection iteratively adds (forward selection) or removes (backward selection) features. The choice of which feature to add/remove is determined by training models with each of the candidate features added/removed and scoring them. The feature that gives the best/worst result is added/removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9068618-beda-49ff-ad59-721320433827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "sfs = SequentialFeatureSelector(\n",
    "    LogisticRegression(), n_features_to_select=2, direction=\"forward\"\n",
    ")\n",
    "\n",
    "sfs_fit = sfs.fit(penguin_df.drop(\"species\"), penguin_df.select(\"species\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322c11d-15e8-4b93-a63b-d7145a5ccfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_df.drop(\"species\")[:, sfs_fit.support_.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeda98c-95f3-4752-aca5-b724753f22cf",
   "metadata": {},
   "source": [
    "### Recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4922769-eeea-4867-b459-33cc8e8138bc",
   "metadata": {},
   "source": [
    "Recursive feature elimination (RFE) is a wrapper selection method. It use feature importances to gradually remove features, refitting the model on the remaining features at each stage. A number of features are removed in each cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f54a52-36f1-4c4b-bc5c-4188a7a56abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe = RFE(LogisticRegression(), n_features_to_select=2)\n",
    "\n",
    "rfe_fit = rfe.fit(penguin_df.drop(\"species\"), penguin_df.select(\"species\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90540be-191d-4954-b01d-a58d56ec0ccb",
   "metadata": {},
   "source": [
    "What two features were retained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2ed4e-0948-4577-ba3c-767e25697dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_df.drop(\"species\")[:, rfe_fit.support_.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe52f34b-6ee2-4f79-8b2b-ac8aa57e787a",
   "metadata": {},
   "source": [
    "How were the features ranked (selected ones have a rank of 1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4b6da-c00a-472d-ad16-a3ab913d5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_fit.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7200d374-b936-446f-a49f-1457755edc7b",
   "metadata": {},
   "source": [
    "We can also perform RFE using cross-validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c1704-f1da-42ce-bbf3-3a19d74dac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rfe_cv = RFECV(LogisticRegression())\n",
    "\n",
    "rfe_cv_fit = rfe_cv.fit(penguin_df.drop(\"species\"), penguin_df.select(\"species\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c09cd4-74ed-413a-a635-fc7d0e0e9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_df.drop(\"species\")[:, rfe_cv_fit.support_.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406f4691-e755-4b2b-bdf1-c60cd1f0ebc0",
   "metadata": {},
   "source": [
    "### Tree-based feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb73dc00-c2ad-43d3-9c21-1eee1349352a",
   "metadata": {},
   "source": [
    "Tree-based algorithms perform their own feature selection. The are free to completely ignore unhelpful features when calculating the optimal splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938c988-b031-425f-8174-3bca3768c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame(\n",
    "    {\n",
    "        \"feature\": penguin_df.drop(\"species\").columns,\n",
    "        \"importance\": DecisionTreeClassifier(max_depth=2)\n",
    "        .fit(penguin_df.drop(\"species\"), penguin_df.select(\"species\"))\n",
    "        .feature_importances_,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa82939-6965-4ea8-a1d1-50c99d261a67",
   "metadata": {},
   "source": [
    "We can see that the decision tree didn't consider body mass to be a useful feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a928d-ab00-4e8e-8421-f3f924b149be",
   "metadata": {},
   "source": [
    "### Feature selection using grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8374d-518c-47f2-af83-7bfbb736c32a",
   "metadata": {},
   "source": [
    "We can combine RFE with grid search to search both the feature and hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ddedc0-cffd-4cb1-b4e5-d37711b59385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "rfe_gs_base_classifier = RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "rfe_gs = RFECV(rfe_gs_base_classifier, cv=StratifiedKFold(2))\n",
    "\n",
    "rfe_gs.fit(scaler.fit_transform(X_train_df_), y_train)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [(\"feature_selection\", rfe_gs), (\"classification\", rfe_gs_base_classifier)]\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid={\n",
    "        \"classification__max_depth\": [2, 3, 4, 5],\n",
    "    },\n",
    "    cv=StratifiedKFold(2),\n",
    ")\n",
    "\n",
    "grid_search.fit(\n",
    "    scaler.fit_transform(X_train_df_),\n",
    "    y_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc026a6-d421-4547-9d13-76748c509a80",
   "metadata": {},
   "source": [
    "Which features were selected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1cecfd-edb6-4ec8-aea6-b9a3f4d0ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_[:, rfe_gs.support_.tolist()].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f4771-2667-43fb-97f9-adef53dda131",
   "metadata": {},
   "source": [
    "What were the optimal hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c53bb-1c20-4cd9-a53d-1fd3a2b81c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d799543-25b6-4986-8d7e-30fe820532b8",
   "metadata": {},
   "source": [
    "How good is the optimal model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2277a-395a-4178-b5d9-5c7397d33cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80abc48-e336-4e44-9a2e-82c462ff00d3",
   "metadata": {},
   "source": [
    "## Handling imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1cccd-a816-4956-b9d4-986637b5c5bb",
   "metadata": {},
   "source": [
    "As we have seen when discussing the accuracy metric, unbalanced target classes can cause problems with ML.\n",
    "\n",
    "In addition to using more informative evaluation metrics (e.g. F1 score, AUC) we can resample our datasets to increase the occurrences of the minority class (oversampling) or reducing the occurences of the majority class (undersampling). We can also adjust the threshold probabilities to assist with classifying minority classes.\n",
    "\n",
    "We can also use algorithms that are more effective at handling unbalanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a1f2e9-11bb-4187-a7c5-7b7a6c4b7a49",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82111c0-4792-477e-ac9c-d5b25ae741e2",
   "metadata": {},
   "source": [
    "To increase the instances of the minority classes, we can simply resample them, with replacement. We only need to resample to balance classes in the training data.\n",
    "\n",
    "Other resampling techniques, such as Synthetic Minority Oversampling Technique (SMOTE), create synthetic observations, based on the existing data. SMOTE makes use of k-nearest neigbbours to generate its synthetic instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97c991-33bc-4455-ae32-cbb07a60c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_unbalanced, y_unbalanced = make_classification(\n",
    "    n_classes=2,\n",
    "    class_sep=2,\n",
    "    weights=[0.1, 0.9],\n",
    "    n_informative=3,\n",
    "    n_redundant=1,\n",
    "    flip_y=0,\n",
    "    n_features=8,\n",
    "    n_clusters_per_class=1,\n",
    "    n_samples=1000,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "X_train_unbalanced, X_test_unbalanced, y_train_unbalanced, y_test_unbalanced = (\n",
    "    train_test_split(X_unbalanced, y_unbalanced, test_size=0.3, random_state=SEED)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d3b39-340d-4a35-9fc1-075c6f41393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unbalanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7698e4-a2a9-4294-abc1-9ad357078f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Series(name=\"y\", values=y_train_unbalanced).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe5f30-05af-407c-b11a-38c7d77883b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy=\"auto\", random_state=SEED)\n",
    "\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(\n",
    "    X_train_unbalanced, y_train_unbalanced\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1cf0a-9913-4b98-8eab-ed185cc1687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77277b52-6047-4619-8392-2d4170dc47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Series(name=\"y\", values=y_train_resampled).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d940249-5882-4c9a-bb9d-0f42c3808398",
   "metadata": {},
   "source": [
    "### Balanced bagging classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962998b2-31ad-4e6d-9513-b6eda403c1c6",
   "metadata": {},
   "source": [
    "A balanced bagging classifier is a wrapper around a base classifier, such as a decision tree classifier, that incorporates target class balancing into the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9c649-2917-4871-8b80-a8ef682e3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "base_classifier = DecisionTreeClassifier(max_depth=4, random_state=SEED)\n",
    "\n",
    "balanced_bagging_classifier = BalancedBaggingClassifier(\n",
    "    base_classifier,\n",
    "    sampling_strategy=\"auto\",\n",
    "    replacement=False,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "balanced_bagging_classifier.fit(scaler.fit_transform(X_train_df_), y_train)\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test, balanced_bagging_classifier.predict(scaler.transform(X_test_df_))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba7b332-62e0-462f-ade7-d872138173ab",
   "metadata": {},
   "source": [
    "### Tree-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad438483-7fa3-48d1-9a4b-5a991ec00397",
   "metadata": {},
   "source": [
    "Tree-based models are flexible enough to incorporate minority classes. However, there is a danger of overfitting to these minority observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af215c18-b756-4615-89c2-4677785ca6b3",
   "metadata": {},
   "source": [
    "## Modelling subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661ab77-7645-4e1e-b78d-528deb1352e3",
   "metadata": {},
   "source": [
    "We don't have to be constrained to using a single model. It may make sense to split our data into subgroups and model each subgroup independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a3e20-b9cf-45b1-a1c3-d1da1547b250",
   "metadata": {},
   "source": [
    "We can use a logistic regression model to determine whether our penguins are Gentoos or another species (i.e. not Gentoos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbc43d-fbcd-4ce5-aa8b-87843efebbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gentoo_df = (\n",
    "    pl.read_csv(\"data/penguins.csv\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"flipper_length_mm\") - pl.col(\"flipper_length_mm\").mean())\n",
    "        / pl.col(\"flipper_length_mm\").std(),\n",
    "        (pl.col(\"bill_length_mm\") - pl.col(\"bill_length_mm\").mean())\n",
    "        / pl.col(\"bill_length_mm\").std(),\n",
    "        pl.when(pl.col(\"species\") == \"Gentoo\").then(1).otherwise(0).alias(\"gentoo\"),\n",
    "    )\n",
    "    .select([\"flipper_length_mm\", \"bill_length_mm\", \"gentoo\"])\n",
    "    .drop_nulls()\n",
    ")\n",
    "\n",
    "gentoo_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3c418-f409-416a-b20b-7aea388af40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(\n",
    "    gentoo_feature_train_df,\n",
    "    gentoo_feature_test_df,\n",
    "    gentoo_target_train_df,\n",
    "    gentoo_target_test_df,\n",
    ") = train_test_split(\n",
    "    gentoo_df.select([\"flipper_length_mm\", \"bill_length_mm\"]),\n",
    "    gentoo_df.select(\"gentoo\"),\n",
    "    test_size=0.30,\n",
    "    random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eec978-b929-44bc-b8f6-3cf5acb1d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_classifier = LogisticRegression(penalty=None)\n",
    "\n",
    "logistic_regression_classifier.fit(\n",
    "    gentoo_feature_train_df,\n",
    "    gentoo_target_train_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3a740-d287-49f8-a976-32923e7616a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        gentoo_target_test_df.get_column(\"gentoo\"),\n",
    "        logistic_regression_classifier.predict(gentoo_feature_test_df),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d126c06-9c68-4a80-911b-079d881ece18",
   "metadata": {},
   "source": [
    "This very simple model delivers a fairly high degree of accuracy.\n",
    "\n",
    "We could now build a model to classify the \"residuals\"---i.e. to distinguish between Adelies and Chinstraps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbb1ae3-77ed-44d6-ab04-b1258a98c7af",
   "metadata": {},
   "source": [
    "What if we use our LDA components with the logistic regression classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaccad8-e584-47b4-822c-eab6ad9d9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "gentoo_lda_df = lda_df.with_columns(\n",
    "    (pl.col(\"species\") == \"Gentoo\").alias(\"gentoo\")\n",
    ").drop(\"species\")\n",
    "\n",
    "(\n",
    "    gentoo_lda_feature_train_df,\n",
    "    gentoo_lda_feature_test_df,\n",
    "    gentoo_lda_target_train_df,\n",
    "    gentoo_lda_target_test_df,\n",
    ") = train_test_split(\n",
    "    gentoo_lda_df.select([\"component1\", \"component2\"]),\n",
    "    gentoo_lda_df.select(\"gentoo\"),\n",
    "    test_size=0.30,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "lda_logistic_regression_classifier = LogisticRegression(penalty=None)\n",
    "\n",
    "lda_logistic_regression_classifier.fit(\n",
    "    gentoo_lda_feature_train_df,\n",
    "    gentoo_lda_target_train_df,\n",
    ")\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        gentoo_lda_target_test_df.get_column(\"gentoo\"),\n",
    "        lda_logistic_regression_classifier.predict(gentoo_lda_feature_test_df),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38f209-0946-4f5a-8ebc-296375b17828",
   "metadata": {},
   "source": [
    "## Hands-on example of selecting features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec15b9-57b9-4be3-848b-64f393df6aef",
   "metadata": {},
   "source": [
    "In this hands-on section you will work with a dataset to perform feature engineering and selection. You can use one of your own datasets, or one from the course. The Lending Club dataset is prepared below, but feel free to substitute your own.\n",
    "\n",
    "With your chosen dataset, consider the following questions.\n",
    "\n",
    "- How good is this data. Where did it come from? Can we trust it?\n",
    "- Look at the distibution of the features?\n",
    "- Are the target values balanced? If not, what should we do about it?\n",
    "- What kind of ML algorithm do we want to start with?\n",
    "- How should we encode the non-numeric features?\n",
    "- Do we need to scale/transform the data?\n",
    "- Should we consider using dimensionality reduction?\n",
    "- Are any of the features highly correlated?\n",
    "- Are any of the features highly correlated with the target?\n",
    "- What features is the model prioritising?\n",
    "- Are there any interaction terms?\n",
    "- Do we know what features we want to use? How do we choose them? Do we need to search for them?\n",
    "\n",
    "Feel free to just consider the questions above, rather than write the code to perform the activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f4d4f5-70f5-4068-9594-415252a684fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(\n",
    "    X_df, y_df, validate_size=0.25, test_size=0.25, random_state=None, stratify=None\n",
    "):\n",
    "    X_remainder_df, X_test_df, y_remainder_df, y_test_df = (\n",
    "        model_selection.train_test_split(\n",
    "            X_df,\n",
    "            y_df,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=stratify,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    X_train_df, X_validate_df, y_train_df, y_validate_df = (\n",
    "        model_selection.train_test_split(\n",
    "            X_remainder_df,\n",
    "            y_remainder_df,\n",
    "            test_size=validate_size / (1 - test_size),\n",
    "            random_state=random_state,\n",
    "            stratify=stratify,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return X_train_df, X_validate_df, X_test_df, y_train_df, y_validate_df, y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f2069-6829-4fc2-8073-730d013b2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_df = pl.read_parquet(\"data/lending-club-sample-preprocessed.parquet\")\n",
    "\n",
    "lending_club_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1594e-f90f-44f5-ab20-f1fc82d615a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    lending_club_feature_train_df,\n",
    "    lending_club_feature_validate_df,\n",
    "    lending_club_feature_test_df,\n",
    "    lending_club_target_train_df,\n",
    "    lending_club_target_validate_df,\n",
    "    lending_club_target_test_df,\n",
    ") = train_validate_test_split(\n",
    "    lending_club_df.drop(\"fully_paid\"),\n",
    "    lending_club_df.select(\"fully_paid\"),\n",
    "    validate_size=0.10,\n",
    "    test_size=0.10,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "lc_X_train = scaler.fit_transform(lending_club_feature_train_df)\n",
    "\n",
    "lc_y_train = lending_club_target_train_df.get_column(\"fully_paid\")\n",
    "\n",
    "lc_X_validate = scaler.fit_transform(lending_club_feature_validate_df)\n",
    "\n",
    "lc_y_validate = lending_club_target_validate_df.get_column(\"fully_paid\")\n",
    "\n",
    "lc_X_test = scaler.fit_transform(lending_club_feature_test_df)\n",
    "\n",
    "lc_y_test = lending_club_target_test_df.get_column(\"fully_paid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821ab6f-d6cf-4f40-b892-da33d9d507b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_classifier = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "lending_club_classifier.fit(\n",
    "    lc_X_train,\n",
    "    lc_y_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab9b42-fe2e-4293-9462-2194dd42a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_classifier.score(lc_X_validate, lc_y_validate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
